{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"yeastdnnexplorer","text":""},{"location":"#introduction","title":"Introduction","text":"<p><code>yeastdnnexplorer</code> is intended to serve as a development environment for exploring different DNN models to infer the relationship between transcription factors and target genes using binding and perturbation expression data.</p>"},{"location":"#installation","title":"Installation","text":"<p>This repo has not yet been added to PyPI. See the developer installation below.</p>"},{"location":"#development","title":"Development","text":"<ol> <li>git clone the repo</li> <li><code>cd</code> into the local version of the repo</li> <li>choose one (or more) of the following (only poetry currently supported)</li> </ol>"},{"location":"#poetry","title":"poetry","text":"<p>You can also install the dependencies using poetry. I prefer setting the following:</p> <pre><code>poetry config virtualenvs.in-project true\n</code></pre> <p>So that the virtual environments are installed in the project directory as <code>.venv</code></p> <p>After cloning and <code>cd</code>ing into the repo, you can install the dependencies with:</p> <pre><code>poetry install\n</code></pre>"},{"location":"#mkdocs","title":"mkdocs","text":"<p>The documentation is build with mkdocs:</p>"},{"location":"#commands","title":"Commands","text":"<p>After building the environment with poetry, you can use <code>poetry run</code> or a poetry shell to execute the following:</p> <ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre> <p>To update the gh-pages documentation, use <code>poetry run mkdocs gh-deply</code></p>"},{"location":"data_loaders/real_data_loader/","title":"Real Data Loader","text":"<p>             Bases: <code>LightningDataModule</code></p> <p>A class to load in data from the CSV data for various binding and perturbation experiments.</p> <p>After loading in the data, the data loader will parse the data into the form expected by our models. It will also split the data into training, testing, and validation sets for the model to use.</p> <p>NOTE: Right now the only binding dataset this works with is the brent_nf_cc dataset because it has the same set of genes in each CSV file. This is the case for all of the perturbation datasets, but not for the other 2 binding datasets. In the future we would like to write a dataModule that handles the other 2 binding datasets. For now, you can only pass in a parameter for the title of the perturb response dataset that you want to use, and brent_nf_cc is hardcoded as the binding dataset.</p> Source code in <code>yeastdnnexplorer/data_loaders/real_data_loader.py</code> <pre><code>class RealDataLoader(LightningDataModule):\n    \"\"\"\n    A class to load in data from the CSV data for various binding and perturbation\n    experiments.\n\n    After loading in the data, the data loader will parse the data into the form\n    expected by our models. It will also split the data into training, testing, and\n    validation sets for the model to use.\n\n    NOTE: Right now the only binding dataset this works with is the brent_nf_cc dataset\n    because it has the same set of genes in each CSV file. This is the case for all of\n    the perturbation datasets, but not for the other 2 binding datasets. In the future\n    we would like to write a dataModule that handles the other 2 binding datasets. For\n    now, you can only pass in a parameter for the title of the perturb response\n    dataset that you want to use, and brent_nf_cc is hardcoded as the binding dataset.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        batch_size: int = 32,\n        val_size: float = 0.1,\n        test_size: float = 0.1,\n        random_state: int = 42,\n        data_dir_path: str | None = None,\n        perturbation_dataset_title: str = \"hu_reimann_tfko\",\n    ) -&gt; None:\n        \"\"\"\n        Constructor of RealDataLoader.\n\n        :param batch_size: The number of samples in each mini-batch\n        :type batch_size: int\n        :param val_size: The proportion of the dataset to include in the validation\n            split\n        :type val_size: float\n        :param test_size: The proportion of the dataset to include in the test split\n        :type test_size: float\n        :param random_state: The random seed to use for splitting the data (keep this\n            consistent to ensure reproduceability)\n        :type random_state: int\n        :param data_dir_path: The path to the directory containing the CSV files for the\n            binding and perturbation data\n        :type data_dir_path: str\n        :param perturbation_dataset_title: The title of the perturbation dataset to use\n            (one of 'hu_reimann_tfko', 'kemmeren_tfko', or 'mcisaac_oe')\n        :type perturbation_dataset_title: str\n        :raises TypeError: If batch_size is not an positive integer\n        :raises TypeError: If val_size is not a float between 0 and 1 (inclusive)\n        :raises TypeError: If test_size is not a float between 0 and 1 (inclusive)\n        :raises TypeError: If random_state is not an integer\n        :raises ValueError: If val_size + test_size is greater than 1 (i.e. the splits\n            are too large)\n        :raises ValueError: if no data_dir is provided\n        :raises AssertinoError: if the dataset sizes do not match up after reading in\n            the data from the CSV files\n\n        \"\"\"\n        if not isinstance(batch_size, int) or batch_size &lt; 1:\n            raise TypeError(\"batch_size must be a positive integer\")\n        if not isinstance(val_size, (int, float)) or val_size &lt;= 0 or val_size &gt;= 1:\n            raise TypeError(\"val_size must be a float between 0 and 1 (inclusive)\")\n        if not isinstance(test_size, (int, float)) or test_size &lt;= 0 or test_size &gt;= 1:\n            raise TypeError(\"test_size must be a float between 0 and 1 (inclusive)\")\n        if not isinstance(random_state, int):\n            raise TypeError(\"random_state must be an integer\")\n        if data_dir_path is None:\n            raise ValueError(\"data_dir_path must be provided\")\n        if test_size + val_size &gt; 1:\n            raise ValueError(\"val_size + test_size must be less than or equal to 1\")\n        if not isinstance(\n            perturbation_dataset_title, str\n        ) and perturbation_dataset_title in [\n            \"hu_reimann_tfko\",\n            \"kemmeren_tfko\",\n            \"mcisaac_oe\",\n        ]:\n            raise TypeError(\n                \"perturbation_dataset_title must be a string and must be one\"\n                \" of 'hu_reimann_tfko', 'kemmeren_tfko', or 'mcisaac_oe'\"\n            )\n\n        super().__init__()\n        self.batch_size = batch_size\n        self.val_size = val_size\n        self.test_size = test_size\n        self.random_state = random_state\n        self.data_dir_path = data_dir_path\n        self.perturbation_dataset_title = perturbation_dataset_title\n\n        self.final_data_tensor: torch.Tensor = None\n        self.binding_effect_matrix: torch.Tensor | None = None\n        self.perturbation_effect_matrix: torch.Tensor | None = None\n        self.val_dataset: TensorDataset | None = None\n        self.test_dataset: TensorDataset | None = None\n\n    def prepare_data(self) -&gt; None:\n        \"\"\"\n        This function reads in the binding data and perturbation data from the CSV files\n        that we have for these datasets.\n\n        It throws out any genes that are not present in both the binding and\n        perturbation sets, and then structures the data in a way that the model expects\n        and can use\n\n        \"\"\"\n\n        brent_cc_path = os.path.join(self.data_dir_path, \"binding/brent_nf_cc\")\n        brent_nf_csv_files = [\n            f for f in os.listdir(brent_cc_path) if f.endswith(\".csv\")\n        ]\n        perturb_dataset_path = os.path.join(\n            self.data_dir_path, f\"perturbation/{self.perturbation_dataset_title}\"\n        )\n        perturb_dataset_csv_files = [\n            f for f in os.listdir(perturb_dataset_path) if f.endswith(\".csv\")\n        ]\n\n        # get a list of the genes in the binding data csvs\n        # for brent_cc (and the 3 perturb response datasets) the genes are\n        # in the same order in each csv, so it suffices to grab the target_locus_tag\n        # column from the first one\n        brent_cc_genes_ids = pd.read_csv(\n            os.path.join(brent_cc_path, brent_nf_csv_files[0])\n        )[\"target_locus_tag\"]\n        perturb_dataset_genes_ids = pd.read_csv(\n            os.path.join(perturb_dataset_path, perturb_dataset_csv_files[0])\n        )[\"target_locus_tag\"]\n\n        # Get the intersection of the genes in the binding and perturbation data\n        common_genes = set(brent_cc_genes_ids).intersection(perturb_dataset_genes_ids)\n\n        # Read in binding data from csv files\n        binding_data_effects = pd.DataFrame()\n        binding_data_pvalues = pd.DataFrame()\n        for i, file in enumerate(brent_nf_csv_files):\n            file_path = os.path.join(brent_cc_path, file)\n            df = pd.read_csv(file_path)\n\n            # only keep the genes that are in the intersection\n            # of the genes in the binding and perturbation data\n            df = df[df[\"target_locus_tag\"].isin(common_genes)]\n\n            # we need to handle duplicates now\n            # (some datasets have multiple occurrences of the same gene)\n            # we will keep the occurrence with the highest value in the 'effect' column\n            # we can do this by sorting the dataframe by the 'effect' column\n            # in descending order and keeping the fist occurrence of each gene\n            # this does require us to do some additional work later (see how we\n            # are consistently setting the index to 'target_locus_tag',\n            # this ensures all of our datasets are in the same order)\n            df = df.sort_values(\"effect\", ascending=False).drop_duplicates(\n                subset=\"target_locus_tag\", keep=\"first\"\n            )\n\n            # on the first iteration, add target_locus_tag column to the binding data\n            if i == 0:\n                binding_data_effects[\"target_locus_tag\"] = df[\"target_locus_tag\"]\n                binding_data_pvalues[\"target_locus_tag\"] = df[\"target_locus_tag\"]\n                binding_data_effects.set_index(\"target_locus_tag\", inplace=True)\n                binding_data_pvalues.set_index(\"target_locus_tag\", inplace=True)\n\n            binding_data_effects[file] = df.set_index(\"target_locus_tag\")[\"effect\"]\n            binding_data_pvalues[file] = df.set_index(\"target_locus_tag\")[\"pvalue\"]\n\n        # Read in perturbation data from csv files\n        perturbation_effects = pd.DataFrame()\n        perturbation_pvalues = pd.DataFrame()\n        for i, file in enumerate(perturb_dataset_csv_files):\n            file_path = os.path.join(perturb_dataset_path, file)\n            df = pd.read_csv(file_path)\n\n            # only keep the genes that are in the\n            # intersection of the genes in the binding and perturbation data\n            df = df[df[\"target_locus_tag\"].isin(common_genes)]\n\n            # handle duplicates\n            df = df.sort_values(\"effect\", ascending=False).drop_duplicates(\n                subset=\"target_locus_tag\", keep=\"first\"\n            )\n\n            # on the first iteration, add the target_locus_tag\n            # column to the perturbation data\n            if i == 0:\n                perturbation_effects[\"target_locus_tag\"] = df[\"target_locus_tag\"]\n                perturbation_pvalues[\"target_locus_tag\"] = df[\"target_locus_tag\"]\n                perturbation_effects.set_index(\"target_locus_tag\", inplace=True)\n                perturbation_pvalues.set_index(\"target_locus_tag\", inplace=True)\n\n            perturbation_effects[file] = df.set_index(\"target_locus_tag\")[\"effect\"]\n            perturbation_pvalues[file] = df.set_index(\"target_locus_tag\")[\"pvalue\"]\n\n        # shapes should be equal at this point\n        assert binding_data_effects.shape == perturbation_effects.shape\n        assert binding_data_pvalues.shape == perturbation_pvalues.shape\n\n        # reindex so that the rows in binding and perturb data match up\n        # (we need genes to be in the same order)\n        perturbation_effects = perturbation_effects.reindex(binding_data_effects.index)\n        perturbation_pvalues = perturbation_pvalues.reindex(binding_data_pvalues.index)\n\n        # concat the data into the shape expected by the model\n        # we need to first convert the data to tensors\n        binding_data_effects_tensor = torch.tensor(\n            binding_data_effects.values, dtype=torch.float64\n        )\n        binding_data_pvalues_tensor = torch.tensor(\n            binding_data_pvalues.values, dtype=torch.float64\n        )\n        perturbation_effects_tensor = torch.tensor(\n            perturbation_effects.values, dtype=torch.float64\n        )\n        perturbation_pvalues_tensor = torch.tensor(\n            perturbation_pvalues.values, dtype=torch.float64\n        )\n\n        # note that we no longer have a bound / unbound tensor\n        # (like for the synthetic data)\n        self.final_data_tensor = torch.stack(\n            [\n                binding_data_effects_tensor,\n                binding_data_pvalues_tensor,\n                perturbation_effects_tensor,\n                perturbation_pvalues_tensor,\n            ],\n            dim=-1,\n        )\n\n    def setup(self, stage: str | None = None) -&gt; None:\n        \"\"\"\n        This function runs after prepare_data finishes and is used to split the data\n        into train, validation, and test sets It ensures that these datasets are of the\n        correct dimensionality and size to be used by the model.\n\n        :param stage: The stage of the data setup (either 'fit' for training, 'validate'\n            for validation, or 'test' for testing), unused for now as the model is not\n            complicated enough to necessitate this\n        :type stage: Optional[str]\n\n        \"\"\"\n        self.binding_effect_matrix = self.final_data_tensor[:, :, 0]\n        self.perturbation_effect_matrix = self.final_data_tensor[:, :, 2]\n\n        # split into train, val, and test\n        X_train, X_temp, Y_train, Y_temp = train_test_split(\n            self.binding_effect_matrix,\n            self.perturbation_effect_matrix,\n            test_size=(self.val_size + self.test_size),\n            random_state=self.random_state,\n        )\n\n        # normalize test_size so that it is a percentage of the remaining data\n        self.test_size = self.test_size / (self.val_size + self.test_size)\n        X_val, X_test, Y_val, Y_test = train_test_split(\n            X_temp, Y_temp, test_size=self.test_size, random_state=self.random_state\n        )\n\n        # Convert to tensors\n        X_train, Y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(\n            Y_train, dtype=torch.float32\n        )\n        X_val, Y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(\n            Y_val, dtype=torch.float32\n        )\n        X_test, Y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(\n            Y_test, dtype=torch.float32\n        )\n\n        # Set our datasets\n        self.train_dataset = TensorDataset(X_train, Y_train)\n        self.val_dataset = TensorDataset(X_val, Y_val)\n        self.test_dataset = TensorDataset(X_test, Y_test)\n\n    def train_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Function to return the training dataloader, we shuffle to avoid learning based\n        on the order of the data.\n\n        :return: The training dataloader\n        :rtype: DataLoader\n\n        \"\"\"\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            num_workers=15,\n            shuffle=True,\n            persistent_workers=True,\n        )\n\n    def val_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Function to return the validation dataloader.\n\n        :return: The validation dataloader\n        :rtype: DataLoader\n\n        \"\"\"\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            num_workers=15,\n            shuffle=False,\n            persistent_workers=True,\n        )\n\n    def test_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Function to return the testing dataloader.\n\n        :return: The testing dataloader\n        :rtype: DataLoader\n\n        \"\"\"\n        return DataLoader(\n            self.test_dataset,\n            batch_size=self.batch_size,\n            num_workers=15,\n            shuffle=False,\n            persistent_workers=True,\n        )\n</code></pre>"},{"location":"data_loaders/real_data_loader/#yeastdnnexplorer.data_loaders.real_data_loader.RealDataLoader.__init__","title":"<code>__init__(batch_size=32, val_size=0.1, test_size=0.1, random_state=42, data_dir_path=None, perturbation_dataset_title='hu_reimann_tfko')</code>","text":"<p>Constructor of RealDataLoader.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The number of samples in each mini-batch</p> <code>32</code> <code>val_size</code> <code>float</code> <p>The proportion of the dataset to include in the validation split</p> <code>0.1</code> <code>test_size</code> <code>float</code> <p>The proportion of the dataset to include in the test split</p> <code>0.1</code> <code>random_state</code> <code>int</code> <p>The random seed to use for splitting the data (keep this consistent to ensure reproduceability)</p> <code>42</code> <code>data_dir_path</code> <code>str | None</code> <p>The path to the directory containing the CSV files for the binding and perturbation data</p> <code>None</code> <code>perturbation_dataset_title</code> <code>str</code> <p>The title of the perturbation dataset to use (one of \u2018hu_reimann_tfko\u2019, \u2018kemmeren_tfko\u2019, or \u2018mcisaac_oe\u2019)</p> <code>'hu_reimann_tfko'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If batch_size is not an positive integer</p> <code>TypeError</code> <p>If val_size is not a float between 0 and 1 (inclusive)</p> <code>TypeError</code> <p>If test_size is not a float between 0 and 1 (inclusive)</p> <code>TypeError</code> <p>If random_state is not an integer</p> <code>ValueError</code> <p>If val_size + test_size is greater than 1 (i.e. the splits are too large)</p> <code>ValueError</code> <p>if no data_dir is provided</p> <code>AssertinoError</code> <p>if the dataset sizes do not match up after reading in the data from the CSV files</p> Source code in <code>yeastdnnexplorer/data_loaders/real_data_loader.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 32,\n    val_size: float = 0.1,\n    test_size: float = 0.1,\n    random_state: int = 42,\n    data_dir_path: str | None = None,\n    perturbation_dataset_title: str = \"hu_reimann_tfko\",\n) -&gt; None:\n    \"\"\"\n    Constructor of RealDataLoader.\n\n    :param batch_size: The number of samples in each mini-batch\n    :type batch_size: int\n    :param val_size: The proportion of the dataset to include in the validation\n        split\n    :type val_size: float\n    :param test_size: The proportion of the dataset to include in the test split\n    :type test_size: float\n    :param random_state: The random seed to use for splitting the data (keep this\n        consistent to ensure reproduceability)\n    :type random_state: int\n    :param data_dir_path: The path to the directory containing the CSV files for the\n        binding and perturbation data\n    :type data_dir_path: str\n    :param perturbation_dataset_title: The title of the perturbation dataset to use\n        (one of 'hu_reimann_tfko', 'kemmeren_tfko', or 'mcisaac_oe')\n    :type perturbation_dataset_title: str\n    :raises TypeError: If batch_size is not an positive integer\n    :raises TypeError: If val_size is not a float between 0 and 1 (inclusive)\n    :raises TypeError: If test_size is not a float between 0 and 1 (inclusive)\n    :raises TypeError: If random_state is not an integer\n    :raises ValueError: If val_size + test_size is greater than 1 (i.e. the splits\n        are too large)\n    :raises ValueError: if no data_dir is provided\n    :raises AssertinoError: if the dataset sizes do not match up after reading in\n        the data from the CSV files\n\n    \"\"\"\n    if not isinstance(batch_size, int) or batch_size &lt; 1:\n        raise TypeError(\"batch_size must be a positive integer\")\n    if not isinstance(val_size, (int, float)) or val_size &lt;= 0 or val_size &gt;= 1:\n        raise TypeError(\"val_size must be a float between 0 and 1 (inclusive)\")\n    if not isinstance(test_size, (int, float)) or test_size &lt;= 0 or test_size &gt;= 1:\n        raise TypeError(\"test_size must be a float between 0 and 1 (inclusive)\")\n    if not isinstance(random_state, int):\n        raise TypeError(\"random_state must be an integer\")\n    if data_dir_path is None:\n        raise ValueError(\"data_dir_path must be provided\")\n    if test_size + val_size &gt; 1:\n        raise ValueError(\"val_size + test_size must be less than or equal to 1\")\n    if not isinstance(\n        perturbation_dataset_title, str\n    ) and perturbation_dataset_title in [\n        \"hu_reimann_tfko\",\n        \"kemmeren_tfko\",\n        \"mcisaac_oe\",\n    ]:\n        raise TypeError(\n            \"perturbation_dataset_title must be a string and must be one\"\n            \" of 'hu_reimann_tfko', 'kemmeren_tfko', or 'mcisaac_oe'\"\n        )\n\n    super().__init__()\n    self.batch_size = batch_size\n    self.val_size = val_size\n    self.test_size = test_size\n    self.random_state = random_state\n    self.data_dir_path = data_dir_path\n    self.perturbation_dataset_title = perturbation_dataset_title\n\n    self.final_data_tensor: torch.Tensor = None\n    self.binding_effect_matrix: torch.Tensor | None = None\n    self.perturbation_effect_matrix: torch.Tensor | None = None\n    self.val_dataset: TensorDataset | None = None\n    self.test_dataset: TensorDataset | None = None\n</code></pre>"},{"location":"data_loaders/real_data_loader/#yeastdnnexplorer.data_loaders.real_data_loader.RealDataLoader.prepare_data","title":"<code>prepare_data()</code>","text":"<p>This function reads in the binding data and perturbation data from the CSV files that we have for these datasets.</p> <p>It throws out any genes that are not present in both the binding and perturbation sets, and then structures the data in a way that the model expects and can use</p> Source code in <code>yeastdnnexplorer/data_loaders/real_data_loader.py</code> <pre><code>def prepare_data(self) -&gt; None:\n    \"\"\"\n    This function reads in the binding data and perturbation data from the CSV files\n    that we have for these datasets.\n\n    It throws out any genes that are not present in both the binding and\n    perturbation sets, and then structures the data in a way that the model expects\n    and can use\n\n    \"\"\"\n\n    brent_cc_path = os.path.join(self.data_dir_path, \"binding/brent_nf_cc\")\n    brent_nf_csv_files = [\n        f for f in os.listdir(brent_cc_path) if f.endswith(\".csv\")\n    ]\n    perturb_dataset_path = os.path.join(\n        self.data_dir_path, f\"perturbation/{self.perturbation_dataset_title}\"\n    )\n    perturb_dataset_csv_files = [\n        f for f in os.listdir(perturb_dataset_path) if f.endswith(\".csv\")\n    ]\n\n    # get a list of the genes in the binding data csvs\n    # for brent_cc (and the 3 perturb response datasets) the genes are\n    # in the same order in each csv, so it suffices to grab the target_locus_tag\n    # column from the first one\n    brent_cc_genes_ids = pd.read_csv(\n        os.path.join(brent_cc_path, brent_nf_csv_files[0])\n    )[\"target_locus_tag\"]\n    perturb_dataset_genes_ids = pd.read_csv(\n        os.path.join(perturb_dataset_path, perturb_dataset_csv_files[0])\n    )[\"target_locus_tag\"]\n\n    # Get the intersection of the genes in the binding and perturbation data\n    common_genes = set(brent_cc_genes_ids).intersection(perturb_dataset_genes_ids)\n\n    # Read in binding data from csv files\n    binding_data_effects = pd.DataFrame()\n    binding_data_pvalues = pd.DataFrame()\n    for i, file in enumerate(brent_nf_csv_files):\n        file_path = os.path.join(brent_cc_path, file)\n        df = pd.read_csv(file_path)\n\n        # only keep the genes that are in the intersection\n        # of the genes in the binding and perturbation data\n        df = df[df[\"target_locus_tag\"].isin(common_genes)]\n\n        # we need to handle duplicates now\n        # (some datasets have multiple occurrences of the same gene)\n        # we will keep the occurrence with the highest value in the 'effect' column\n        # we can do this by sorting the dataframe by the 'effect' column\n        # in descending order and keeping the fist occurrence of each gene\n        # this does require us to do some additional work later (see how we\n        # are consistently setting the index to 'target_locus_tag',\n        # this ensures all of our datasets are in the same order)\n        df = df.sort_values(\"effect\", ascending=False).drop_duplicates(\n            subset=\"target_locus_tag\", keep=\"first\"\n        )\n\n        # on the first iteration, add target_locus_tag column to the binding data\n        if i == 0:\n            binding_data_effects[\"target_locus_tag\"] = df[\"target_locus_tag\"]\n            binding_data_pvalues[\"target_locus_tag\"] = df[\"target_locus_tag\"]\n            binding_data_effects.set_index(\"target_locus_tag\", inplace=True)\n            binding_data_pvalues.set_index(\"target_locus_tag\", inplace=True)\n\n        binding_data_effects[file] = df.set_index(\"target_locus_tag\")[\"effect\"]\n        binding_data_pvalues[file] = df.set_index(\"target_locus_tag\")[\"pvalue\"]\n\n    # Read in perturbation data from csv files\n    perturbation_effects = pd.DataFrame()\n    perturbation_pvalues = pd.DataFrame()\n    for i, file in enumerate(perturb_dataset_csv_files):\n        file_path = os.path.join(perturb_dataset_path, file)\n        df = pd.read_csv(file_path)\n\n        # only keep the genes that are in the\n        # intersection of the genes in the binding and perturbation data\n        df = df[df[\"target_locus_tag\"].isin(common_genes)]\n\n        # handle duplicates\n        df = df.sort_values(\"effect\", ascending=False).drop_duplicates(\n            subset=\"target_locus_tag\", keep=\"first\"\n        )\n\n        # on the first iteration, add the target_locus_tag\n        # column to the perturbation data\n        if i == 0:\n            perturbation_effects[\"target_locus_tag\"] = df[\"target_locus_tag\"]\n            perturbation_pvalues[\"target_locus_tag\"] = df[\"target_locus_tag\"]\n            perturbation_effects.set_index(\"target_locus_tag\", inplace=True)\n            perturbation_pvalues.set_index(\"target_locus_tag\", inplace=True)\n\n        perturbation_effects[file] = df.set_index(\"target_locus_tag\")[\"effect\"]\n        perturbation_pvalues[file] = df.set_index(\"target_locus_tag\")[\"pvalue\"]\n\n    # shapes should be equal at this point\n    assert binding_data_effects.shape == perturbation_effects.shape\n    assert binding_data_pvalues.shape == perturbation_pvalues.shape\n\n    # reindex so that the rows in binding and perturb data match up\n    # (we need genes to be in the same order)\n    perturbation_effects = perturbation_effects.reindex(binding_data_effects.index)\n    perturbation_pvalues = perturbation_pvalues.reindex(binding_data_pvalues.index)\n\n    # concat the data into the shape expected by the model\n    # we need to first convert the data to tensors\n    binding_data_effects_tensor = torch.tensor(\n        binding_data_effects.values, dtype=torch.float64\n    )\n    binding_data_pvalues_tensor = torch.tensor(\n        binding_data_pvalues.values, dtype=torch.float64\n    )\n    perturbation_effects_tensor = torch.tensor(\n        perturbation_effects.values, dtype=torch.float64\n    )\n    perturbation_pvalues_tensor = torch.tensor(\n        perturbation_pvalues.values, dtype=torch.float64\n    )\n\n    # note that we no longer have a bound / unbound tensor\n    # (like for the synthetic data)\n    self.final_data_tensor = torch.stack(\n        [\n            binding_data_effects_tensor,\n            binding_data_pvalues_tensor,\n            perturbation_effects_tensor,\n            perturbation_pvalues_tensor,\n        ],\n        dim=-1,\n    )\n</code></pre>"},{"location":"data_loaders/real_data_loader/#yeastdnnexplorer.data_loaders.real_data_loader.RealDataLoader.setup","title":"<code>setup(stage=None)</code>","text":"<p>This function runs after prepare_data finishes and is used to split the data into train, validation, and test sets It ensures that these datasets are of the correct dimensionality and size to be used by the model.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str | None</code> <p>The stage of the data setup (either \u2018fit\u2019 for training, \u2018validate\u2019 for validation, or \u2018test\u2019 for testing), unused for now as the model is not complicated enough to necessitate this</p> <code>None</code> Source code in <code>yeastdnnexplorer/data_loaders/real_data_loader.py</code> <pre><code>def setup(self, stage: str | None = None) -&gt; None:\n    \"\"\"\n    This function runs after prepare_data finishes and is used to split the data\n    into train, validation, and test sets It ensures that these datasets are of the\n    correct dimensionality and size to be used by the model.\n\n    :param stage: The stage of the data setup (either 'fit' for training, 'validate'\n        for validation, or 'test' for testing), unused for now as the model is not\n        complicated enough to necessitate this\n    :type stage: Optional[str]\n\n    \"\"\"\n    self.binding_effect_matrix = self.final_data_tensor[:, :, 0]\n    self.perturbation_effect_matrix = self.final_data_tensor[:, :, 2]\n\n    # split into train, val, and test\n    X_train, X_temp, Y_train, Y_temp = train_test_split(\n        self.binding_effect_matrix,\n        self.perturbation_effect_matrix,\n        test_size=(self.val_size + self.test_size),\n        random_state=self.random_state,\n    )\n\n    # normalize test_size so that it is a percentage of the remaining data\n    self.test_size = self.test_size / (self.val_size + self.test_size)\n    X_val, X_test, Y_val, Y_test = train_test_split(\n        X_temp, Y_temp, test_size=self.test_size, random_state=self.random_state\n    )\n\n    # Convert to tensors\n    X_train, Y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(\n        Y_train, dtype=torch.float32\n    )\n    X_val, Y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(\n        Y_val, dtype=torch.float32\n    )\n    X_test, Y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(\n        Y_test, dtype=torch.float32\n    )\n\n    # Set our datasets\n    self.train_dataset = TensorDataset(X_train, Y_train)\n    self.val_dataset = TensorDataset(X_val, Y_val)\n    self.test_dataset = TensorDataset(X_test, Y_test)\n</code></pre>"},{"location":"data_loaders/real_data_loader/#yeastdnnexplorer.data_loaders.real_data_loader.RealDataLoader.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Function to return the testing dataloader.</p> <p>Returns:</p> Type Description <code>DataLoader</code> <p>The testing dataloader</p> Source code in <code>yeastdnnexplorer/data_loaders/real_data_loader.py</code> <pre><code>def test_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Function to return the testing dataloader.\n\n    :return: The testing dataloader\n    :rtype: DataLoader\n\n    \"\"\"\n    return DataLoader(\n        self.test_dataset,\n        batch_size=self.batch_size,\n        num_workers=15,\n        shuffle=False,\n        persistent_workers=True,\n    )\n</code></pre>"},{"location":"data_loaders/real_data_loader/#yeastdnnexplorer.data_loaders.real_data_loader.RealDataLoader.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Function to return the training dataloader, we shuffle to avoid learning based on the order of the data.</p> <p>Returns:</p> Type Description <code>DataLoader</code> <p>The training dataloader</p> Source code in <code>yeastdnnexplorer/data_loaders/real_data_loader.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Function to return the training dataloader, we shuffle to avoid learning based\n    on the order of the data.\n\n    :return: The training dataloader\n    :rtype: DataLoader\n\n    \"\"\"\n    return DataLoader(\n        self.train_dataset,\n        batch_size=self.batch_size,\n        num_workers=15,\n        shuffle=True,\n        persistent_workers=True,\n    )\n</code></pre>"},{"location":"data_loaders/real_data_loader/#yeastdnnexplorer.data_loaders.real_data_loader.RealDataLoader.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Function to return the validation dataloader.</p> <p>Returns:</p> Type Description <code>DataLoader</code> <p>The validation dataloader</p> Source code in <code>yeastdnnexplorer/data_loaders/real_data_loader.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Function to return the validation dataloader.\n\n    :return: The validation dataloader\n    :rtype: DataLoader\n\n    \"\"\"\n    return DataLoader(\n        self.val_dataset,\n        batch_size=self.batch_size,\n        num_workers=15,\n        shuffle=False,\n        persistent_workers=True,\n    )\n</code></pre>"},{"location":"data_loaders/synthetic_data_loader/","title":"Synthetic Data Loader","text":"<p>             Bases: <code>LightningDataModule</code></p> <p>A class for a synthetic data loader that generates synthetic bindiing &amp; perturbation effect data for training, validation, and testing a model This class contains all of the logic for generating and parsing the synthetic data, as well as splitting it into train, validation, and test sets It is a subclass of pytorch_lightning.LightningDataModule, which is similar to a regular PyTorch DataLoader but with added functionality for data loading.</p> Source code in <code>yeastdnnexplorer/data_loaders/synthetic_data_loader.py</code> <pre><code>class SyntheticDataLoader(LightningDataModule):\n    \"\"\"A class for a synthetic data loader that generates synthetic bindiing &amp;\n    perturbation effect data for training, validation, and testing a model This class\n    contains all of the logic for generating and parsing the synthetic data, as well as\n    splitting it into train, validation, and test sets It is a subclass of\n    pytorch_lightning.LightningDataModule, which is similar to a regular PyTorch\n    DataLoader but with added functionality for data loading.\"\"\"\n\n    def __init__(\n        self,\n        batch_size: int = 32,\n        num_genes: int = 1000,\n        bound: list[float] = [0.1, 0.2, 0.2, 0.4, 0.5],\n        bound_mean: float = 3.0,\n        n_sample: list[int] = [1, 2, 2, 4, 4],\n        val_size: float = 0.1,\n        test_size: float = 0.1,\n        random_state: int = 42,\n        max_mean_adjustment: float = 0.0,\n        adjustment_function: Callable[\n            [torch.Tensor, float, float, float], torch.Tensor\n        ] = default_perturbation_effect_adjustment_function,\n        tf_relationships: dict[int, list[int] | list[Relation]] = {},\n    ) -&gt; None:\n        \"\"\"\n        Constructor of SyntheticDataLoader.\n\n        :param batch_size: The number of samples in each mini-batch\n        :type batch_size: int\n        :param num_genes: The number of genes in the synthetic data (this is the number\n            of datapoints in our dataset)\n        :type num_genes: int\n        :param bound: The proportion of genes in each sample group that are put in the\n            bound grop (i.e. have a non-zero binding effect and expression response)\n        :type bound: List[int]\n        :param n_sample: The number of samples to draw from each bound group\n        :type n_sample: List[int]\n        :param val_size: The proportion of the dataset to include in the validation\n            split\n        :type val_size: float\n        :param test_size: The proportion of the dataset to include in the test split\n        :type test_size: float\n        :param random_state: The random seed to use for splitting the data (keep this\n            consistent to ensure reproduceability)\n        :type random_state: int\n        :param bound_mean: The mean of the bound distribution\n        :type bound_mean: float\n        :param max_mean_adjustment: The maximum mean adjustment to apply to the mean\n                                    of the bound (bound) perturbation effects\n        :type max_mean_adjustment: float\n        :param adjustment_function: A function that adjusts the mean of the bound\n                                    (bound) perturbation effects\n        :type adjustment_function: Callable[[torch.Tensor, float, float,\n                                   float, dict[int, list[int]]], torch.Tensor]\n        :raises TypeError: If batch_size is not an positive integer\n        :raises TypeError: If num_genes is not an positive integer\n        :raises TypeError: If bound is not a list of integers or floats\n        :raises TypeError: If n_sample is not a list of integers\n        :raises TypeError: If val_size is not a float between 0 and 1 (inclusive)\n        :raises TypeError: If test_size is not a float between 0 and 1 (inclusive)\n        :raises TypeError: If random_state is not an integer\n        :raises TypeError: If bound_mean is not a float\n        :raises ValueError: If val_size + test_size is greater than 1 (i.e. the splits\n            are too large)\n\n        \"\"\"\n        if not isinstance(batch_size, int) or batch_size &lt; 1:\n            raise TypeError(\"batch_size must be a positive integer\")\n        if not isinstance(num_genes, int) or num_genes &lt; 1:\n            raise TypeError(\"num_genes must be a positive integer\")\n        if not isinstance(bound, list) or not all(\n            isinstance(x, (int, float)) for x in bound\n        ):\n            raise TypeError(\"bound must be a list of integers or floats\")\n        if not isinstance(n_sample, list) or not all(\n            isinstance(x, int) for x in n_sample\n        ):\n            raise TypeError(\"n_sample must be a list of integers\")\n        if not isinstance(val_size, (int, float)) or val_size &lt;= 0 or val_size &gt;= 1:\n            raise TypeError(\"val_size must be a float between 0 and 1 (inclusive)\")\n        if not isinstance(test_size, (int, float)) or test_size &lt;= 0 or test_size &gt;= 1:\n            raise TypeError(\"test_size must be a float between 0 and 1 (inclusive)\")\n        if not isinstance(random_state, int):\n            raise TypeError(\"random_state must be an integer\")\n        if not isinstance(bound_mean, float):\n            raise TypeError(\"bound_mean must be a float\")\n        if test_size + val_size &gt; 1:\n            raise ValueError(\"val_size + test_size must be less than or equal to 1\")\n\n        super().__init__()\n        self.batch_size = batch_size\n        self.num_genes = num_genes\n        self.bound_mean = bound_mean\n        self.bound = bound or [0.1, 0.15, 0.2, 0.25, 0.3]\n        self.n_sample = n_sample or [1 for _ in range(len(self.bound))]\n        self.num_tfs = sum(self.n_sample)  # sum of all n_sample is the number of TFs\n        self.val_size = val_size\n        self.test_size = test_size\n        self.random_state = random_state\n\n        self.max_mean_adjustment = max_mean_adjustment\n        self.adjustment_function = adjustment_function\n        self.tf_relationships = tf_relationships\n\n        self.final_data_tensor: torch.Tensor = None\n        self.binding_effect_matrix: torch.Tensor | None = None\n        self.perturbation_effect_matrix: torch.Tensor | None = None\n        self.val_dataset: TensorDataset | None = None\n        self.test_dataset: TensorDataset | None = None\n\n    def prepare_data(self) -&gt; None:\n        \"\"\"Function to generate the raw synthetic data and save it in a tensor For\n        explanations of the functions used to generate the data, see the\n        generate_in_silico_data tutorial notebook in the docs No assertion checks are\n        performed as that is handled in the functions in generate_data.py.\"\"\"\n        # this will be a list of length 10 with a GenePopulation object in each element\n        gene_populations_list = []\n        for bound_proportion, n_draws in zip(self.bound, self.n_sample):\n            for _ in range(n_draws):\n                gene_populations_list.append(\n                    generate_gene_population(self.num_genes, bound_proportion)\n                )\n\n        # Generate binding data for each gene population\n        binding_effect_list = [\n            generate_binding_effects(gene_population)\n            for gene_population in gene_populations_list\n        ]\n\n        # Calculate p-values for binding data\n        binding_pvalue_list = [\n            generate_pvalues(binding_data) for binding_data in binding_effect_list\n        ]\n\n        binding_data_combined = [\n            torch.stack((gene_population.labels, binding_effect, binding_pval), dim=1)\n            for gene_population, binding_effect, binding_pval in zip(\n                gene_populations_list, binding_effect_list, binding_pvalue_list\n            )\n        ]\n\n        # Stack along a new dimension (dim=1) to create a tensor of shape\n        # [num_genes, num_TFs, 3]\n        binding_data_tensor = torch.stack(binding_data_combined, dim=1)\n\n        # if we are using a mean adjustment, we need to generate perturbation\n        # effects in a slightly different way than if we are not using\n        # a mean adjustment\n        if self.max_mean_adjustment &gt; 0:\n            perturbation_effects_list = generate_perturbation_effects(\n                binding_data_tensor,\n                bound_mean=self.bound_mean,\n                tf_index=0,  # unused\n                max_mean_adjustment=self.max_mean_adjustment,\n                adjustment_function=self.adjustment_function,\n                tf_relationships=self.tf_relationships,\n            )\n\n            perturbation_pvalue_list = torch.zeros_like(perturbation_effects_list)\n            for col_index in range(perturbation_effects_list.shape[1]):\n                perturbation_pvalue_list[:, col_index] = generate_pvalues(\n                    perturbation_effects_list[:, col_index]\n                )\n\n            # take absolute values\n            perturbation_effects_list = torch.abs(perturbation_effects_list)\n\n            perturbation_effects_tensor = perturbation_effects_list\n            perturbation_pvalues_tensor = perturbation_pvalue_list\n        else:\n            perturbation_effects_list = [\n                generate_perturbation_effects(\n                    binding_data_tensor[:, tf_index, :].unsqueeze(1),\n                    bound_mean=self.bound_mean,\n                    tf_index=0,  # unused\n                )\n                for tf_index in range(sum(self.n_sample))\n            ]\n            perturbation_pvalue_list = [\n                generate_pvalues(perturbation_effects)\n                for perturbation_effects in perturbation_effects_list\n            ]\n\n            # take absolute values\n            perturbation_effects_list = [\n                torch.abs(perturbation_effects)\n                for perturbation_effects in perturbation_effects_list\n            ]\n\n            # Convert lists to tensors\n            perturbation_effects_tensor = torch.stack(perturbation_effects_list, dim=1)\n            perturbation_pvalues_tensor = torch.stack(perturbation_pvalue_list, dim=1)\n\n        # Ensure perturbation data is reshaped to match [n_genes, n_tfs]\n        # This step might need adjustment based on the actual shapes of your tensors.\n        perturbation_effects_tensor = perturbation_effects_tensor.unsqueeze(\n            -1\n        )  # Adds an extra dimension for concatenation\n        perturbation_pvalues_tensor = perturbation_pvalues_tensor.unsqueeze(\n            -1\n        )  # Adds an extra dimension for concatenation\n\n        # Concatenate along the last dimension to form a [n_genes, n_tfs, 5] tensor\n        self.final_data_tensor = torch.cat(\n            (\n                binding_data_tensor,\n                perturbation_effects_tensor,\n                perturbation_pvalues_tensor,\n            ),\n            dim=2,\n        )\n\n    def setup(self, stage: str | None = None) -&gt; None:\n        \"\"\"\n        This function runs after prepare_data finishes and is used to split the data\n        into train, validation, and test sets It ensures that these datasets are of the\n        correct dimensionality and size to be used by the model.\n\n        :param stage: The stage of the data setup (either 'fit' for training, 'validate'\n            for validation, or 'test' for testing), unused for now as the model is not\n            complicated enough to necessitate this\n        :type stage: Optional[str]\n\n        \"\"\"\n        self.binding_effect_matrix = self.final_data_tensor[:, :, 1]\n        self.perturbation_effect_matrix = self.final_data_tensor[:, :, 3]\n\n        # split into train, val, and test\n        X_train, X_temp, Y_train, Y_temp = train_test_split(\n            self.binding_effect_matrix,\n            self.perturbation_effect_matrix,\n            test_size=(self.val_size + self.test_size),\n            random_state=self.random_state,\n        )\n\n        # normalize test_size so that it is a percentage of the remaining data\n        self.test_size = self.test_size / (self.val_size + self.test_size)\n        X_val, X_test, Y_val, Y_test = train_test_split(\n            X_temp, Y_temp, test_size=self.test_size, random_state=self.random_state\n        )\n\n        # Convert to tensors\n        X_train, Y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(\n            Y_train, dtype=torch.float32\n        )\n        X_val, Y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(\n            Y_val, dtype=torch.float32\n        )\n        X_test, Y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(\n            Y_test, dtype=torch.float32\n        )\n\n        # Set our datasets\n        self.train_dataset = TensorDataset(X_train, Y_train)\n        self.val_dataset = TensorDataset(X_val, Y_val)\n        self.test_dataset = TensorDataset(X_test, Y_test)\n\n    def train_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Function to return the training dataloader, we shuffle to avoid learning based\n        on the order of the data.\n\n        :return: The training dataloader\n        :rtype: DataLoader\n\n        \"\"\"\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            num_workers=15,\n            shuffle=True,\n            persistent_workers=True,\n        )\n\n    def val_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Function to return the validation dataloader.\n\n        :return: The validation dataloader\n        :rtype: DataLoader\n\n        \"\"\"\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            num_workers=15,\n            shuffle=False,\n            persistent_workers=True,\n        )\n\n    def test_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Function to return the testing dataloader.\n\n        :return: The testing dataloader\n        :rtype: DataLoader\n\n        \"\"\"\n        return DataLoader(\n            self.test_dataset,\n            batch_size=self.batch_size,\n            num_workers=15,\n            shuffle=False,\n            persistent_workers=True,\n        )\n</code></pre>"},{"location":"data_loaders/synthetic_data_loader/#yeastdnnexplorer.data_loaders.synthetic_data_loader.SyntheticDataLoader.__init__","title":"<code>__init__(batch_size=32, num_genes=1000, bound=[0.1, 0.2, 0.2, 0.4, 0.5], bound_mean=3.0, n_sample=[1, 2, 2, 4, 4], val_size=0.1, test_size=0.1, random_state=42, max_mean_adjustment=0.0, adjustment_function=default_perturbation_effect_adjustment_function, tf_relationships={})</code>","text":"<p>Constructor of SyntheticDataLoader.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The number of samples in each mini-batch</p> <code>32</code> <code>num_genes</code> <code>int</code> <p>The number of genes in the synthetic data (this is the number of datapoints in our dataset)</p> <code>1000</code> <code>bound</code> <code>list[float]</code> <p>The proportion of genes in each sample group that are put in the bound grop (i.e. have a non-zero binding effect and expression response)</p> <code>[0.1, 0.2, 0.2, 0.4, 0.5]</code> <code>n_sample</code> <code>list[int]</code> <p>The number of samples to draw from each bound group</p> <code>[1, 2, 2, 4, 4]</code> <code>val_size</code> <code>float</code> <p>The proportion of the dataset to include in the validation split</p> <code>0.1</code> <code>test_size</code> <code>float</code> <p>The proportion of the dataset to include in the test split</p> <code>0.1</code> <code>random_state</code> <code>int</code> <p>The random seed to use for splitting the data (keep this consistent to ensure reproduceability)</p> <code>42</code> <code>bound_mean</code> <code>float</code> <p>The mean of the bound distribution</p> <code>3.0</code> <code>max_mean_adjustment</code> <code>float</code> <p>The maximum mean adjustment to apply to the mean of the bound (bound) perturbation effects</p> <code>0.0</code> <code>adjustment_function</code> <code>Callable[[Tensor, float, float, float], Tensor]</code> <p>A function that adjusts the mean of the bound (bound) perturbation effects</p> <code>default_perturbation_effect_adjustment_function</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If batch_size is not an positive integer</p> <code>TypeError</code> <p>If num_genes is not an positive integer</p> <code>TypeError</code> <p>If bound is not a list of integers or floats</p> <code>TypeError</code> <p>If n_sample is not a list of integers</p> <code>TypeError</code> <p>If val_size is not a float between 0 and 1 (inclusive)</p> <code>TypeError</code> <p>If test_size is not a float between 0 and 1 (inclusive)</p> <code>TypeError</code> <p>If random_state is not an integer</p> <code>TypeError</code> <p>If bound_mean is not a float</p> <code>ValueError</code> <p>If val_size + test_size is greater than 1 (i.e. the splits are too large)</p> Source code in <code>yeastdnnexplorer/data_loaders/synthetic_data_loader.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 32,\n    num_genes: int = 1000,\n    bound: list[float] = [0.1, 0.2, 0.2, 0.4, 0.5],\n    bound_mean: float = 3.0,\n    n_sample: list[int] = [1, 2, 2, 4, 4],\n    val_size: float = 0.1,\n    test_size: float = 0.1,\n    random_state: int = 42,\n    max_mean_adjustment: float = 0.0,\n    adjustment_function: Callable[\n        [torch.Tensor, float, float, float], torch.Tensor\n    ] = default_perturbation_effect_adjustment_function,\n    tf_relationships: dict[int, list[int] | list[Relation]] = {},\n) -&gt; None:\n    \"\"\"\n    Constructor of SyntheticDataLoader.\n\n    :param batch_size: The number of samples in each mini-batch\n    :type batch_size: int\n    :param num_genes: The number of genes in the synthetic data (this is the number\n        of datapoints in our dataset)\n    :type num_genes: int\n    :param bound: The proportion of genes in each sample group that are put in the\n        bound grop (i.e. have a non-zero binding effect and expression response)\n    :type bound: List[int]\n    :param n_sample: The number of samples to draw from each bound group\n    :type n_sample: List[int]\n    :param val_size: The proportion of the dataset to include in the validation\n        split\n    :type val_size: float\n    :param test_size: The proportion of the dataset to include in the test split\n    :type test_size: float\n    :param random_state: The random seed to use for splitting the data (keep this\n        consistent to ensure reproduceability)\n    :type random_state: int\n    :param bound_mean: The mean of the bound distribution\n    :type bound_mean: float\n    :param max_mean_adjustment: The maximum mean adjustment to apply to the mean\n                                of the bound (bound) perturbation effects\n    :type max_mean_adjustment: float\n    :param adjustment_function: A function that adjusts the mean of the bound\n                                (bound) perturbation effects\n    :type adjustment_function: Callable[[torch.Tensor, float, float,\n                               float, dict[int, list[int]]], torch.Tensor]\n    :raises TypeError: If batch_size is not an positive integer\n    :raises TypeError: If num_genes is not an positive integer\n    :raises TypeError: If bound is not a list of integers or floats\n    :raises TypeError: If n_sample is not a list of integers\n    :raises TypeError: If val_size is not a float between 0 and 1 (inclusive)\n    :raises TypeError: If test_size is not a float between 0 and 1 (inclusive)\n    :raises TypeError: If random_state is not an integer\n    :raises TypeError: If bound_mean is not a float\n    :raises ValueError: If val_size + test_size is greater than 1 (i.e. the splits\n        are too large)\n\n    \"\"\"\n    if not isinstance(batch_size, int) or batch_size &lt; 1:\n        raise TypeError(\"batch_size must be a positive integer\")\n    if not isinstance(num_genes, int) or num_genes &lt; 1:\n        raise TypeError(\"num_genes must be a positive integer\")\n    if not isinstance(bound, list) or not all(\n        isinstance(x, (int, float)) for x in bound\n    ):\n        raise TypeError(\"bound must be a list of integers or floats\")\n    if not isinstance(n_sample, list) or not all(\n        isinstance(x, int) for x in n_sample\n    ):\n        raise TypeError(\"n_sample must be a list of integers\")\n    if not isinstance(val_size, (int, float)) or val_size &lt;= 0 or val_size &gt;= 1:\n        raise TypeError(\"val_size must be a float between 0 and 1 (inclusive)\")\n    if not isinstance(test_size, (int, float)) or test_size &lt;= 0 or test_size &gt;= 1:\n        raise TypeError(\"test_size must be a float between 0 and 1 (inclusive)\")\n    if not isinstance(random_state, int):\n        raise TypeError(\"random_state must be an integer\")\n    if not isinstance(bound_mean, float):\n        raise TypeError(\"bound_mean must be a float\")\n    if test_size + val_size &gt; 1:\n        raise ValueError(\"val_size + test_size must be less than or equal to 1\")\n\n    super().__init__()\n    self.batch_size = batch_size\n    self.num_genes = num_genes\n    self.bound_mean = bound_mean\n    self.bound = bound or [0.1, 0.15, 0.2, 0.25, 0.3]\n    self.n_sample = n_sample or [1 for _ in range(len(self.bound))]\n    self.num_tfs = sum(self.n_sample)  # sum of all n_sample is the number of TFs\n    self.val_size = val_size\n    self.test_size = test_size\n    self.random_state = random_state\n\n    self.max_mean_adjustment = max_mean_adjustment\n    self.adjustment_function = adjustment_function\n    self.tf_relationships = tf_relationships\n\n    self.final_data_tensor: torch.Tensor = None\n    self.binding_effect_matrix: torch.Tensor | None = None\n    self.perturbation_effect_matrix: torch.Tensor | None = None\n    self.val_dataset: TensorDataset | None = None\n    self.test_dataset: TensorDataset | None = None\n</code></pre>"},{"location":"data_loaders/synthetic_data_loader/#yeastdnnexplorer.data_loaders.synthetic_data_loader.SyntheticDataLoader.prepare_data","title":"<code>prepare_data()</code>","text":"<p>Function to generate the raw synthetic data and save it in a tensor For explanations of the functions used to generate the data, see the generate_in_silico_data tutorial notebook in the docs No assertion checks are performed as that is handled in the functions in generate_data.py.</p> Source code in <code>yeastdnnexplorer/data_loaders/synthetic_data_loader.py</code> <pre><code>def prepare_data(self) -&gt; None:\n    \"\"\"Function to generate the raw synthetic data and save it in a tensor For\n    explanations of the functions used to generate the data, see the\n    generate_in_silico_data tutorial notebook in the docs No assertion checks are\n    performed as that is handled in the functions in generate_data.py.\"\"\"\n    # this will be a list of length 10 with a GenePopulation object in each element\n    gene_populations_list = []\n    for bound_proportion, n_draws in zip(self.bound, self.n_sample):\n        for _ in range(n_draws):\n            gene_populations_list.append(\n                generate_gene_population(self.num_genes, bound_proportion)\n            )\n\n    # Generate binding data for each gene population\n    binding_effect_list = [\n        generate_binding_effects(gene_population)\n        for gene_population in gene_populations_list\n    ]\n\n    # Calculate p-values for binding data\n    binding_pvalue_list = [\n        generate_pvalues(binding_data) for binding_data in binding_effect_list\n    ]\n\n    binding_data_combined = [\n        torch.stack((gene_population.labels, binding_effect, binding_pval), dim=1)\n        for gene_population, binding_effect, binding_pval in zip(\n            gene_populations_list, binding_effect_list, binding_pvalue_list\n        )\n    ]\n\n    # Stack along a new dimension (dim=1) to create a tensor of shape\n    # [num_genes, num_TFs, 3]\n    binding_data_tensor = torch.stack(binding_data_combined, dim=1)\n\n    # if we are using a mean adjustment, we need to generate perturbation\n    # effects in a slightly different way than if we are not using\n    # a mean adjustment\n    if self.max_mean_adjustment &gt; 0:\n        perturbation_effects_list = generate_perturbation_effects(\n            binding_data_tensor,\n            bound_mean=self.bound_mean,\n            tf_index=0,  # unused\n            max_mean_adjustment=self.max_mean_adjustment,\n            adjustment_function=self.adjustment_function,\n            tf_relationships=self.tf_relationships,\n        )\n\n        perturbation_pvalue_list = torch.zeros_like(perturbation_effects_list)\n        for col_index in range(perturbation_effects_list.shape[1]):\n            perturbation_pvalue_list[:, col_index] = generate_pvalues(\n                perturbation_effects_list[:, col_index]\n            )\n\n        # take absolute values\n        perturbation_effects_list = torch.abs(perturbation_effects_list)\n\n        perturbation_effects_tensor = perturbation_effects_list\n        perturbation_pvalues_tensor = perturbation_pvalue_list\n    else:\n        perturbation_effects_list = [\n            generate_perturbation_effects(\n                binding_data_tensor[:, tf_index, :].unsqueeze(1),\n                bound_mean=self.bound_mean,\n                tf_index=0,  # unused\n            )\n            for tf_index in range(sum(self.n_sample))\n        ]\n        perturbation_pvalue_list = [\n            generate_pvalues(perturbation_effects)\n            for perturbation_effects in perturbation_effects_list\n        ]\n\n        # take absolute values\n        perturbation_effects_list = [\n            torch.abs(perturbation_effects)\n            for perturbation_effects in perturbation_effects_list\n        ]\n\n        # Convert lists to tensors\n        perturbation_effects_tensor = torch.stack(perturbation_effects_list, dim=1)\n        perturbation_pvalues_tensor = torch.stack(perturbation_pvalue_list, dim=1)\n\n    # Ensure perturbation data is reshaped to match [n_genes, n_tfs]\n    # This step might need adjustment based on the actual shapes of your tensors.\n    perturbation_effects_tensor = perturbation_effects_tensor.unsqueeze(\n        -1\n    )  # Adds an extra dimension for concatenation\n    perturbation_pvalues_tensor = perturbation_pvalues_tensor.unsqueeze(\n        -1\n    )  # Adds an extra dimension for concatenation\n\n    # Concatenate along the last dimension to form a [n_genes, n_tfs, 5] tensor\n    self.final_data_tensor = torch.cat(\n        (\n            binding_data_tensor,\n            perturbation_effects_tensor,\n            perturbation_pvalues_tensor,\n        ),\n        dim=2,\n    )\n</code></pre>"},{"location":"data_loaders/synthetic_data_loader/#yeastdnnexplorer.data_loaders.synthetic_data_loader.SyntheticDataLoader.setup","title":"<code>setup(stage=None)</code>","text":"<p>This function runs after prepare_data finishes and is used to split the data into train, validation, and test sets It ensures that these datasets are of the correct dimensionality and size to be used by the model.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str | None</code> <p>The stage of the data setup (either \u2018fit\u2019 for training, \u2018validate\u2019 for validation, or \u2018test\u2019 for testing), unused for now as the model is not complicated enough to necessitate this</p> <code>None</code> Source code in <code>yeastdnnexplorer/data_loaders/synthetic_data_loader.py</code> <pre><code>def setup(self, stage: str | None = None) -&gt; None:\n    \"\"\"\n    This function runs after prepare_data finishes and is used to split the data\n    into train, validation, and test sets It ensures that these datasets are of the\n    correct dimensionality and size to be used by the model.\n\n    :param stage: The stage of the data setup (either 'fit' for training, 'validate'\n        for validation, or 'test' for testing), unused for now as the model is not\n        complicated enough to necessitate this\n    :type stage: Optional[str]\n\n    \"\"\"\n    self.binding_effect_matrix = self.final_data_tensor[:, :, 1]\n    self.perturbation_effect_matrix = self.final_data_tensor[:, :, 3]\n\n    # split into train, val, and test\n    X_train, X_temp, Y_train, Y_temp = train_test_split(\n        self.binding_effect_matrix,\n        self.perturbation_effect_matrix,\n        test_size=(self.val_size + self.test_size),\n        random_state=self.random_state,\n    )\n\n    # normalize test_size so that it is a percentage of the remaining data\n    self.test_size = self.test_size / (self.val_size + self.test_size)\n    X_val, X_test, Y_val, Y_test = train_test_split(\n        X_temp, Y_temp, test_size=self.test_size, random_state=self.random_state\n    )\n\n    # Convert to tensors\n    X_train, Y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(\n        Y_train, dtype=torch.float32\n    )\n    X_val, Y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(\n        Y_val, dtype=torch.float32\n    )\n    X_test, Y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(\n        Y_test, dtype=torch.float32\n    )\n\n    # Set our datasets\n    self.train_dataset = TensorDataset(X_train, Y_train)\n    self.val_dataset = TensorDataset(X_val, Y_val)\n    self.test_dataset = TensorDataset(X_test, Y_test)\n</code></pre>"},{"location":"data_loaders/synthetic_data_loader/#yeastdnnexplorer.data_loaders.synthetic_data_loader.SyntheticDataLoader.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Function to return the testing dataloader.</p> <p>Returns:</p> Type Description <code>DataLoader</code> <p>The testing dataloader</p> Source code in <code>yeastdnnexplorer/data_loaders/synthetic_data_loader.py</code> <pre><code>def test_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Function to return the testing dataloader.\n\n    :return: The testing dataloader\n    :rtype: DataLoader\n\n    \"\"\"\n    return DataLoader(\n        self.test_dataset,\n        batch_size=self.batch_size,\n        num_workers=15,\n        shuffle=False,\n        persistent_workers=True,\n    )\n</code></pre>"},{"location":"data_loaders/synthetic_data_loader/#yeastdnnexplorer.data_loaders.synthetic_data_loader.SyntheticDataLoader.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Function to return the training dataloader, we shuffle to avoid learning based on the order of the data.</p> <p>Returns:</p> Type Description <code>DataLoader</code> <p>The training dataloader</p> Source code in <code>yeastdnnexplorer/data_loaders/synthetic_data_loader.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Function to return the training dataloader, we shuffle to avoid learning based\n    on the order of the data.\n\n    :return: The training dataloader\n    :rtype: DataLoader\n\n    \"\"\"\n    return DataLoader(\n        self.train_dataset,\n        batch_size=self.batch_size,\n        num_workers=15,\n        shuffle=True,\n        persistent_workers=True,\n    )\n</code></pre>"},{"location":"data_loaders/synthetic_data_loader/#yeastdnnexplorer.data_loaders.synthetic_data_loader.SyntheticDataLoader.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Function to return the validation dataloader.</p> <p>Returns:</p> Type Description <code>DataLoader</code> <p>The validation dataloader</p> Source code in <code>yeastdnnexplorer/data_loaders/synthetic_data_loader.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Function to return the validation dataloader.\n\n    :return: The validation dataloader\n    :rtype: DataLoader\n\n    \"\"\"\n    return DataLoader(\n        self.val_dataset,\n        batch_size=self.batch_size,\n        num_workers=15,\n        shuffle=False,\n        persistent_workers=True,\n    )\n</code></pre>"},{"location":"interface/AbstractAPI/","title":"Developer Classes","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for creating API clients that require token authentication.</p> <p>This class provides a template for connecting to a cache for caching API responses, validating parameters against a list of valid keys, and provides an interface for CRUD operations.</p> Source code in <code>yeastdnnexplorer/interface/AbstractAPI.py</code> <pre><code>class AbstractAPI(ABC):\n    \"\"\"\n    Abstract base class for creating API clients that require token authentication.\n\n    This class provides a template for connecting to a cache for caching API responses,\n    validating parameters against a list of valid keys, and provides an interface for\n    CRUD operations.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        url: str = \"\",\n        token: str = \"\",\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize the API client.\n\n        :param url: The API endpoint URL. Defaults to the `BASE_URL`\n            environment variable.\n        :param token: The authentication token. Defaults to the `TOKEN`\n            environment variable.\n        :param valid_param_keys: A list of valid parameter keys for the API.\n        :param params: A ParamsDict object containing parameters for the API request.\n        :param cache: a Cache object for caching API responses.\n        :param kwargs: Additional keyword arguments that may be passed on to the\n            ParamsDict and Cache constructors.\n\n        \"\"\"\n        self.logger = logging.getLogger(__name__)\n        self.url = url or os.getenv(\"BASE_URL\", \"\")\n        self.token = token or os.getenv(\"TOKEN\", \"\")\n        self.params = ParamsDict(\n            params=kwargs.pop(\"params\", {}),\n            valid_keys=kwargs.pop(\"valid_keys\", []),\n        )\n        self.cache = Cache(\n            maxsize=kwargs.pop(\"maxsize\", 100), ttl=kwargs.pop(\"ttl\", 300)\n        )\n\n    @property\n    def header(self) -&gt; dict[str, str]:\n        \"\"\"The HTTP authorization header.\"\"\"\n        return {\n            \"Authorization\": f\"token {self.token}\",\n            \"Content-Type\": \"application/json\",\n        }\n\n    @property\n    def url(self) -&gt; str:\n        \"\"\"The URL for the API.\"\"\"\n        return self._url  # type: ignore\n\n    @url.setter\n    def url(self, value: str) -&gt; None:\n        if not value:\n            self._url = None\n        elif hasattr(self, \"token\") and self.token:\n            # validate the URL with the new token\n            self._is_valid_url(value)\n            self._url = value\n        else:\n            self.logger.warning(\"No token provided: URL un-validated\")\n            self._url = value\n\n    @property\n    def token(self) -&gt; str:\n        \"\"\"The authentication token for the API.\"\"\"\n        return self._token\n\n    @token.setter\n    def token(self, value: str) -&gt; None:\n        self._token = value\n        # validate the URL with the new token\n        if hasattr(self, \"url\") and self.url:\n            self.logger.info(\"Validating URL with new token\")\n            self._is_valid_url(self.url)\n\n    @property\n    def cache(self) -&gt; Cache:\n        \"\"\"The cache object for caching API responses.\"\"\"\n        return self._cache\n\n    @cache.setter\n    def cache(self, value: Cache) -&gt; None:\n        self._cache = value\n\n    @property\n    def params(self) -&gt; ParamsDict:\n        \"\"\"The ParamsDict object containing parameters for the API request.\"\"\"\n        return self._params\n\n    @params.setter\n    def params(self, value: ParamsDict) -&gt; None:\n        self._params = value\n\n    def push_params(self, params: dict[str, Any]) -&gt; None:\n        \"\"\"Adds or updates parameters in the ParamsDict.\"\"\"\n        self.params.update(params)\n\n    def pop_params(self, keys: list[str] | None = None) -&gt; None:\n        \"\"\"Removes parameters from the ParamsDict.\"\"\"\n        if keys is None:\n            self.params.clear()\n            return\n        if keys is not None and not isinstance(keys, list):\n            keys = [keys]\n        for key in keys:\n            del self.params[key]\n\n    @abstractmethod\n    def create(self, data: dict[str, Any], **kwargs) -&gt; Any:\n        \"\"\"Placeholder for the create method.\"\"\"\n        raise NotImplementedError(\n            f\"`create()` is not implemented for {self.__class__.__name__}\"\n        )\n\n    @abstractmethod\n    def read(self, **kwargs) -&gt; Any:\n        \"\"\"Placeholder for the read method.\"\"\"\n        raise NotImplementedError(\n            f\"`read()` is not implemented for {self.__class__.__name__}\"\n        )\n\n    @abstractmethod\n    def update(self, **kwargs) -&gt; Any:\n        \"\"\"Placeholder for the update method.\"\"\"\n        raise NotImplementedError(\n            f\"`update()` is not implemented for {self.__class__.__name__}\"\n        )\n\n    @abstractmethod\n    def delete(self, id: str, **kwargs) -&gt; Any:\n        \"\"\"Placeholder for the delete method.\"\"\"\n        raise NotImplementedError(\n            f\"`delete()` is not implemented for {self.__class__.__name__}\"\n        )\n\n    def _is_valid_url(self, url: str) -&gt; None:\n        \"\"\"\n        Confirms that the URL is valid and the header authorization is appropriate.\n\n        :param url: The URL to validate.\n        :type url: str\n        :raises ValueError: If the URL is invalid or the token is not set.\n\n        \"\"\"\n        try:\n            response = requests.head(url, headers=self.header, allow_redirects=True)\n            if response.status_code != 200:\n                raise ValueError(f\"Invalid URL or token provided: {response.content}\")\n        except requests.RequestException as e:\n            raise AttributeError(f\"Error validating URL: {e}\") from e\n        except AttributeError as e:\n            self.logger.error(f\"Error validating URL: {e}\")\n\n    def _cache_get(self, key: str, default: Any = None) -&gt; Any:\n        \"\"\"\n        Get a value from the cache if configured.\n\n        :param key: The key to retrieve from the cache.\n        :type key: str\n        :param default: The default value to return if the key is not found.\n        :type default: any, optional\n        :return: The value from the cache or the default value.\n        :rtype: any\n\n        \"\"\"\n        return self.cache.get(key, default=default)\n\n    def _cache_set(self, key: str, value: Any) -&gt; None:\n        \"\"\"\n        Set a value in the cache if configured.\n\n        :param key: The key to set in the cache.\n        :type key: str\n        :param value: The value to set in the cache.\n        :type value: any\n\n        \"\"\"\n        self.cache.set(key, value)\n\n    def _cache_list(self) -&gt; list[str]:\n        \"\"\"List keys in the cache if configured.\"\"\"\n        return self.cache.list()\n\n    def _cache_delete(self, key: str) -&gt; None:\n        \"\"\"\n        Delete a key from the cache if configured.\n\n        :param key: The key to delete from the cache.\n        :type key: str\n\n        \"\"\"\n        self.cache.delete(key)\n</code></pre>"},{"location":"interface/AbstractAPI/#yeastdnnexplorer.interface.AbstractAPI.AbstractAPI.cache","title":"<code>cache: Cache</code>  <code>property</code> <code>writable</code>","text":"<p>The cache object for caching API responses.</p>"},{"location":"interface/AbstractAPI/#yeastdnnexplorer.interface.AbstractAPI.AbstractAPI.header","title":"<code>header: dict[str, str]</code>  <code>property</code>","text":"<p>The HTTP authorization header.</p>"},{"location":"interface/AbstractAPI/#yeastdnnexplorer.interface.AbstractAPI.AbstractAPI.params","title":"<code>params: ParamsDict</code>  <code>property</code> <code>writable</code>","text":"<p>The ParamsDict object containing parameters for the API request.</p>"},{"location":"interface/AbstractAPI/#yeastdnnexplorer.interface.AbstractAPI.AbstractAPI.token","title":"<code>token: str</code>  <code>property</code> <code>writable</code>","text":"<p>The authentication token for the API.</p>"},{"location":"interface/AbstractAPI/#yeastdnnexplorer.interface.AbstractAPI.AbstractAPI.url","title":"<code>url: str</code>  <code>property</code> <code>writable</code>","text":"<p>The URL for the API.</p>"},{"location":"interface/AbstractAPI/#yeastdnnexplorer.interface.AbstractAPI.AbstractAPI.__init__","title":"<code>__init__(url='', token='', **kwargs)</code>","text":"<p>Initialize the API client.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The API endpoint URL. Defaults to the <code>BASE_URL</code> environment variable.</p> <code>''</code> <code>token</code> <code>str</code> <p>The authentication token. Defaults to the <code>TOKEN</code> environment variable.</p> <code>''</code> <code>valid_param_keys</code> <p>A list of valid parameter keys for the API.</p> required <code>params</code> <p>A ParamsDict object containing parameters for the API request.</p> required <code>cache</code> <p>a Cache object for caching API responses.</p> required <code>kwargs</code> <p>Additional keyword arguments that may be passed on to the ParamsDict and Cache constructors.</p> <code>{}</code> Source code in <code>yeastdnnexplorer/interface/AbstractAPI.py</code> <pre><code>def __init__(\n    self,\n    url: str = \"\",\n    token: str = \"\",\n    **kwargs,\n):\n    \"\"\"\n    Initialize the API client.\n\n    :param url: The API endpoint URL. Defaults to the `BASE_URL`\n        environment variable.\n    :param token: The authentication token. Defaults to the `TOKEN`\n        environment variable.\n    :param valid_param_keys: A list of valid parameter keys for the API.\n    :param params: A ParamsDict object containing parameters for the API request.\n    :param cache: a Cache object for caching API responses.\n    :param kwargs: Additional keyword arguments that may be passed on to the\n        ParamsDict and Cache constructors.\n\n    \"\"\"\n    self.logger = logging.getLogger(__name__)\n    self.url = url or os.getenv(\"BASE_URL\", \"\")\n    self.token = token or os.getenv(\"TOKEN\", \"\")\n    self.params = ParamsDict(\n        params=kwargs.pop(\"params\", {}),\n        valid_keys=kwargs.pop(\"valid_keys\", []),\n    )\n    self.cache = Cache(\n        maxsize=kwargs.pop(\"maxsize\", 100), ttl=kwargs.pop(\"ttl\", 300)\n    )\n</code></pre>"},{"location":"interface/AbstractAPI/#yeastdnnexplorer.interface.AbstractAPI.AbstractAPI.create","title":"<code>create(data, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Placeholder for the create method.</p> Source code in <code>yeastdnnexplorer/interface/AbstractAPI.py</code> <pre><code>@abstractmethod\ndef create(self, data: dict[str, Any], **kwargs) -&gt; Any:\n    \"\"\"Placeholder for the create method.\"\"\"\n    raise NotImplementedError(\n        f\"`create()` is not implemented for {self.__class__.__name__}\"\n    )\n</code></pre>"},{"location":"interface/AbstractAPI/#yeastdnnexplorer.interface.AbstractAPI.AbstractAPI.delete","title":"<code>delete(id, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Placeholder for the delete method.</p> Source code in <code>yeastdnnexplorer/interface/AbstractAPI.py</code> <pre><code>@abstractmethod\ndef delete(self, id: str, **kwargs) -&gt; Any:\n    \"\"\"Placeholder for the delete method.\"\"\"\n    raise NotImplementedError(\n        f\"`delete()` is not implemented for {self.__class__.__name__}\"\n    )\n</code></pre>"},{"location":"interface/AbstractAPI/#yeastdnnexplorer.interface.AbstractAPI.AbstractAPI.pop_params","title":"<code>pop_params(keys=None)</code>","text":"<p>Removes parameters from the ParamsDict.</p> Source code in <code>yeastdnnexplorer/interface/AbstractAPI.py</code> <pre><code>def pop_params(self, keys: list[str] | None = None) -&gt; None:\n    \"\"\"Removes parameters from the ParamsDict.\"\"\"\n    if keys is None:\n        self.params.clear()\n        return\n    if keys is not None and not isinstance(keys, list):\n        keys = [keys]\n    for key in keys:\n        del self.params[key]\n</code></pre>"},{"location":"interface/AbstractAPI/#yeastdnnexplorer.interface.AbstractAPI.AbstractAPI.push_params","title":"<code>push_params(params)</code>","text":"<p>Adds or updates parameters in the ParamsDict.</p> Source code in <code>yeastdnnexplorer/interface/AbstractAPI.py</code> <pre><code>def push_params(self, params: dict[str, Any]) -&gt; None:\n    \"\"\"Adds or updates parameters in the ParamsDict.\"\"\"\n    self.params.update(params)\n</code></pre>"},{"location":"interface/AbstractAPI/#yeastdnnexplorer.interface.AbstractAPI.AbstractAPI.read","title":"<code>read(**kwargs)</code>  <code>abstractmethod</code>","text":"<p>Placeholder for the read method.</p> Source code in <code>yeastdnnexplorer/interface/AbstractAPI.py</code> <pre><code>@abstractmethod\ndef read(self, **kwargs) -&gt; Any:\n    \"\"\"Placeholder for the read method.\"\"\"\n    raise NotImplementedError(\n        f\"`read()` is not implemented for {self.__class__.__name__}\"\n    )\n</code></pre>"},{"location":"interface/AbstractAPI/#yeastdnnexplorer.interface.AbstractAPI.AbstractAPI.update","title":"<code>update(**kwargs)</code>  <code>abstractmethod</code>","text":"<p>Placeholder for the update method.</p> Source code in <code>yeastdnnexplorer/interface/AbstractAPI.py</code> <pre><code>@abstractmethod\ndef update(self, **kwargs) -&gt; Any:\n    \"\"\"Placeholder for the update method.\"\"\"\n    raise NotImplementedError(\n        f\"`update()` is not implemented for {self.__class__.__name__}\"\n    )\n</code></pre>"},{"location":"interface/AbstractRecordsAndFilesAPI/","title":"AbstractRecordsAndFilesAPI","text":"<p>             Bases: <code>AbstractAPI</code></p> <p>Abstract class to interact with both the records and the data stored in the <code>file</code> field.</p> <p>The return for this class must be records, against the <code>/export</code> endpoint when <code>retrieve_files</code> is False. When <code>retrieve_files</code> is True, the cache should be checked first. If the file doesn\u2019t exist there, it should be retrieved from the database against the <code>/record_table_and_files</code> endpoint. The file should be a tarball with the metadata.csv and the file associated with the record, where the file is named according to the <code>id</code> field in metadata.csv. Data files should be <code>.csv.gz</code>.</p> Source code in <code>yeastdnnexplorer/interface/AbstractRecordsAndFilesAPI.py</code> <pre><code>class AbstractRecordsAndFilesAPI(AbstractAPI):\n    \"\"\"\n    Abstract class to interact with both the records and the data stored in the `file`\n    field.\n\n    The return for this class must be records, against the `/export`\n    endpoint when `retrieve_files` is False. When `retrieve_files` is True, the cache\n    should be checked first. If the file doesn't exist there, it should be retrieved\n    from the database against the `/record_table_and_files` endpoint. The file should\n    be a tarball with the metadata.csv and the file associated with the record,\n    where the file is named according to the `id` field in metadata.csv. Data files\n    should be `.csv.gz`.\n\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the AbstractRecordsAndFilesAPI object. This will serve as an\n        interface to an endpoint that can serve both records and files, and cache the\n        file/retrieve from the cache if it exists.\n\n        :param kwargs: parameters to pass to AbstractAPI.\n\n        \"\"\"\n        self.export_url_suffix = kwargs.pop(\"export_url_suffix\", \"export\")\n        self.export_files_url_suffix = kwargs.pop(\n            \"export_files_url_suffix\", \"record_table_and_files\"\n        )\n        super().__init__(**kwargs)\n\n    @property\n    def export_url_suffix(self) -&gt; str:\n        \"\"\"The URL suffix for exporting records.\"\"\"\n        return self._export_url_suffix\n\n    @export_url_suffix.setter\n    def export_url_suffix(self, value: str) -&gt; None:\n        self._export_url_suffix = value\n\n    @property\n    def export_files_url_suffix(self) -&gt; str:\n        \"\"\"The URL suffix for exporting files.\"\"\"\n        return self._export_files_url_suffix\n\n    @export_files_url_suffix.setter\n    def export_files_url_suffix(self, value: str) -&gt; None:\n        self._export_files_url_suffix = value\n\n    async def read(\n        self,\n        callback: Callable[\n            [pd.DataFrame, dict[str, Any] | None, Any], Any\n        ] = lambda metadata, data, cache, **kwargs: (\n            {\"metadata\": metadata, \"data\": data}\n        ),\n        retrieve_files: bool = False,\n        **kwargs,\n    ) -&gt; Any:\n        \"\"\"\n        Retrieve data from the endpoint according to the `retrieve_files` parameter. If\n        `retrieve_files` is False, the records will be returned as a dataframe. If\n        `retrieve_files` is True, the files associated with the records will be\n        retrieved either from the local cache or from the database.\n\n        :param callback: The function to call with the metadata. Signature must\n            include `metadata`, `data`, and `cache`.\n        :param retrieve_files: Boolean. Whether to retrieve the files associated with\n            the records. Defaults to False.\n        :param kwargs: Additional arguments to pass to the callback function.\n        :return: The result of the callback function.\n\n        \"\"\"\n        if not callable(callback) or {\"metadata\", \"data\", \"cache\"} - set(\n            callback.__code__.co_varnames\n        ):\n            raise ValueError(\n                \"The callback must be a callable function with `metadata`, `data`, \",\n                \"and `cache` as parameters.\",\n            )\n\n        export_url = f\"{self.url.rstrip('/')}/{self.export_url_suffix}\"\n        self.logger.debug(\"export_url: %s\", export_url)\n\n        async with aiohttp.ClientSession() as session:\n            try:\n                async with session.get(\n                    export_url, headers=self.header, params=self.params\n                ) as response:\n                    response.raise_for_status()\n                    text = await response.text()\n                    records_df = pd.read_csv(BytesIO(text.encode()))\n\n                    if not retrieve_files:\n                        return callback(records_df, None, self.cache, **kwargs)\n                    else:\n                        data_list = await self._retrieve_files(session, records_df)\n                        return callback(\n                            records_df,\n                            data_list,\n                            self.cache,\n                            **kwargs,\n                        )\n\n            except aiohttp.ClientError as e:\n                logging.error(f\"Error in GET request: {e}\")\n                raise\n            except pd.errors.ParserError as e:\n                logging.error(f\"Error reading request content: {e}\")\n                raise\n\n    async def _retrieve_files(\n        self, session: aiohttp.ClientSession, records_df: pd.DataFrame\n    ) -&gt; dict[str, pd.DataFrame]:\n        \"\"\"\n        Retrieve files associated with the records either from the local cache or from\n        the database.\n\n        :param session: The aiohttp ClientSession.\n        :param records_df: The DataFrame containing the records.\n        :return: A dictionary where the keys are record IDs and the values are\n            DataFrames of the associated files.\n\n        \"\"\"\n        data_list = {}\n        for record_id in records_df[\"id\"]:\n            data_list[str(record_id)] = await self._retrieve_file(session, record_id)\n        return data_list\n\n    async def _retrieve_file(\n        self, session: aiohttp.ClientSession, record_id: int\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Retrieve a file associated with a record either from the local cache or from the\n        database.\n\n        :param session: The aiohttp ClientSession.\n        :param record_id: The ID of the record.\n        :return: A DataFrame containing the file's data.\n\n        \"\"\"\n        export_files_url = f\"{self.url.rstrip('/')}/{self.export_files_url_suffix}\"\n        self.logger.debug(\"export_url: %s\", export_files_url)\n        # Try to get the data from the cache first\n        cache_key = str(record_id)\n        cached_data = self._cache_get(cache_key)\n        if cached_data is not None:\n            logging.info(f\"Record ID {record_id} retrieved from cache.\")\n            return pd.read_json(BytesIO(cached_data.encode()))\n\n        # Retrieve from the database if not in cache\n        logging.info(\n            f\"Record ID {record_id} not found in cache. Retrieving from the database.\"\n        )\n        try:\n            header = self.header.copy()\n            header[\"Content-Type\"] = \"application/gzip\"\n            async with session.get(\n                export_files_url, headers=header, params={\"id\": record_id}, timeout=120\n            ) as response:\n                response.raise_for_status()\n                tar_data = await response.read()\n\n            # Create a temporary file for the tarball\n            tar_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".tar.gz\")\n            try:\n                tar_file.write(tar_data)\n                tar_file.flush()\n                tar_file.seek(0)\n\n                # Create a temporary directory for extraction\n                with tempfile.TemporaryDirectory() as extract_dir:\n                    # Open the tar file and log its contents\n                    with tarfile.open(fileobj=tar_file, mode=\"r:gz\") as tar:\n                        tar_members = tar.getmembers()\n                        self.logger.debug(\n                            \"Tar file contains: \",\n                            \"{[member.name for member in tar_members]}\",\n                        )\n\n                        # Find the specific file to extract\n                        csv_filename = f\"{record_id}.csv.gz\"\n                        member = next(\n                            (m for m in tar_members if m.name == csv_filename), None\n                        )\n                        if member is None:\n                            raise FileNotFoundError(\n                                f\"{csv_filename} not found in tar archive\"\n                            )\n\n                        # Extract only the specific member\n                        tar.extract(member, path=extract_dir)\n\n                    # Read the extracted CSV file\n                    csv_path = os.path.join(extract_dir, csv_filename)\n                    df = pd.read_csv(csv_path)\n\n                    # Store the data in the cache\n                    self._cache_set(cache_key, df.to_json())\n            finally:\n                os.unlink(tar_file.name)\n\n            return df\n        except Exception as e:\n            logging.error(f\"Error retrieving file for record ID {record_id}: {e}\")\n            raise\n</code></pre>"},{"location":"interface/AbstractRecordsAndFilesAPI/#yeastdnnexplorer.interface.AbstractRecordsAndFilesAPI.AbstractRecordsAndFilesAPI.export_files_url_suffix","title":"<code>export_files_url_suffix: str</code>  <code>property</code> <code>writable</code>","text":"<p>The URL suffix for exporting files.</p>"},{"location":"interface/AbstractRecordsAndFilesAPI/#yeastdnnexplorer.interface.AbstractRecordsAndFilesAPI.AbstractRecordsAndFilesAPI.export_url_suffix","title":"<code>export_url_suffix: str</code>  <code>property</code> <code>writable</code>","text":"<p>The URL suffix for exporting records.</p>"},{"location":"interface/AbstractRecordsAndFilesAPI/#yeastdnnexplorer.interface.AbstractRecordsAndFilesAPI.AbstractRecordsAndFilesAPI.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the AbstractRecordsAndFilesAPI object. This will serve as an interface to an endpoint that can serve both records and files, and cache the file/retrieve from the cache if it exists.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>parameters to pass to AbstractAPI.</p> <code>{}</code> Source code in <code>yeastdnnexplorer/interface/AbstractRecordsAndFilesAPI.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the AbstractRecordsAndFilesAPI object. This will serve as an\n    interface to an endpoint that can serve both records and files, and cache the\n    file/retrieve from the cache if it exists.\n\n    :param kwargs: parameters to pass to AbstractAPI.\n\n    \"\"\"\n    self.export_url_suffix = kwargs.pop(\"export_url_suffix\", \"export\")\n    self.export_files_url_suffix = kwargs.pop(\n        \"export_files_url_suffix\", \"record_table_and_files\"\n    )\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"interface/AbstractRecordsAndFilesAPI/#yeastdnnexplorer.interface.AbstractRecordsAndFilesAPI.AbstractRecordsAndFilesAPI.read","title":"<code>read(callback=lambda , , : {'metadata': metadata, 'data': data}, retrieve_files=False, **kwargs)</code>  <code>async</code>","text":"<p>Retrieve data from the endpoint according to the <code>retrieve_files</code> parameter. If <code>retrieve_files</code> is False, the records will be returned as a dataframe. If <code>retrieve_files</code> is True, the files associated with the records will be retrieved either from the local cache or from the database.</p> <p>Parameters:</p> Name Type Description Default <code>callback</code> <code>Callable[[DataFrame, dict[str, Any] | None, Any], Any]</code> <p>The function to call with the metadata. Signature must include <code>metadata</code>, <code>data</code>, and <code>cache</code>.</p> <code>lambda , , : {'metadata': metadata, 'data': data}</code> <code>retrieve_files</code> <code>bool</code> <p>Boolean. Whether to retrieve the files associated with the records. Defaults to False.</p> <code>False</code> <code>kwargs</code> <p>Additional arguments to pass to the callback function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The result of the callback function.</p> Source code in <code>yeastdnnexplorer/interface/AbstractRecordsAndFilesAPI.py</code> <pre><code>async def read(\n    self,\n    callback: Callable[\n        [pd.DataFrame, dict[str, Any] | None, Any], Any\n    ] = lambda metadata, data, cache, **kwargs: (\n        {\"metadata\": metadata, \"data\": data}\n    ),\n    retrieve_files: bool = False,\n    **kwargs,\n) -&gt; Any:\n    \"\"\"\n    Retrieve data from the endpoint according to the `retrieve_files` parameter. If\n    `retrieve_files` is False, the records will be returned as a dataframe. If\n    `retrieve_files` is True, the files associated with the records will be\n    retrieved either from the local cache or from the database.\n\n    :param callback: The function to call with the metadata. Signature must\n        include `metadata`, `data`, and `cache`.\n    :param retrieve_files: Boolean. Whether to retrieve the files associated with\n        the records. Defaults to False.\n    :param kwargs: Additional arguments to pass to the callback function.\n    :return: The result of the callback function.\n\n    \"\"\"\n    if not callable(callback) or {\"metadata\", \"data\", \"cache\"} - set(\n        callback.__code__.co_varnames\n    ):\n        raise ValueError(\n            \"The callback must be a callable function with `metadata`, `data`, \",\n            \"and `cache` as parameters.\",\n        )\n\n    export_url = f\"{self.url.rstrip('/')}/{self.export_url_suffix}\"\n    self.logger.debug(\"export_url: %s\", export_url)\n\n    async with aiohttp.ClientSession() as session:\n        try:\n            async with session.get(\n                export_url, headers=self.header, params=self.params\n            ) as response:\n                response.raise_for_status()\n                text = await response.text()\n                records_df = pd.read_csv(BytesIO(text.encode()))\n\n                if not retrieve_files:\n                    return callback(records_df, None, self.cache, **kwargs)\n                else:\n                    data_list = await self._retrieve_files(session, records_df)\n                    return callback(\n                        records_df,\n                        data_list,\n                        self.cache,\n                        **kwargs,\n                    )\n\n        except aiohttp.ClientError as e:\n            logging.error(f\"Error in GET request: {e}\")\n            raise\n        except pd.errors.ParserError as e:\n            logging.error(f\"Error reading request content: {e}\")\n            raise\n</code></pre>"},{"location":"interface/AbstractRecordsOnlyAPI/","title":"AbstractRecordsOnlyAPI","text":"<p>             Bases: <code>AbstractAPI</code></p> <p>Abstract class for CRUD operations on records-only (no file storage) endpoints.</p> Source code in <code>yeastdnnexplorer/interface/AbstractRecordsOnlyAPI.py</code> <pre><code>class AbstractRecordsOnlyAPI(AbstractAPI):\n    \"\"\"Abstract class for CRUD operations on records-only (no file storage)\n    endpoints.\"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the RecordsOnlyAPI object.\n\n        :param kwargs: Additional parameters to pass to AbstractAPI.\n\n        \"\"\"\n        self.logger = logging.getLogger(__name__)\n        super().__init__(**kwargs)\n\n    async def read(\n        self,\n        callback: Callable[\n            [pd.DataFrame, dict[str, Any] | None, Any], Any\n        ] = lambda metadata, data, cache, **kwargs: {\n            \"metadata\": metadata,\n            \"data\": data,\n        },\n        export_url_suffix=\"export\",\n        **kwargs,\n    ) -&gt; Any:\n        \"\"\"\n        Retrieve data from the endpoint. The data will be returned as a dataframe. The\n        callback function must take metadata, data, and cache as parameters.\n\n        :param callback: The function to call with the data. Signature must\n            include `metadata`, `data`, and `cache` as parameters.\n        :param export_url_suffix: The URL suffix for the export endpoint. This will\n            return a response object with a csv file.\n        :param kwargs: Additional arguments to pass to the callback function.\n        :return: The result of the callback function.\n\n        \"\"\"\n        if not callable(callback) or {\"metadata\", \"data\", \"cache\"} - set(\n            callback.__code__.co_varnames\n        ):\n            raise ValueError(\n                \"The callback must be a callable function with `metadata`,\",\n                \"`data`, and `cache` as parameters.\",\n            )\n\n        export_url = f\"{self.url.rstrip('/')}/{export_url_suffix}\"\n        self.logger.debug(\"export_url: %s\", export_url)\n\n        async with aiohttp.ClientSession() as session:\n            try:\n                # note that the url and the export suffix are joined such that\n                # the url is stripped of any trailing slashes and the export suffix is\n                # added without a leading slash\n                async with session.get(\n                    export_url,\n                    headers=self.header,\n                    params=self.params,\n                ) as response:\n                    response.raise_for_status()\n                    text = await response.text()\n                    records_df = pd.read_csv(StringIO(text))\n                    return callback(records_df, None, self.cache, **kwargs)\n            except aiohttp.ClientError as e:\n                self.logger.error(f\"Error in GET request: {e}\")\n                raise\n            except pd.errors.ParserError as e:\n                self.logger.error(f\"Error reading request content: {e}\")\n                raise\n</code></pre>"},{"location":"interface/AbstractRecordsOnlyAPI/#yeastdnnexplorer.interface.AbstractRecordsOnlyAPI.AbstractRecordsOnlyAPI.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the RecordsOnlyAPI object.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>Additional parameters to pass to AbstractAPI.</p> <code>{}</code> Source code in <code>yeastdnnexplorer/interface/AbstractRecordsOnlyAPI.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the RecordsOnlyAPI object.\n\n    :param kwargs: Additional parameters to pass to AbstractAPI.\n\n    \"\"\"\n    self.logger = logging.getLogger(__name__)\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"interface/AbstractRecordsOnlyAPI/#yeastdnnexplorer.interface.AbstractRecordsOnlyAPI.AbstractRecordsOnlyAPI.read","title":"<code>read(callback=lambda , , : {'metadata': metadata, 'data': data}, export_url_suffix='export', **kwargs)</code>  <code>async</code>","text":"<p>Retrieve data from the endpoint. The data will be returned as a dataframe. The callback function must take metadata, data, and cache as parameters.</p> <p>Parameters:</p> Name Type Description Default <code>callback</code> <code>Callable[[DataFrame, dict[str, Any] | None, Any], Any]</code> <p>The function to call with the data. Signature must include <code>metadata</code>, <code>data</code>, and <code>cache</code> as parameters.</p> <code>lambda , , : {'metadata': metadata, 'data': data}</code> <code>export_url_suffix</code> <p>The URL suffix for the export endpoint. This will return a response object with a csv file.</p> <code>'export'</code> <code>kwargs</code> <p>Additional arguments to pass to the callback function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The result of the callback function.</p> Source code in <code>yeastdnnexplorer/interface/AbstractRecordsOnlyAPI.py</code> <pre><code>async def read(\n    self,\n    callback: Callable[\n        [pd.DataFrame, dict[str, Any] | None, Any], Any\n    ] = lambda metadata, data, cache, **kwargs: {\n        \"metadata\": metadata,\n        \"data\": data,\n    },\n    export_url_suffix=\"export\",\n    **kwargs,\n) -&gt; Any:\n    \"\"\"\n    Retrieve data from the endpoint. The data will be returned as a dataframe. The\n    callback function must take metadata, data, and cache as parameters.\n\n    :param callback: The function to call with the data. Signature must\n        include `metadata`, `data`, and `cache` as parameters.\n    :param export_url_suffix: The URL suffix for the export endpoint. This will\n        return a response object with a csv file.\n    :param kwargs: Additional arguments to pass to the callback function.\n    :return: The result of the callback function.\n\n    \"\"\"\n    if not callable(callback) or {\"metadata\", \"data\", \"cache\"} - set(\n        callback.__code__.co_varnames\n    ):\n        raise ValueError(\n            \"The callback must be a callable function with `metadata`,\",\n            \"`data`, and `cache` as parameters.\",\n        )\n\n    export_url = f\"{self.url.rstrip('/')}/{export_url_suffix}\"\n    self.logger.debug(\"export_url: %s\", export_url)\n\n    async with aiohttp.ClientSession() as session:\n        try:\n            # note that the url and the export suffix are joined such that\n            # the url is stripped of any trailing slashes and the export suffix is\n            # added without a leading slash\n            async with session.get(\n                export_url,\n                headers=self.header,\n                params=self.params,\n            ) as response:\n                response.raise_for_status()\n                text = await response.text()\n                records_df = pd.read_csv(StringIO(text))\n                return callback(records_df, None, self.cache, **kwargs)\n        except aiohttp.ClientError as e:\n            self.logger.error(f\"Error in GET request: {e}\")\n            raise\n        except pd.errors.ParserError as e:\n            self.logger.error(f\"Error reading request content: {e}\")\n            raise\n</code></pre>"},{"location":"interface/BindingAPI/","title":"Records and Files Classes","text":"<p>             Bases: <code>AbstractRecordsAndFilesAPI</code></p> <p>Class to interact with the BindingAPI endpoint.</p> Source code in <code>yeastdnnexplorer/interface/BindingAPI.py</code> <pre><code>class BindingAPI(AbstractRecordsAndFilesAPI):\n    \"\"\"Class to interact with the BindingAPI endpoint.\"\"\"\n\n    def __init__(self, **kwargs) -&gt; None:\n        \"\"\"\n        Initialize the BindingAPI object.\n\n        :param kwargs: parameters to pass through AbstractRecordsAndFilesAPI to\n            AbstractAPI.\n\n        \"\"\"\n        valid_param_keys = kwargs.pop(\n            \"valid_param_keys\",\n            [\n                \"id\",\n                \"regulator\",\n                \"regulator_locus_tag\",\n                \"regulator_symbol\",\n                \"batch\",\n                \"replicate\",\n                \"source\",\n                \"source_orig_id\",\n                \"strain\",\n                \"condition\",\n                \"lab\",\n                \"assay\",\n                \"workflow\",\n                \"data_usable\",\n            ],\n        )\n\n        url = kwargs.pop(\"url\", os.getenv(\"BINDING_URL\", None))\n\n        super().__init__(url=url, valid_param_keys=valid_param_keys, **kwargs)\n\n    def create(self):\n        pass\n\n    def update(self):\n        pass\n\n    def delete(self):\n        pass\n</code></pre>"},{"location":"interface/BindingAPI/#yeastdnnexplorer.interface.BindingAPI.BindingAPI.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the BindingAPI object.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>parameters to pass through AbstractRecordsAndFilesAPI to AbstractAPI.</p> <code>{}</code> Source code in <code>yeastdnnexplorer/interface/BindingAPI.py</code> <pre><code>def __init__(self, **kwargs) -&gt; None:\n    \"\"\"\n    Initialize the BindingAPI object.\n\n    :param kwargs: parameters to pass through AbstractRecordsAndFilesAPI to\n        AbstractAPI.\n\n    \"\"\"\n    valid_param_keys = kwargs.pop(\n        \"valid_param_keys\",\n        [\n            \"id\",\n            \"regulator\",\n            \"regulator_locus_tag\",\n            \"regulator_symbol\",\n            \"batch\",\n            \"replicate\",\n            \"source\",\n            \"source_orig_id\",\n            \"strain\",\n            \"condition\",\n            \"lab\",\n            \"assay\",\n            \"workflow\",\n            \"data_usable\",\n        ],\n    )\n\n    url = kwargs.pop(\"url\", os.getenv(\"BINDING_URL\", None))\n\n    super().__init__(url=url, valid_param_keys=valid_param_keys, **kwargs)\n</code></pre>"},{"location":"interface/BindingManualQCAPI/","title":"Records Only Classes","text":"<p>             Bases: <code>AbstractRecordsOnlyAPI</code></p> <p>A class to interact with the BindingManualQCAPI endpoint.</p> Source code in <code>yeastdnnexplorer/interface/BindingManualQCAPI.py</code> <pre><code>class BindingManualQCAPI(AbstractRecordsOnlyAPI):\n    \"\"\"A class to interact with the BindingManualQCAPI endpoint.\"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the BindingManualQCAPI object.\n\n        :param kwargs: parameters to pass to AbstractAPI via AbstractRecordsOnlyAPI.\n\n        \"\"\"\n        valid_param_keys = kwargs.pop(\n            \"valid_param_keys\",\n            [\n                \"id\",\n                \"binding\",\n                \"best_datatype\",\n                \"data_usable\",\n                \"passing_replicate\",\n                \"rank_recall\",\n                \"regulator\",\n                \"regulator_locus_tag\",\n                \"regulator_symbol\",\n                \"batch\",\n                \"source\",\n            ],\n        )\n\n        url = kwargs.pop(\"url\", os.getenv(\"BINDINGMANUALQC_URL\", None))\n        if not url:\n            raise AttributeError(\n                \"url must be provided or the environmental variable \",\n                \"`BINDINGMANUALQC_URL` must be set\",\n            )\n\n        super().__init__(url=url, valid_param_keys=valid_param_keys, **kwargs)\n\n    def create(self):\n        pass\n\n    def update(self):\n        pass\n\n    def delete(self):\n        pass\n</code></pre>"},{"location":"interface/BindingManualQCAPI/#yeastdnnexplorer.interface.BindingManualQCAPI.BindingManualQCAPI.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the BindingManualQCAPI object.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>parameters to pass to AbstractAPI via AbstractRecordsOnlyAPI.</p> <code>{}</code> Source code in <code>yeastdnnexplorer/interface/BindingManualQCAPI.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the BindingManualQCAPI object.\n\n    :param kwargs: parameters to pass to AbstractAPI via AbstractRecordsOnlyAPI.\n\n    \"\"\"\n    valid_param_keys = kwargs.pop(\n        \"valid_param_keys\",\n        [\n            \"id\",\n            \"binding\",\n            \"best_datatype\",\n            \"data_usable\",\n            \"passing_replicate\",\n            \"rank_recall\",\n            \"regulator\",\n            \"regulator_locus_tag\",\n            \"regulator_symbol\",\n            \"batch\",\n            \"source\",\n        ],\n    )\n\n    url = kwargs.pop(\"url\", os.getenv(\"BINDINGMANUALQC_URL\", None))\n    if not url:\n        raise AttributeError(\n            \"url must be provided or the environmental variable \",\n            \"`BINDINGMANUALQC_URL` must be set\",\n        )\n\n    super().__init__(url=url, valid_param_keys=valid_param_keys, **kwargs)\n</code></pre>"},{"location":"interface/Cache/","title":"Cache","text":"<p>A caching class that uses cachetools for TTL caching with an LRU eviction policy.</p> Source code in <code>yeastdnnexplorer/interface/Cache.py</code> <pre><code>class Cache:\n    \"\"\"A caching class that uses cachetools for TTL caching with an LRU eviction\n    policy.\"\"\"\n\n    def __init__(self, maxsize: int = 100, ttl: int = 300):\n        self.ttl_cache = TTLCache(maxsize=maxsize, ttl=ttl)\n        self.logger = logging.getLogger(__name__)\n\n    def get(self, key: str, default: Any = None) -&gt; Any:\n        \"\"\"Get a value from the cache.\"\"\"\n        return self.ttl_cache.get(key, default)\n\n    def set(self, key: str, value: Any) -&gt; None:\n        \"\"\"Set a value in the cache.\"\"\"\n        self.ttl_cache[key] = value\n\n    def list(self) -&gt; list[str]:\n        \"\"\"List all keys in the cache.\"\"\"\n        return list(self.ttl_cache.keys())\n\n    def delete(self, key: str) -&gt; None:\n        \"\"\"Delete a key from the cache.\"\"\"\n        self.ttl_cache.pop(key, None)\n</code></pre>"},{"location":"interface/Cache/#yeastdnnexplorer.interface.Cache.Cache.delete","title":"<code>delete(key)</code>","text":"<p>Delete a key from the cache.</p> Source code in <code>yeastdnnexplorer/interface/Cache.py</code> <pre><code>def delete(self, key: str) -&gt; None:\n    \"\"\"Delete a key from the cache.\"\"\"\n    self.ttl_cache.pop(key, None)\n</code></pre>"},{"location":"interface/Cache/#yeastdnnexplorer.interface.Cache.Cache.get","title":"<code>get(key, default=None)</code>","text":"<p>Get a value from the cache.</p> Source code in <code>yeastdnnexplorer/interface/Cache.py</code> <pre><code>def get(self, key: str, default: Any = None) -&gt; Any:\n    \"\"\"Get a value from the cache.\"\"\"\n    return self.ttl_cache.get(key, default)\n</code></pre>"},{"location":"interface/Cache/#yeastdnnexplorer.interface.Cache.Cache.list","title":"<code>list()</code>","text":"<p>List all keys in the cache.</p> Source code in <code>yeastdnnexplorer/interface/Cache.py</code> <pre><code>def list(self) -&gt; list[str]:\n    \"\"\"List all keys in the cache.\"\"\"\n    return list(self.ttl_cache.keys())\n</code></pre>"},{"location":"interface/Cache/#yeastdnnexplorer.interface.Cache.Cache.set","title":"<code>set(key, value)</code>","text":"<p>Set a value in the cache.</p> Source code in <code>yeastdnnexplorer/interface/Cache.py</code> <pre><code>def set(self, key: str, value: Any) -&gt; None:\n    \"\"\"Set a value in the cache.\"\"\"\n    self.ttl_cache[key] = value\n</code></pre>"},{"location":"interface/CallingCardsBackgroundAPI/","title":"CallingCardsBackgroundAPI","text":"<p>             Bases: <code>AbstractRecordsAndFilesAPI</code></p> <p>Class to interact with the CallingCardsBackgroundAPI endpoint.</p> Source code in <code>yeastdnnexplorer/interface/CallingCardsBackgroundAPI.py</code> <pre><code>class CallingCardsBackgroundAPI(AbstractRecordsAndFilesAPI):\n    \"\"\"Class to interact with the CallingCardsBackgroundAPI endpoint.\"\"\"\n\n    def __init__(self, **kwargs) -&gt; None:\n        \"\"\"\n        Initialize the CallingCardsBackgroundAPI object.\n\n        :param kwargs: parameters to pass through AbstractRecordsAndFilesAPI to\n            AbstractAPI.\n\n        \"\"\"\n        valid_param_keys = kwargs.pop(\n            \"valid_param_keys\",\n            [\"id\", \"name\"],\n        )\n\n        url = kwargs.pop(\"url\", os.getenv(\"CALLINGCARDSBACKGROUND_URL\", None))\n\n        super().__init__(url=url, valid_param_keys=valid_param_keys, **kwargs)\n\n    def create(self):\n        pass\n\n    def update(self):\n        pass\n\n    def delete(self):\n        pass\n</code></pre>"},{"location":"interface/CallingCardsBackgroundAPI/#yeastdnnexplorer.interface.CallingCardsBackgroundAPI.CallingCardsBackgroundAPI.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the CallingCardsBackgroundAPI object.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>parameters to pass through AbstractRecordsAndFilesAPI to AbstractAPI.</p> <code>{}</code> Source code in <code>yeastdnnexplorer/interface/CallingCardsBackgroundAPI.py</code> <pre><code>def __init__(self, **kwargs) -&gt; None:\n    \"\"\"\n    Initialize the CallingCardsBackgroundAPI object.\n\n    :param kwargs: parameters to pass through AbstractRecordsAndFilesAPI to\n        AbstractAPI.\n\n    \"\"\"\n    valid_param_keys = kwargs.pop(\n        \"valid_param_keys\",\n        [\"id\", \"name\"],\n    )\n\n    url = kwargs.pop(\"url\", os.getenv(\"CALLINGCARDSBACKGROUND_URL\", None))\n\n    super().__init__(url=url, valid_param_keys=valid_param_keys, **kwargs)\n</code></pre>"},{"location":"interface/DataSourceAPI/","title":"DataSourceAPI","text":"<p>             Bases: <code>AbstractRecordsOnlyAPI</code></p> <p>A class to interact with the DataSourceAPI endpoint.</p> Source code in <code>yeastdnnexplorer/interface/DataSourceAPI.py</code> <pre><code>class DataSourceAPI(AbstractRecordsOnlyAPI):\n    \"\"\"A class to interact with the DataSourceAPI endpoint.\"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the DataSourceAPI object.\n\n        :param kwargs: parameters to pass to AbstractAPI via AbstractRecordsOnlyAPI.\n\n        \"\"\"\n        valid_param_keys = kwargs.pop(\n            \"valid_param_keys\",\n            [\"id\", \"fileformat_id\", \"fileformat\", \"lab\", \"assay\", \"workflow\"],\n        )\n\n        url = kwargs.pop(\"url\", os.getenv(\"DATASOURCE_URL\", None))\n        if not url:\n            raise AttributeError(\n                \"url must be provided or the environmental variable \",\n                \"`DATASOURCE_URL` must be set\",\n            )\n\n        super().__init__(url=url, valid_param_keys=valid_param_keys, **kwargs)\n\n    def create(self):\n        pass\n\n    def update(self):\n        pass\n\n    def delete(self):\n        pass\n</code></pre>"},{"location":"interface/DataSourceAPI/#yeastdnnexplorer.interface.DataSourceAPI.DataSourceAPI.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the DataSourceAPI object.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>parameters to pass to AbstractAPI via AbstractRecordsOnlyAPI.</p> <code>{}</code> Source code in <code>yeastdnnexplorer/interface/DataSourceAPI.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the DataSourceAPI object.\n\n    :param kwargs: parameters to pass to AbstractAPI via AbstractRecordsOnlyAPI.\n\n    \"\"\"\n    valid_param_keys = kwargs.pop(\n        \"valid_param_keys\",\n        [\"id\", \"fileformat_id\", \"fileformat\", \"lab\", \"assay\", \"workflow\"],\n    )\n\n    url = kwargs.pop(\"url\", os.getenv(\"DATASOURCE_URL\", None))\n    if not url:\n        raise AttributeError(\n            \"url must be provided or the environmental variable \",\n            \"`DATASOURCE_URL` must be set\",\n        )\n\n    super().__init__(url=url, valid_param_keys=valid_param_keys, **kwargs)\n</code></pre>"},{"location":"interface/ExpressionAPI/","title":"ExpressionAPI","text":"<p>             Bases: <code>AbstractRecordsAndFilesAPI</code></p> <p>Class to interact with the ExpressionAPI endpoint.</p> Source code in <code>yeastdnnexplorer/interface/ExpressionAPI.py</code> <pre><code>class ExpressionAPI(AbstractRecordsAndFilesAPI):\n    \"\"\"Class to interact with the ExpressionAPI endpoint.\"\"\"\n\n    def __init__(self, **kwargs) -&gt; None:\n        \"\"\"\n        Initialize the ExpressionAPI object.\n\n        :param kwargs: parameters to pass through AbstractRecordsAndFilesAPI to\n            AbstractAPI.\n\n        \"\"\"\n        valid_param_keys = kwargs.pop(\n            \"valid_param_keys\",\n            [\n                \"id\",\n                \"regulator\",\n                \"regulator_locus_tag\",\n                \"regulator_symbol\",\n                \"batch\",\n                \"control\",\n                \"mechanism\",\n                \"restriction\",\n                \"time\",\n                \"source\",\n                \"source_time\",\n                \"lab\",\n                \"assay\",\n                \"workflow\",\n            ],\n        )\n\n        url = kwargs.pop(\"url\", os.getenv(\"EXPRESSION_URL\", None))\n\n        super().__init__(url=url, valid_param_keys=valid_param_keys, **kwargs)\n\n    def create(self):\n        pass\n\n    def update(self):\n        pass\n\n    def delete(self):\n        pass\n</code></pre>"},{"location":"interface/ExpressionAPI/#yeastdnnexplorer.interface.ExpressionAPI.ExpressionAPI.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the ExpressionAPI object.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>parameters to pass through AbstractRecordsAndFilesAPI to AbstractAPI.</p> <code>{}</code> Source code in <code>yeastdnnexplorer/interface/ExpressionAPI.py</code> <pre><code>def __init__(self, **kwargs) -&gt; None:\n    \"\"\"\n    Initialize the ExpressionAPI object.\n\n    :param kwargs: parameters to pass through AbstractRecordsAndFilesAPI to\n        AbstractAPI.\n\n    \"\"\"\n    valid_param_keys = kwargs.pop(\n        \"valid_param_keys\",\n        [\n            \"id\",\n            \"regulator\",\n            \"regulator_locus_tag\",\n            \"regulator_symbol\",\n            \"batch\",\n            \"control\",\n            \"mechanism\",\n            \"restriction\",\n            \"time\",\n            \"source\",\n            \"source_time\",\n            \"lab\",\n            \"assay\",\n            \"workflow\",\n        ],\n    )\n\n    url = kwargs.pop(\"url\", os.getenv(\"EXPRESSION_URL\", None))\n\n    super().__init__(url=url, valid_param_keys=valid_param_keys, **kwargs)\n</code></pre>"},{"location":"interface/ExpressionManualQCAPI/","title":"ExpressionManualQCAPI","text":"<p>             Bases: <code>AbstractRecordsOnlyAPI</code></p> <p>A class to interact with the ExpressionManualQCAPI endpoint.</p> Source code in <code>yeastdnnexplorer/interface/ExpressionManualQCAPI.py</code> <pre><code>class ExpressionManualQCAPI(AbstractRecordsOnlyAPI):\n    \"\"\"A class to interact with the ExpressionManualQCAPI endpoint.\"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the ExpressionManualQCAPI object.\n\n        :param kwargs: parameters to pass to AbstractAPI via AbstractRecordsOnlyAPI.\n\n        \"\"\"\n        valid_param_keys = kwargs.pop(\n            \"valid_param_keys\",\n            [\n                \"id\",\n                \"expression\",\n                \"strain_verified\",\n                \"regulator_locus_tag\",\n                \"regulator_symbol\",\n                \"batch\",\n                \"replicate\",\n                \"control\",\n                \"mechanism\",\n                \"restriction\",\n                \"time\",\n                \"source\",\n                \"lab\",\n                \"assay\",\n                \"workflow\",\n            ],\n        )\n\n        url = kwargs.pop(\"url\", os.getenv(\"EXPRESSIONMANUALQC_URL\", None))\n        if not url:\n            raise AttributeError(\n                \"url must be provided or the environmental variable \",\n                \"`EXPRESSIONMANUALQC_URL` must be set\",\n            )\n\n        super().__init__(url=url, valid_param_keys=valid_param_keys, **kwargs)\n\n    def create(self):\n        pass\n\n    def update(self):\n        pass\n\n    def delete(self):\n        pass\n</code></pre>"},{"location":"interface/ExpressionManualQCAPI/#yeastdnnexplorer.interface.ExpressionManualQCAPI.ExpressionManualQCAPI.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the ExpressionManualQCAPI object.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>parameters to pass to AbstractAPI via AbstractRecordsOnlyAPI.</p> <code>{}</code> Source code in <code>yeastdnnexplorer/interface/ExpressionManualQCAPI.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the ExpressionManualQCAPI object.\n\n    :param kwargs: parameters to pass to AbstractAPI via AbstractRecordsOnlyAPI.\n\n    \"\"\"\n    valid_param_keys = kwargs.pop(\n        \"valid_param_keys\",\n        [\n            \"id\",\n            \"expression\",\n            \"strain_verified\",\n            \"regulator_locus_tag\",\n            \"regulator_symbol\",\n            \"batch\",\n            \"replicate\",\n            \"control\",\n            \"mechanism\",\n            \"restriction\",\n            \"time\",\n            \"source\",\n            \"lab\",\n            \"assay\",\n            \"workflow\",\n        ],\n    )\n\n    url = kwargs.pop(\"url\", os.getenv(\"EXPRESSIONMANUALQC_URL\", None))\n    if not url:\n        raise AttributeError(\n            \"url must be provided or the environmental variable \",\n            \"`EXPRESSIONMANUALQC_URL` must be set\",\n        )\n\n    super().__init__(url=url, valid_param_keys=valid_param_keys, **kwargs)\n</code></pre>"},{"location":"interface/FileFormatAPI/","title":"FileFormatAPI","text":"<p>             Bases: <code>AbstractRecordsOnlyAPI</code></p> <p>A class to interact with the FileFormatAPI endpoint.</p> Source code in <code>yeastdnnexplorer/interface/FileFormatAPI.py</code> <pre><code>class FileFormatAPI(AbstractRecordsOnlyAPI):\n    \"\"\"A class to interact with the FileFormatAPI endpoint.\"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the FileFormatAPI object.\n\n        :param kwargs: parameters to pass to AbstractAPI via AbstractRecordsOnlyAPI.\n\n        \"\"\"\n        valid_param_keys = kwargs.pop(\n            \"valid_param_keys\",\n            [\n                \"fileformat\",\n                \"fields\",\n                \"separator\",\n                \"feature_identifier_col\",\n                \"effect_col\",\n                \"default_effect_threshold\",\n                \"pval_col\",\n                \"default_pvalue_threshold\",\n            ],\n        )\n\n        url = kwargs.pop(\"url\", os.getenv(\"FILEFORMAT_URL\", None))\n        if not url:\n            raise AttributeError(\n                \"url must be provided or the environmental variable \",\n                \"`FILEFORMAT_URL` must be set\",\n            )\n\n        super().__init__(url=url, valid_param_keys=valid_param_keys, **kwargs)\n\n    def create(self):\n        pass\n\n    def update(self):\n        pass\n\n    def delete(self):\n        pass\n</code></pre>"},{"location":"interface/FileFormatAPI/#yeastdnnexplorer.interface.FileFormatAPI.FileFormatAPI.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the FileFormatAPI object.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>parameters to pass to AbstractAPI via AbstractRecordsOnlyAPI.</p> <code>{}</code> Source code in <code>yeastdnnexplorer/interface/FileFormatAPI.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the FileFormatAPI object.\n\n    :param kwargs: parameters to pass to AbstractAPI via AbstractRecordsOnlyAPI.\n\n    \"\"\"\n    valid_param_keys = kwargs.pop(\n        \"valid_param_keys\",\n        [\n            \"fileformat\",\n            \"fields\",\n            \"separator\",\n            \"feature_identifier_col\",\n            \"effect_col\",\n            \"default_effect_threshold\",\n            \"pval_col\",\n            \"default_pvalue_threshold\",\n        ],\n    )\n\n    url = kwargs.pop(\"url\", os.getenv(\"FILEFORMAT_URL\", None))\n    if not url:\n        raise AttributeError(\n            \"url must be provided or the environmental variable \",\n            \"`FILEFORMAT_URL` must be set\",\n        )\n\n    super().__init__(url=url, valid_param_keys=valid_param_keys, **kwargs)\n</code></pre>"},{"location":"interface/GenomicFeatureAPI/","title":"GenomicFeatureAPI","text":"<p>             Bases: <code>AbstractRecordsOnlyAPI</code></p> <p>A class to interact with the GenomicFeatureAPI endpoint.</p> Source code in <code>yeastdnnexplorer/interface/GenomicFeatureAPI.py</code> <pre><code>class GenomicFeatureAPI(AbstractRecordsOnlyAPI):\n    \"\"\"A class to interact with the GenomicFeatureAPI endpoint.\"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the GenomicFeatureAPI object.\n\n        :param kwargs: parameters to pass to AbstractAPI via AbstractRecordsOnlyAPI.\n\n        \"\"\"\n        valid_param_keys = kwargs.pop(\n            \"valid_param_keys\",\n            [\n                \"id\",\n                \"chr\",\n                \"start\",\n                \"end\",\n                \"strand\",\n                \"type\",\n                \"locus_tag\",\n                \"symbol\",\n                \"source\",\n                \"alias\",\n                \"note\",\n            ],\n        )\n\n        url = kwargs.pop(\"url\", os.getenv(\"GENOMICFEATURE_URL\", None))\n        if not url:\n            raise AttributeError(\n                \"url must be provided or the environmental variable \",\n                \"`GENOMICFEATURE_URL` must be set\",\n            )\n\n        super().__init__(url=url, valid_param_keys=valid_param_keys, **kwargs)\n\n    def create(self):\n        pass\n\n    def update(self):\n        pass\n\n    def delete(self):\n        pass\n</code></pre>"},{"location":"interface/GenomicFeatureAPI/#yeastdnnexplorer.interface.GenomicFeatureAPI.GenomicFeatureAPI.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the GenomicFeatureAPI object.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>parameters to pass to AbstractAPI via AbstractRecordsOnlyAPI.</p> <code>{}</code> Source code in <code>yeastdnnexplorer/interface/GenomicFeatureAPI.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the GenomicFeatureAPI object.\n\n    :param kwargs: parameters to pass to AbstractAPI via AbstractRecordsOnlyAPI.\n\n    \"\"\"\n    valid_param_keys = kwargs.pop(\n        \"valid_param_keys\",\n        [\n            \"id\",\n            \"chr\",\n            \"start\",\n            \"end\",\n            \"strand\",\n            \"type\",\n            \"locus_tag\",\n            \"symbol\",\n            \"source\",\n            \"alias\",\n            \"note\",\n        ],\n    )\n\n    url = kwargs.pop(\"url\", os.getenv(\"GENOMICFEATURE_URL\", None))\n    if not url:\n        raise AttributeError(\n            \"url must be provided or the environmental variable \",\n            \"`GENOMICFEATURE_URL` must be set\",\n        )\n\n    super().__init__(url=url, valid_param_keys=valid_param_keys, **kwargs)\n</code></pre>"},{"location":"interface/ParamsDict/","title":"ParamsDict","text":"<p>             Bases: <code>dict</code></p> <p>A dictionary subclass that ensures all keys are strings and supports multiple key- value assignments at once, with validation against a list of valid keys.</p> <p>This class is designed to be used for passing parameters to HTTP requests and extends the base dictionary class, ensuring that insertion order is preserved.</p> Source code in <code>yeastdnnexplorer/interface/ParamsDict.py</code> <pre><code>class ParamsDict(dict):\n    \"\"\"\n    A dictionary subclass that ensures all keys are strings and supports multiple key-\n    value assignments at once, with validation against a list of valid keys.\n\n    This class is designed to be used for passing parameters to HTTP requests and\n    extends the base dictionary class, ensuring that insertion order is preserved.\n\n    \"\"\"\n\n    def __init__(self, params: dict[str, Any] = {}, valid_keys: list[str] = []) -&gt; None:\n        \"\"\"\n        Initialize the ParamsDict with optional initial parameters and valid keys.\n\n        :param params: A dictionary of initial parameters. All keys must be strings.\n        :type params: dict, optional\n        :param valid_keys: A list of valid keys for validation.\n        :type valid_keys: list of str, optional\n        :raises ValueError: If `params` is not a dictionary or if any of the keys\n            are not strings.\n\n        \"\"\"\n        params = params or {}\n        valid_keys = valid_keys or []\n        if not isinstance(params, dict):\n            raise ValueError(\"params must be a dictionary\")\n        if len(params) &gt; 0 and not all(isinstance(k, str) for k in params.keys()):\n            raise ValueError(\"params must be a dictionary with string keys\")\n        super().__init__(params)\n        self._valid_keys = valid_keys\n\n    def __setitem__(self, key: str | list[str], value: Any | list[Any]) -&gt; None:\n        \"\"\"\n        Set a parameter value or multiple parameter values.\n\n        :param key: The parameter key or a list of parameter keys.\n        :type key: str or list of str\n        :param value: The parameter value or a list of parameter values.\n        :type value: any or list of any\n        :raises ValueError: If the length of `key` and `value` lists do not match.\n        :raises KeyError: If `key` is not a string or a list of strings.\n\n        \"\"\"\n        if isinstance(key, str):\n            self._validate_key(key)\n            super().__setitem__(key, value)\n        elif isinstance(key, list) and isinstance(value, list):\n            if len(key) != len(value):\n                raise ValueError(\"Length of keys and values must match\")\n            for k, v in zip(key, value):\n                if not isinstance(k, str):\n                    raise KeyError(\"All keys must be strings\")\n                self._validate_key(k)\n                super().__setitem__(k, v)\n        else:\n            raise KeyError(\"Key must be a string or list of strings\")\n\n    def __getitem__(self, key: str | list[str]) -&gt; Union[Any, \"ParamsDict\"]:\n        \"\"\"\n        Get a parameter value or a new ParamsDict with specified keys.\n\n        :param key: The parameter key or a list of parameter keys.\n        :type key: str or list of str\n        :return: The parameter value or a new ParamsDict with the specified keys.\n        :rtype: any or ParamsDict\n        :raises KeyError: If `key` is not a string or a list of strings.\n\n        \"\"\"\n        if isinstance(key, str):\n            return super().__getitem__(key)\n        elif isinstance(key, list):\n            return ParamsDict({k: dict.__getitem__(self, k) for k in key if k in self})\n        else:\n            raise KeyError(\"Key must be a string or list of strings\")\n\n    def __delitem__(self, key: str) -&gt; None:\n        \"\"\"\n        Delete a parameter by key.\n\n        :param key: The parameter key.\n        :type key: str\n        :raises KeyError: If `key` is not a string.\n\n        \"\"\"\n        if isinstance(key, str):\n            super().__delitem__(key)\n        else:\n            raise KeyError(\"Key must be a string\")\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Return a string representation of the ParamsDict.\n\n        :return: A string representation of the ParamsDict.\n        :rtype: str\n\n        \"\"\"\n        return f\"ParamsDict({super().__repr__()})\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"\n        Return a human-readable string representation of the ParamsDict.\n\n        :return: A human-readable string representation of the ParamsDict.\n        :rtype: str\n\n        \"\"\"\n        return \", \".join(f\"{k}: {v}\" for k, v in self.items())\n\n    def as_dict(self) -&gt; dict:\n        \"\"\"\n        Convert the ParamsDict to a standard dictionary.\n\n        :return: A standard dictionary with the same items as the ParamsDict.\n        :rtype: dict\n\n        \"\"\"\n        return dict(self)\n\n    def _validate_key(self, key: str) -&gt; None:\n        \"\"\"Validate that the key is in the list of valid keys.\"\"\"\n        if self._valid_keys and key not in self._valid_keys:\n            raise ValueError(f\"Invalid parameter key provided: {key}\")\n\n    @property\n    def valid_keys(self) -&gt; list[str]:\n        \"\"\"Get the list of valid keys.\"\"\"\n        return self._valid_keys\n\n    @valid_keys.setter\n    def valid_keys(self, keys: list[str]) -&gt; None:\n        \"\"\"Set the list of valid keys.\"\"\"\n        if not all(isinstance(k, str) for k in keys):\n            raise ValueError(\"valid_keys must be a list of strings\")\n        self._valid_keys = keys\n</code></pre>"},{"location":"interface/ParamsDict/#yeastdnnexplorer.interface.ParamsDict.ParamsDict.valid_keys","title":"<code>valid_keys: list[str]</code>  <code>property</code> <code>writable</code>","text":"<p>Get the list of valid keys.</p>"},{"location":"interface/ParamsDict/#yeastdnnexplorer.interface.ParamsDict.ParamsDict.__delitem__","title":"<code>__delitem__(key)</code>","text":"<p>Delete a parameter by key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The parameter key.</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>If <code>key</code> is not a string.</p> Source code in <code>yeastdnnexplorer/interface/ParamsDict.py</code> <pre><code>def __delitem__(self, key: str) -&gt; None:\n    \"\"\"\n    Delete a parameter by key.\n\n    :param key: The parameter key.\n    :type key: str\n    :raises KeyError: If `key` is not a string.\n\n    \"\"\"\n    if isinstance(key, str):\n        super().__delitem__(key)\n    else:\n        raise KeyError(\"Key must be a string\")\n</code></pre>"},{"location":"interface/ParamsDict/#yeastdnnexplorer.interface.ParamsDict.ParamsDict.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Get a parameter value or a new ParamsDict with specified keys.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str | list[str]</code> <p>The parameter key or a list of parameter keys.</p> required <p>Returns:</p> Type Description <code>any | ParamsDict</code> <p>The parameter value or a new ParamsDict with the specified keys.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If <code>key</code> is not a string or a list of strings.</p> Source code in <code>yeastdnnexplorer/interface/ParamsDict.py</code> <pre><code>def __getitem__(self, key: str | list[str]) -&gt; Union[Any, \"ParamsDict\"]:\n    \"\"\"\n    Get a parameter value or a new ParamsDict with specified keys.\n\n    :param key: The parameter key or a list of parameter keys.\n    :type key: str or list of str\n    :return: The parameter value or a new ParamsDict with the specified keys.\n    :rtype: any or ParamsDict\n    :raises KeyError: If `key` is not a string or a list of strings.\n\n    \"\"\"\n    if isinstance(key, str):\n        return super().__getitem__(key)\n    elif isinstance(key, list):\n        return ParamsDict({k: dict.__getitem__(self, k) for k in key if k in self})\n    else:\n        raise KeyError(\"Key must be a string or list of strings\")\n</code></pre>"},{"location":"interface/ParamsDict/#yeastdnnexplorer.interface.ParamsDict.ParamsDict.__init__","title":"<code>__init__(params={}, valid_keys=[])</code>","text":"<p>Initialize the ParamsDict with optional initial parameters and valid keys.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict[str, Any]</code> <p>A dictionary of initial parameters. All keys must be strings.</p> <code>{}</code> <code>valid_keys</code> <code>list[str]</code> <p>A list of valid keys for validation.</p> <code>[]</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>params</code> is not a dictionary or if any of the keys are not strings.</p> Source code in <code>yeastdnnexplorer/interface/ParamsDict.py</code> <pre><code>def __init__(self, params: dict[str, Any] = {}, valid_keys: list[str] = []) -&gt; None:\n    \"\"\"\n    Initialize the ParamsDict with optional initial parameters and valid keys.\n\n    :param params: A dictionary of initial parameters. All keys must be strings.\n    :type params: dict, optional\n    :param valid_keys: A list of valid keys for validation.\n    :type valid_keys: list of str, optional\n    :raises ValueError: If `params` is not a dictionary or if any of the keys\n        are not strings.\n\n    \"\"\"\n    params = params or {}\n    valid_keys = valid_keys or []\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be a dictionary\")\n    if len(params) &gt; 0 and not all(isinstance(k, str) for k in params.keys()):\n        raise ValueError(\"params must be a dictionary with string keys\")\n    super().__init__(params)\n    self._valid_keys = valid_keys\n</code></pre>"},{"location":"interface/ParamsDict/#yeastdnnexplorer.interface.ParamsDict.ParamsDict.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a string representation of the ParamsDict.</p> <p>Returns:</p> Type Description <code>str</code> <p>A string representation of the ParamsDict.</p> Source code in <code>yeastdnnexplorer/interface/ParamsDict.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Return a string representation of the ParamsDict.\n\n    :return: A string representation of the ParamsDict.\n    :rtype: str\n\n    \"\"\"\n    return f\"ParamsDict({super().__repr__()})\"\n</code></pre>"},{"location":"interface/ParamsDict/#yeastdnnexplorer.interface.ParamsDict.ParamsDict.__setitem__","title":"<code>__setitem__(key, value)</code>","text":"<p>Set a parameter value or multiple parameter values.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str | list[str]</code> <p>The parameter key or a list of parameter keys.</p> required <code>value</code> <code>Any | list[Any]</code> <p>The parameter value or a list of parameter values.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the length of <code>key</code> and <code>value</code> lists do not match.</p> <code>KeyError</code> <p>If <code>key</code> is not a string or a list of strings.</p> Source code in <code>yeastdnnexplorer/interface/ParamsDict.py</code> <pre><code>def __setitem__(self, key: str | list[str], value: Any | list[Any]) -&gt; None:\n    \"\"\"\n    Set a parameter value or multiple parameter values.\n\n    :param key: The parameter key or a list of parameter keys.\n    :type key: str or list of str\n    :param value: The parameter value or a list of parameter values.\n    :type value: any or list of any\n    :raises ValueError: If the length of `key` and `value` lists do not match.\n    :raises KeyError: If `key` is not a string or a list of strings.\n\n    \"\"\"\n    if isinstance(key, str):\n        self._validate_key(key)\n        super().__setitem__(key, value)\n    elif isinstance(key, list) and isinstance(value, list):\n        if len(key) != len(value):\n            raise ValueError(\"Length of keys and values must match\")\n        for k, v in zip(key, value):\n            if not isinstance(k, str):\n                raise KeyError(\"All keys must be strings\")\n            self._validate_key(k)\n            super().__setitem__(k, v)\n    else:\n        raise KeyError(\"Key must be a string or list of strings\")\n</code></pre>"},{"location":"interface/ParamsDict/#yeastdnnexplorer.interface.ParamsDict.ParamsDict.__str__","title":"<code>__str__()</code>","text":"<p>Return a human-readable string representation of the ParamsDict.</p> <p>Returns:</p> Type Description <code>str</code> <p>A human-readable string representation of the ParamsDict.</p> Source code in <code>yeastdnnexplorer/interface/ParamsDict.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"\n    Return a human-readable string representation of the ParamsDict.\n\n    :return: A human-readable string representation of the ParamsDict.\n    :rtype: str\n\n    \"\"\"\n    return \", \".join(f\"{k}: {v}\" for k, v in self.items())\n</code></pre>"},{"location":"interface/ParamsDict/#yeastdnnexplorer.interface.ParamsDict.ParamsDict.as_dict","title":"<code>as_dict()</code>","text":"<p>Convert the ParamsDict to a standard dictionary.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A standard dictionary with the same items as the ParamsDict.</p> Source code in <code>yeastdnnexplorer/interface/ParamsDict.py</code> <pre><code>def as_dict(self) -&gt; dict:\n    \"\"\"\n    Convert the ParamsDict to a standard dictionary.\n\n    :return: A standard dictionary with the same items as the ParamsDict.\n    :rtype: dict\n\n    \"\"\"\n    return dict(self)\n</code></pre>"},{"location":"interface/PromoterSetAPI/","title":"PromoterSetAPI","text":"<p>             Bases: <code>AbstractRecordsAndFilesAPI</code></p> <p>Class to interact with the PromoterSetAPI endpoint.</p> Source code in <code>yeastdnnexplorer/interface/PromoterSetAPI.py</code> <pre><code>class PromoterSetAPI(AbstractRecordsAndFilesAPI):\n    \"\"\"Class to interact with the PromoterSetAPI endpoint.\"\"\"\n\n    def __init__(self, **kwargs) -&gt; None:\n        \"\"\"\n        Initialize the PromoterSetAPI object.\n\n        :param kwargs: parameters to pass through AbstractRecordsAndFilesAPI to\n            AbstractAPI.\n\n        \"\"\"\n        valid_param_keys = kwargs.pop(\n            \"valid_param_keys\",\n            [\"id\", \"name\"],\n        )\n\n        url = kwargs.pop(\"url\", os.getenv(\"PROMOTERSET_URL\", None))\n\n        super().__init__(url=url, valid_param_keys=valid_param_keys, **kwargs)\n\n    def create(self):\n        pass\n\n    def update(self):\n        pass\n\n    def delete(self):\n        pass\n</code></pre>"},{"location":"interface/PromoterSetAPI/#yeastdnnexplorer.interface.PromoterSetAPI.PromoterSetAPI.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the PromoterSetAPI object.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>parameters to pass through AbstractRecordsAndFilesAPI to AbstractAPI.</p> <code>{}</code> Source code in <code>yeastdnnexplorer/interface/PromoterSetAPI.py</code> <pre><code>def __init__(self, **kwargs) -&gt; None:\n    \"\"\"\n    Initialize the PromoterSetAPI object.\n\n    :param kwargs: parameters to pass through AbstractRecordsAndFilesAPI to\n        AbstractAPI.\n\n    \"\"\"\n    valid_param_keys = kwargs.pop(\n        \"valid_param_keys\",\n        [\"id\", \"name\"],\n    )\n\n    url = kwargs.pop(\"url\", os.getenv(\"PROMOTERSET_URL\", None))\n\n    super().__init__(url=url, valid_param_keys=valid_param_keys, **kwargs)\n</code></pre>"},{"location":"interface/PromoterSetSigAPI/","title":"PromoterSetSigAPI","text":"<p>             Bases: <code>AbstractRecordsAndFilesAPI</code></p> <p>Class to interact with the PromoterSetSigAPI endpoint.</p> Source code in <code>yeastdnnexplorer/interface/PromoterSetSigAPI.py</code> <pre><code>class PromoterSetSigAPI(AbstractRecordsAndFilesAPI):\n    \"\"\"Class to interact with the PromoterSetSigAPI endpoint.\"\"\"\n\n    def __init__(self, **kwargs) -&gt; None:\n        \"\"\"\n        Initialize the PromoterSetSigAPI object.\n\n        :param kwargs: parameters to pass through AbstractRecordsAndFilesAPI to\n            AbstractAPI.\n\n        \"\"\"\n        valid_param_keys = kwargs.pop(\n            \"valid_param_keys\",\n            [\n                \"id\",\n                \"binding\",\n                \"promoter\",\n                \"promoter_name\",\n                \"background\",\n                \"background_name\",\n                \"regulator_locus_tag\",\n                \"regulator_symbol\",\n                \"batch\",\n                \"replicate\",\n                \"source\",\n                \"lab\",\n                \"assay\",\n                \"workflow\",\n                \"data_usable\",\n            ],\n        )\n\n        url = kwargs.pop(\"url\", os.getenv(\"PROMOTERSETSIG_URL\", None))\n\n        super().__init__(url=url, valid_param_keys=valid_param_keys, **kwargs)\n\n    def create(self):\n        pass\n\n    def update(self):\n        pass\n\n    def delete(self):\n        pass\n</code></pre>"},{"location":"interface/PromoterSetSigAPI/#yeastdnnexplorer.interface.PromoterSetSigAPI.PromoterSetSigAPI.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the PromoterSetSigAPI object.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>parameters to pass through AbstractRecordsAndFilesAPI to AbstractAPI.</p> <code>{}</code> Source code in <code>yeastdnnexplorer/interface/PromoterSetSigAPI.py</code> <pre><code>def __init__(self, **kwargs) -&gt; None:\n    \"\"\"\n    Initialize the PromoterSetSigAPI object.\n\n    :param kwargs: parameters to pass through AbstractRecordsAndFilesAPI to\n        AbstractAPI.\n\n    \"\"\"\n    valid_param_keys = kwargs.pop(\n        \"valid_param_keys\",\n        [\n            \"id\",\n            \"binding\",\n            \"promoter\",\n            \"promoter_name\",\n            \"background\",\n            \"background_name\",\n            \"regulator_locus_tag\",\n            \"regulator_symbol\",\n            \"batch\",\n            \"replicate\",\n            \"source\",\n            \"lab\",\n            \"assay\",\n            \"workflow\",\n            \"data_usable\",\n        ],\n    )\n\n    url = kwargs.pop(\"url\", os.getenv(\"PROMOTERSETSIG_URL\", None))\n\n    super().__init__(url=url, valid_param_keys=valid_param_keys, **kwargs)\n</code></pre>"},{"location":"interface/RegulatorAPI/","title":"RegulatorAPI","text":"<p>             Bases: <code>AbstractRecordsOnlyAPI</code></p> <p>A class to interact with the RegulatorAPI endpoint.</p> Source code in <code>yeastdnnexplorer/interface/RegulatorAPI.py</code> <pre><code>class RegulatorAPI(AbstractRecordsOnlyAPI):\n    \"\"\"A class to interact with the RegulatorAPI endpoint.\"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the RegulatorAPI object.\n\n        :param kwargs: parameters to pass to AbstractAPI via AbstractRecordsOnlyAPI.\n\n        \"\"\"\n        valid_param_keys = kwargs.pop(\n            \"valid_param_keys\",\n            [\n                \"id\",\n                \"regulator_locus_tag\",\n                \"regulator_symbol\",\n                \"under_development\",\n            ],\n        )\n\n        url = kwargs.pop(\"url\", os.getenv(\"REGULATOR_URL\", None))\n        if not url:\n            raise AttributeError(\n                \"url must be provided or the environmental variable \",\n                \"`REGULATOR_URL` must be set\",\n            )\n\n        super().__init__(url=url, valid_param_keys=valid_param_keys, **kwargs)\n\n    def create(self):\n        pass\n\n    def update(self):\n        pass\n\n    def delete(self):\n        pass\n</code></pre>"},{"location":"interface/RegulatorAPI/#yeastdnnexplorer.interface.RegulatorAPI.RegulatorAPI.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the RegulatorAPI object.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>parameters to pass to AbstractAPI via AbstractRecordsOnlyAPI.</p> <code>{}</code> Source code in <code>yeastdnnexplorer/interface/RegulatorAPI.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the RegulatorAPI object.\n\n    :param kwargs: parameters to pass to AbstractAPI via AbstractRecordsOnlyAPI.\n\n    \"\"\"\n    valid_param_keys = kwargs.pop(\n        \"valid_param_keys\",\n        [\n            \"id\",\n            \"regulator_locus_tag\",\n            \"regulator_symbol\",\n            \"under_development\",\n        ],\n    )\n\n    url = kwargs.pop(\"url\", os.getenv(\"REGULATOR_URL\", None))\n    if not url:\n        raise AttributeError(\n            \"url must be provided or the environmental variable \",\n            \"`REGULATOR_URL` must be set\",\n        )\n\n    super().__init__(url=url, valid_param_keys=valid_param_keys, **kwargs)\n</code></pre>"},{"location":"ml_models/customizable_model/","title":"Models","text":"<p>             Bases: <code>LightningModule</code></p> <p>A class for a customizable model that takes in binding effects for each transcription factor and predicts gene expression values This class contains all of the logic for setup, training, validation, and testing of the model, as well as defining how data is passed through the model It is a subclass of pytorch_lightning.LightningModule, which is similar to a regular PyTorch nn.module but with added functionality for training and validation.</p> <p>This model takes in many more parameters that SimpleModel, allowing us to experiement with many hyperparameter and architecture choices in order to decide what is best for our task &amp; data</p> Source code in <code>yeastdnnexplorer/ml_models/customizable_model.py</code> <pre><code>class CustomizableModel(pl.LightningModule):\n    \"\"\"\n    A class for a customizable model that takes in binding effects for each\n    transcription factor and predicts gene expression values This class contains all of\n    the logic for setup, training, validation, and testing of the model, as well as\n    defining how data is passed through the model It is a subclass of\n    pytorch_lightning.LightningModule, which is similar to a regular PyTorch nn.module\n    but with added functionality for training and validation.\n\n    This model takes in many more parameters that SimpleModel, allowing us to\n    experiement with many hyperparameter and architecture choices in order to decide\n    what is best for our task &amp; data\n\n    \"\"\"\n\n    def __init__(\n        self,\n        input_dim: int,\n        output_dim: int,\n        lr: float = 0.001,\n        hidden_layer_num: int = 1,\n        hidden_layer_sizes: list = [128],\n        activation: str = \"ReLU\",  # can be \"ReLU\", \"Sigmoid\", \"Tanh\", \"LeakyReLU\"\n        optimizer: str = \"Adam\",  # can be \"Adam\", \"SGD\", \"RMSprop\"\n        L2_regularization_term: float = 0.0,\n        dropout_rate: float = 0.0,\n    ) -&gt; None:\n        \"\"\"\n        Constructor of CustomizableModel.\n\n        :param input_dim: The number of input features to our model, these are the\n            binding effects for each transcription factor for a specific gene\n        :type input_dim: int\n        :param output_dim: The number of output features of our model, this is the\n            predicted gene expression value for each TF\n        :type output_dim: int\n        :param lr: The learning rate for the optimizer\n        :type lr: float\n        :raises TypeError: If input_dim is not an integer\n        :raises TypeError: If output_dim is not an integer\n        :raises TypeError: If lr is not a positive float\n        :raises ValueError: If input_dim or output_dim are not positive\n        :param hidden_layer_num: The number of hidden layers in the model\n        :type hidden_layer_num: int\n        :param hidden_layer_sizes: The size of each hidden layer in the model\n        :type hidden_layer_sizes: list\n\n        \"\"\"\n        if not isinstance(input_dim, int):\n            raise TypeError(\"input_dim must be an integer\")\n        if not isinstance(output_dim, int):\n            raise TypeError(\"output_dim must be an integer\")\n        if not isinstance(lr, float) or lr &lt;= 0:\n            raise TypeError(\"lr must be a positive float\")\n        if input_dim &lt; 1 or output_dim &lt; 1:\n            raise ValueError(\"input_dim and output_dim must be positive integers\")\n        if not isinstance(hidden_layer_num, int):\n            raise TypeError(\"hidden_layer_num must be an integer\")\n        if not isinstance(hidden_layer_sizes, list) or not all(\n            isinstance(i, int) for i in hidden_layer_sizes\n        ):\n            raise TypeError(\"hidden_layer_sizes must be a list of integers\")\n        if len(hidden_layer_sizes) != hidden_layer_num:\n            raise ValueError(\n                \"hidden_layer_sizes must have length equal to hidden_layer_num\"\n            )\n        if not isinstance(activation, str) or activation not in [\n            \"ReLU\",\n            \"Sigmoid\",\n            \"Tanh\",\n            \"LeakyReLU\",\n        ]:\n            raise ValueError(\n                \"activation must be one of 'ReLU', 'Sigmoid', 'Tanh', 'LeakyReLU'\"\n            )\n        if not isinstance(optimizer, str) or optimizer not in [\n            \"Adam\",\n            \"SGD\",\n            \"RMSprop\",\n        ]:\n            raise ValueError(\"optimizer must be one of 'Adam', 'SGD', 'RMSprop'\")\n        if not isinstance(L2_regularization_term, float) or L2_regularization_term &lt; 0:\n            raise TypeError(\"L2_regularization_term must be a non-negative float\")\n        if not isinstance(dropout_rate, float) or dropout_rate &lt; 0 or dropout_rate &gt; 1:\n            raise TypeError(\"dropout_rate must be a float between 0 and 1\")\n\n        super().__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.lr = lr\n        self.hidden_layer_num = hidden_layer_num\n        self.hidden_layer_sizes = hidden_layer_sizes\n        self.optimizer = optimizer\n        self.L2_regularization_term = L2_regularization_term\n        self.save_hyperparameters()\n\n        match activation:\n            case \"ReLU\":\n                self.activation = nn.ReLU()\n            case \"Sigmoid\":\n                self.activation = nn.Sigmoid()\n            case \"Tanh\":\n                self.activation = nn.Tanh()\n            case \"LeakyReLU\":\n                self.activation = nn.LeakyReLU()\n            case _:\n                raise ValueError(\n                    \"activation must be one of 'ReLU', 'Sigmoid', 'Tanh', 'LeakyReLU'\"\n                )\n\n        self.input_layer = nn.Linear(input_dim, hidden_layer_sizes[0])\n        self.hidden_layers = nn.ModuleList([])\n        for i in range(hidden_layer_num - 1):\n            self.hidden_layers.append(\n                nn.Linear(hidden_layer_sizes[i], hidden_layer_sizes[i + 1])\n            )\n        self.output_layer = nn.Linear(hidden_layer_sizes[-1], output_dim)\n\n        self.dropout = nn.Dropout(p=dropout_rate)\n\n        self.mae = MeanAbsoluteError()\n        self.SMSE = SMSE()\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the model (i.e. how predictions are made for a given input)\n\n        :param x: The input data to the model (minus the target y values)\n        :type x: torch.Tensor\n        :return: The predicted y values for the input x, this is a tensor of shape\n            (batch_size, output_dim)\n        :rtype: torch.Tensor\n\n        \"\"\"\n        x = self.dropout(self.activation(self.input_layer(x)))\n        for hidden_layer in self.hidden_layers:\n            x = self.dropout(self.activation(hidden_layer(x)))\n        x = self.output_layer(x)\n        return x\n\n    def training_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Training step for the model, this is called for each batch of data during\n        training.\n\n        :param batch: The batch of data to train on\n        :type batch: Any\n        :param batch_idx: The index of the batch\n        :type batch_idx: int\n        :return: The loss for the training batch\n        :rtype: torch.Tensor\n\n        \"\"\"\n        x, y = batch\n        y_pred = self(x)\n        mse_loss = nn.functional.mse_loss(y_pred, y)\n        self.log(\"train_mse\", mse_loss)\n        self.log(\"train_mae\", self.mae(y_pred, y))\n        self.log(\"train_smse\", self.SMSE(y_pred, y))\n        return mse_loss\n\n    def validation_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Validation step for the model, this is called for each batch of data during\n        validation.\n\n        :param batch: The batch of data to validate on\n        :type batch: Any\n        :param batch_idx: The index of the batch\n        :type batch_idx: int\n        :return: The loss for the validation batch\n        :rtype: torch.Tensor\n\n        \"\"\"\n        x, y = batch\n        y_pred = self(x)\n        mse_loss = nn.functional.mse_loss(y_pred, y)\n        self.log(\"val_mse\", mse_loss)\n        self.log(\"val_mae\", self.mae(y_pred, y))\n        self.log(\"val_smse\", self.SMSE(y_pred, y))\n        return mse_loss\n\n    def test_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Test step for the model, this is called for each batch of data during testing\n        Testing is only performed after training and validation when we have chosen a\n        final model We want to test our final model on unseen data (which is why we use\n        validation sets to \"test\" during training)\n\n        :param batch: The batch of data to test on (this will have size (batch_size,\n            input_dim)\n        :type batch: Any\n        :param batch_idx: The index of the batch\n        :type batch_idx: int\n        :return: The loss for the test batch\n        :rtype: torch.Tensor\n\n        \"\"\"\n        x, y = batch\n        y_pred = self(x)\n        mse_loss = nn.functional.mse_loss(y_pred, y)\n        self.log(\"test_mse\", mse_loss)\n        self.log(\"test_mae\", self.mae(y_pred, y))\n        self.log(\"test_smse\", self.SMSE(y_pred, y))\n        return mse_loss\n\n    def configure_optimizers(self) -&gt; Optimizer:\n        \"\"\"\n        Configure the optimizer for the model.\n\n        :return: The optimizer for the model\n        :rtype: Optimizer\n\n        \"\"\"\n        match self.optimizer:\n            case \"Adam\":\n                return torch.optim.Adam(\n                    self.parameters(),\n                    lr=self.lr,\n                    weight_decay=self.L2_regularization_term,\n                )\n            case \"SGD\":\n                return torch.optim.SGD(\n                    self.parameters(),\n                    lr=self.lr,\n                    weight_decay=self.L2_regularization_term,\n                )\n            case \"RMSprop\":\n                return torch.optim.RMSprop(\n                    self.parameters(),\n                    lr=self.lr,\n                    weight_decay=self.L2_regularization_term,\n                )\n            case _:\n                raise ValueError(\"optimizer must be one of 'Adam', 'SGD', 'RMSprop'\")\n</code></pre>"},{"location":"ml_models/customizable_model/#yeastdnnexplorer.ml_models.customizable_model.CustomizableModel.__init__","title":"<code>__init__(input_dim, output_dim, lr=0.001, hidden_layer_num=1, hidden_layer_sizes=[128], activation='ReLU', optimizer='Adam', L2_regularization_term=0.0, dropout_rate=0.0)</code>","text":"<p>Constructor of CustomizableModel.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>The number of input features to our model, these are the binding effects for each transcription factor for a specific gene</p> required <code>output_dim</code> <code>int</code> <p>The number of output features of our model, this is the predicted gene expression value for each TF</p> required <code>lr</code> <code>float</code> <p>The learning rate for the optimizer</p> <code>0.001</code> <code>hidden_layer_num</code> <code>int</code> <p>The number of hidden layers in the model</p> <code>1</code> <code>hidden_layer_sizes</code> <code>list</code> <p>The size of each hidden layer in the model</p> <code>[128]</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If input_dim is not an integer</p> <code>TypeError</code> <p>If output_dim is not an integer</p> <code>TypeError</code> <p>If lr is not a positive float</p> <code>ValueError</code> <p>If input_dim or output_dim are not positive</p> Source code in <code>yeastdnnexplorer/ml_models/customizable_model.py</code> <pre><code>def __init__(\n    self,\n    input_dim: int,\n    output_dim: int,\n    lr: float = 0.001,\n    hidden_layer_num: int = 1,\n    hidden_layer_sizes: list = [128],\n    activation: str = \"ReLU\",  # can be \"ReLU\", \"Sigmoid\", \"Tanh\", \"LeakyReLU\"\n    optimizer: str = \"Adam\",  # can be \"Adam\", \"SGD\", \"RMSprop\"\n    L2_regularization_term: float = 0.0,\n    dropout_rate: float = 0.0,\n) -&gt; None:\n    \"\"\"\n    Constructor of CustomizableModel.\n\n    :param input_dim: The number of input features to our model, these are the\n        binding effects for each transcription factor for a specific gene\n    :type input_dim: int\n    :param output_dim: The number of output features of our model, this is the\n        predicted gene expression value for each TF\n    :type output_dim: int\n    :param lr: The learning rate for the optimizer\n    :type lr: float\n    :raises TypeError: If input_dim is not an integer\n    :raises TypeError: If output_dim is not an integer\n    :raises TypeError: If lr is not a positive float\n    :raises ValueError: If input_dim or output_dim are not positive\n    :param hidden_layer_num: The number of hidden layers in the model\n    :type hidden_layer_num: int\n    :param hidden_layer_sizes: The size of each hidden layer in the model\n    :type hidden_layer_sizes: list\n\n    \"\"\"\n    if not isinstance(input_dim, int):\n        raise TypeError(\"input_dim must be an integer\")\n    if not isinstance(output_dim, int):\n        raise TypeError(\"output_dim must be an integer\")\n    if not isinstance(lr, float) or lr &lt;= 0:\n        raise TypeError(\"lr must be a positive float\")\n    if input_dim &lt; 1 or output_dim &lt; 1:\n        raise ValueError(\"input_dim and output_dim must be positive integers\")\n    if not isinstance(hidden_layer_num, int):\n        raise TypeError(\"hidden_layer_num must be an integer\")\n    if not isinstance(hidden_layer_sizes, list) or not all(\n        isinstance(i, int) for i in hidden_layer_sizes\n    ):\n        raise TypeError(\"hidden_layer_sizes must be a list of integers\")\n    if len(hidden_layer_sizes) != hidden_layer_num:\n        raise ValueError(\n            \"hidden_layer_sizes must have length equal to hidden_layer_num\"\n        )\n    if not isinstance(activation, str) or activation not in [\n        \"ReLU\",\n        \"Sigmoid\",\n        \"Tanh\",\n        \"LeakyReLU\",\n    ]:\n        raise ValueError(\n            \"activation must be one of 'ReLU', 'Sigmoid', 'Tanh', 'LeakyReLU'\"\n        )\n    if not isinstance(optimizer, str) or optimizer not in [\n        \"Adam\",\n        \"SGD\",\n        \"RMSprop\",\n    ]:\n        raise ValueError(\"optimizer must be one of 'Adam', 'SGD', 'RMSprop'\")\n    if not isinstance(L2_regularization_term, float) or L2_regularization_term &lt; 0:\n        raise TypeError(\"L2_regularization_term must be a non-negative float\")\n    if not isinstance(dropout_rate, float) or dropout_rate &lt; 0 or dropout_rate &gt; 1:\n        raise TypeError(\"dropout_rate must be a float between 0 and 1\")\n\n    super().__init__()\n    self.input_dim = input_dim\n    self.output_dim = output_dim\n    self.lr = lr\n    self.hidden_layer_num = hidden_layer_num\n    self.hidden_layer_sizes = hidden_layer_sizes\n    self.optimizer = optimizer\n    self.L2_regularization_term = L2_regularization_term\n    self.save_hyperparameters()\n\n    match activation:\n        case \"ReLU\":\n            self.activation = nn.ReLU()\n        case \"Sigmoid\":\n            self.activation = nn.Sigmoid()\n        case \"Tanh\":\n            self.activation = nn.Tanh()\n        case \"LeakyReLU\":\n            self.activation = nn.LeakyReLU()\n        case _:\n            raise ValueError(\n                \"activation must be one of 'ReLU', 'Sigmoid', 'Tanh', 'LeakyReLU'\"\n            )\n\n    self.input_layer = nn.Linear(input_dim, hidden_layer_sizes[0])\n    self.hidden_layers = nn.ModuleList([])\n    for i in range(hidden_layer_num - 1):\n        self.hidden_layers.append(\n            nn.Linear(hidden_layer_sizes[i], hidden_layer_sizes[i + 1])\n        )\n    self.output_layer = nn.Linear(hidden_layer_sizes[-1], output_dim)\n\n    self.dropout = nn.Dropout(p=dropout_rate)\n\n    self.mae = MeanAbsoluteError()\n    self.SMSE = SMSE()\n</code></pre>"},{"location":"ml_models/customizable_model/#yeastdnnexplorer.ml_models.customizable_model.CustomizableModel.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure the optimizer for the model.</p> <p>Returns:</p> Type Description <code>Optimizer</code> <p>The optimizer for the model</p> Source code in <code>yeastdnnexplorer/ml_models/customizable_model.py</code> <pre><code>def configure_optimizers(self) -&gt; Optimizer:\n    \"\"\"\n    Configure the optimizer for the model.\n\n    :return: The optimizer for the model\n    :rtype: Optimizer\n\n    \"\"\"\n    match self.optimizer:\n        case \"Adam\":\n            return torch.optim.Adam(\n                self.parameters(),\n                lr=self.lr,\n                weight_decay=self.L2_regularization_term,\n            )\n        case \"SGD\":\n            return torch.optim.SGD(\n                self.parameters(),\n                lr=self.lr,\n                weight_decay=self.L2_regularization_term,\n            )\n        case \"RMSprop\":\n            return torch.optim.RMSprop(\n                self.parameters(),\n                lr=self.lr,\n                weight_decay=self.L2_regularization_term,\n            )\n        case _:\n            raise ValueError(\"optimizer must be one of 'Adam', 'SGD', 'RMSprop'\")\n</code></pre>"},{"location":"ml_models/customizable_model/#yeastdnnexplorer.ml_models.customizable_model.CustomizableModel.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the model (i.e. how predictions are made for a given input)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input data to the model (minus the target y values)</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The predicted y values for the input x, this is a tensor of shape (batch_size, output_dim)</p> Source code in <code>yeastdnnexplorer/ml_models/customizable_model.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the model (i.e. how predictions are made for a given input)\n\n    :param x: The input data to the model (minus the target y values)\n    :type x: torch.Tensor\n    :return: The predicted y values for the input x, this is a tensor of shape\n        (batch_size, output_dim)\n    :rtype: torch.Tensor\n\n    \"\"\"\n    x = self.dropout(self.activation(self.input_layer(x)))\n    for hidden_layer in self.hidden_layers:\n        x = self.dropout(self.activation(hidden_layer(x)))\n    x = self.output_layer(x)\n    return x\n</code></pre>"},{"location":"ml_models/customizable_model/#yeastdnnexplorer.ml_models.customizable_model.CustomizableModel.test_step","title":"<code>test_step(batch, batch_idx)</code>","text":"<p>Test step for the model, this is called for each batch of data during testing Testing is only performed after training and validation when we have chosen a final model We want to test our final model on unseen data (which is why we use validation sets to \u201ctest\u201d during training)</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The batch of data to test on (this will have size (batch_size, input_dim)</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The loss for the test batch</p> Source code in <code>yeastdnnexplorer/ml_models/customizable_model.py</code> <pre><code>def test_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Test step for the model, this is called for each batch of data during testing\n    Testing is only performed after training and validation when we have chosen a\n    final model We want to test our final model on unseen data (which is why we use\n    validation sets to \"test\" during training)\n\n    :param batch: The batch of data to test on (this will have size (batch_size,\n        input_dim)\n    :type batch: Any\n    :param batch_idx: The index of the batch\n    :type batch_idx: int\n    :return: The loss for the test batch\n    :rtype: torch.Tensor\n\n    \"\"\"\n    x, y = batch\n    y_pred = self(x)\n    mse_loss = nn.functional.mse_loss(y_pred, y)\n    self.log(\"test_mse\", mse_loss)\n    self.log(\"test_mae\", self.mae(y_pred, y))\n    self.log(\"test_smse\", self.SMSE(y_pred, y))\n    return mse_loss\n</code></pre>"},{"location":"ml_models/customizable_model/#yeastdnnexplorer.ml_models.customizable_model.CustomizableModel.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Training step for the model, this is called for each batch of data during training.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The batch of data to train on</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The loss for the training batch</p> Source code in <code>yeastdnnexplorer/ml_models/customizable_model.py</code> <pre><code>def training_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Training step for the model, this is called for each batch of data during\n    training.\n\n    :param batch: The batch of data to train on\n    :type batch: Any\n    :param batch_idx: The index of the batch\n    :type batch_idx: int\n    :return: The loss for the training batch\n    :rtype: torch.Tensor\n\n    \"\"\"\n    x, y = batch\n    y_pred = self(x)\n    mse_loss = nn.functional.mse_loss(y_pred, y)\n    self.log(\"train_mse\", mse_loss)\n    self.log(\"train_mae\", self.mae(y_pred, y))\n    self.log(\"train_smse\", self.SMSE(y_pred, y))\n    return mse_loss\n</code></pre>"},{"location":"ml_models/customizable_model/#yeastdnnexplorer.ml_models.customizable_model.CustomizableModel.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Validation step for the model, this is called for each batch of data during validation.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The batch of data to validate on</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The loss for the validation batch</p> Source code in <code>yeastdnnexplorer/ml_models/customizable_model.py</code> <pre><code>def validation_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Validation step for the model, this is called for each batch of data during\n    validation.\n\n    :param batch: The batch of data to validate on\n    :type batch: Any\n    :param batch_idx: The index of the batch\n    :type batch_idx: int\n    :return: The loss for the validation batch\n    :rtype: torch.Tensor\n\n    \"\"\"\n    x, y = batch\n    y_pred = self(x)\n    mse_loss = nn.functional.mse_loss(y_pred, y)\n    self.log(\"val_mse\", mse_loss)\n    self.log(\"val_mae\", self.mae(y_pred, y))\n    self.log(\"val_smse\", self.SMSE(y_pred, y))\n    return mse_loss\n</code></pre>"},{"location":"ml_models/metrics_compute_nrmse/","title":"Metrics compute nrmse","text":"<p>Compute the Normalized Root Mean Squared Error. This can be used to better compare models trained on different datasets with differnet scales, although it is not perfectly scale invariant.</p> <p>Parameters:</p> Name Type Description Default <code>y_pred</code> <code>torch.Tensor</code> <p>The predicted y values</p> required <code>y_true</code> <code>torch.Tensor</code> <p>The true y values</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The normalized root mean squared error</p> Source code in <code>yeastdnnexplorer/ml_models/metrics.py</code> <pre><code>def compute_nrmse(self, y_pred, y_true):\n    \"\"\"\n    Compute the Normalized Root Mean Squared Error. This can be used to better compare\n    models trained on different datasets with differnet scales, although it is not\n    perfectly scale invariant.\n\n    :param y_pred: The predicted y values\n    :type y_pred: torch.Tensor\n    :param y_true: The true y values\n    :type y_true: torch.Tensor\n    :return: The normalized root mean squared error\n    :rtype: torch.Tensor\n\n    \"\"\"\n    rmse = torch.sqrt(F.mse_loss(y_pred, y_true))\n\n    # normalize with the range of true y values\n    y_range = y_true.max() - y_true.min()\n    nrmse = rmse / y_range\n    return nrmse\n</code></pre>"},{"location":"ml_models/metrics_smse/","title":"Metrics smse","text":"<p>             Bases: <code>Metric</code></p> <p>A class for computing the standardized mean squared error (SMSE) metric.</p> <p>This metric is defined as the mean squared error divided by the variance of the true values (the target data). Because we are dividing by the variance of the true values, this metric is scale-independent and does not depend on the mean of the true values. It allows us to effectively compare models drawn from different datasets with differring scales or means (as long as their variances are at least relatively similar)</p> Source code in <code>yeastdnnexplorer/ml_models/metrics.py</code> <pre><code>class SMSE(Metric):\n    \"\"\"\n    A class for computing the standardized mean squared error (SMSE) metric.\n\n    This metric is defined as the mean squared error divided by the variance of the true\n    values (the target data). Because we are dividing by the variance of the true\n    values, this metric is scale-independent and does not depend on the mean of the true\n    values. It allows us to effectively compare models drawn from different datasets\n    with differring scales or means (as long as their variances are at least relatively\n    similar)\n\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the SMSE metric.\"\"\"\n        super().__init__()\n        self.add_state(\"mse\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n        self.add_state(\"variance\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n        self.add_state(\"num_samples\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n\n    def update(self, y_pred: torch.Tensor, y_true: torch.Tensor):\n        \"\"\"\n        Update the metric with new predictions and true values.\n\n        :param y_pred: The predicted y values\n        :type y_pred: torch.Tensor\n        :param y_true: The true y values\n        :type y_true: torch.Tensor\n\n        \"\"\"\n        self.mse += F.mse_loss(y_pred, y_true, reduction=\"sum\")\n        self.variance += torch.var(y_true, unbiased=False) * y_true.size(\n            0\n        )  # Total variance (TODO should we have unbiased=False here?)\n        self.num_samples += y_true.numel()\n\n    def compute(self):\n        \"\"\"\n        Compute the SMSE metric.\n\n        :return: The SMSE metric\n        :rtype: torch.Tensor\n\n        \"\"\"\n        mean_mse = self.mse / self.num_samples\n        mean_variance = self.variance / self.num_samples\n        return mean_mse / mean_variance\n</code></pre>"},{"location":"ml_models/metrics_smse/#yeastdnnexplorer.ml_models.metrics.SMSE.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the SMSE metric.</p> Source code in <code>yeastdnnexplorer/ml_models/metrics.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the SMSE metric.\"\"\"\n    super().__init__()\n    self.add_state(\"mse\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n    self.add_state(\"variance\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n    self.add_state(\"num_samples\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n</code></pre>"},{"location":"ml_models/metrics_smse/#yeastdnnexplorer.ml_models.metrics.SMSE.compute","title":"<code>compute()</code>","text":"<p>Compute the SMSE metric.</p> <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The SMSE metric</p> Source code in <code>yeastdnnexplorer/ml_models/metrics.py</code> <pre><code>def compute(self):\n    \"\"\"\n    Compute the SMSE metric.\n\n    :return: The SMSE metric\n    :rtype: torch.Tensor\n\n    \"\"\"\n    mean_mse = self.mse / self.num_samples\n    mean_variance = self.variance / self.num_samples\n    return mean_mse / mean_variance\n</code></pre>"},{"location":"ml_models/metrics_smse/#yeastdnnexplorer.ml_models.metrics.SMSE.update","title":"<code>update(y_pred, y_true)</code>","text":"<p>Update the metric with new predictions and true values.</p> <p>Parameters:</p> Name Type Description Default <code>y_pred</code> <code>Tensor</code> <p>The predicted y values</p> required <code>y_true</code> <code>Tensor</code> <p>The true y values</p> required Source code in <code>yeastdnnexplorer/ml_models/metrics.py</code> <pre><code>def update(self, y_pred: torch.Tensor, y_true: torch.Tensor):\n    \"\"\"\n    Update the metric with new predictions and true values.\n\n    :param y_pred: The predicted y values\n    :type y_pred: torch.Tensor\n    :param y_true: The true y values\n    :type y_true: torch.Tensor\n\n    \"\"\"\n    self.mse += F.mse_loss(y_pred, y_true, reduction=\"sum\")\n    self.variance += torch.var(y_true, unbiased=False) * y_true.size(\n        0\n    )  # Total variance (TODO should we have unbiased=False here?)\n    self.num_samples += y_true.numel()\n</code></pre>"},{"location":"ml_models/simple_model/","title":"Simple model","text":""},{"location":"ml_models/simple_model/#yeastdnnexplorer.ml_models.simple_model.SimpleModel","title":"<code>SimpleModel</code>","text":"<p>             Bases: <code>LightningModule</code></p> <p>A class for a simple linear model that takes in binding effects for each transcription factor and predicts gene expression values This class contains all of the logic for setup, training, validation, and testing of the model, as well as defining how data is passed through the model It is a subclass of pytorch_lightning.LightningModule, which is similar to a regular PyTorch nn.module but with added functionality for training and validation.</p> Source code in <code>yeastdnnexplorer/ml_models/simple_model.py</code> <pre><code>class SimpleModel(pl.LightningModule):\n    \"\"\"A class for a simple linear model that takes in binding effects for each\n    transcription factor and predicts gene expression values This class contains all of\n    the logic for setup, training, validation, and testing of the model, as well as\n    defining how data is passed through the model It is a subclass of\n    pytorch_lightning.LightningModule, which is similar to a regular PyTorch nn.module\n    but with added functionality for training and validation.\"\"\"\n\n    def __init__(self, input_dim: int, output_dim: int, lr: float = 0.001) -&gt; None:\n        \"\"\"\n        Constructor of SimpleModel.\n\n        :param input_dim: The number of input features to our model, these are the\n            binding effects for each transcription factor for a specific gene\n        :type input_dim: int\n        :param output_dim: The number of output features of our model, this is the\n            predicted gene expression value for each TF\n        :type output_dim: int\n        :param lr: The learning rate for the optimizer\n        :type lr: float\n        :raises TypeError: If input_dim is not an integer\n        :raises TypeError: If output_dim is not an integer\n        :raises TypeError: If lr is not a positive float\n        :raises ValueError: If input_dim or output_dim are not positive\n\n        \"\"\"\n        if not isinstance(input_dim, int):\n            raise TypeError(\"input_dim must be an integer\")\n        if not isinstance(output_dim, int):\n            raise TypeError(\"output_dim must be an integer\")\n        if not isinstance(lr, float) or lr &lt;= 0:\n            raise TypeError(\"lr must be a positive float\")\n        if input_dim &lt; 1 or output_dim &lt; 1:\n            raise ValueError(\"input_dim and output_dim must be positive integers\")\n\n        super().__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.lr = lr\n        self.save_hyperparameters()\n\n        self.mae = MeanAbsoluteError()\n        self.SMSE = SMSE()\n\n        # define layers for the model here\n        self.linear1 = nn.Linear(input_dim, output_dim)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the model (i.e. how predictions are made for a given input)\n\n        :param x: The input data to the model (minus the target y values)\n        :type x: torch.Tensor\n        :return: The predicted y values for the input x, this is a tensor of shape\n            (batch_size, output_dim)\n        :rtype: torch.Tensor\n\n        \"\"\"\n        return self.linear1(x)\n\n    def training_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Training step for the model, this is called for each batch of data during\n        training.\n\n        :param batch: The batch of data to train on\n        :type batch: Any\n        :param batch_idx: The index of the batch\n        :type batch_idx: int\n        :return: The loss for the training batch\n        :rtype: torch.Tensor\n\n        \"\"\"\n        x, y = batch\n        y_pred = self(x)\n        loss = nn.functional.mse_loss(y_pred, y)\n        self.log(\"train_mse\", loss)\n        self.log(\"train_mae\", self.mae(y_pred, y))\n        self.log(\"train_smse\", self.SMSE(y_pred, y))\n        return loss\n\n    def validation_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Validation step for the model, this is called for each batch of data during\n        validation.\n\n        :param batch: The batch of data to validate on\n        :type batch: Any\n        :param batch_idx: The index of the batch\n        :type batch_idx: int\n        :return: The loss for the validation batch\n        :rtype: torch.Tensor\n\n        \"\"\"\n        x, y = batch\n        y_pred = self(x)\n        loss = nn.functional.mse_loss(y_pred, y)\n\n        self.log(\"val_mse\", loss)\n        self.log(\"val_mae\", self.mae(y_pred, y))\n        self.log(\"val_smse\", self.SMSE(y_pred, y))\n        return loss\n\n    def test_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Test step for the model, this is called for each batch of data during testing\n        Testing is only performed after training and validation when we have chosen a\n        final model We want to test our final model on unseen data (which is why we use\n        validation sets to \"test\" during training)\n\n        :param batch: The batch of data to test on (this will have size (batch_size,\n            input_dim)\n        :type batch: Any\n        :param batch_idx: The index of the batch\n        :type batch_idx: int\n        :return: The loss for the test batch\n        :rtype: torch.Tensor\n\n        \"\"\"\n        x, y = batch\n        y_pred = self(x)\n        loss = nn.functional.mse_loss(y_pred, y)\n        self.log(\"test_mse\", loss)\n        self.log(\"test_mae\", self.mae(y_pred, y))\n        self.log(\"test_smse\", self.SMSE(y_pred, y))\n        return loss\n\n    def configure_optimizers(self) -&gt; Optimizer:\n        \"\"\"\n        Configure the optimizer for the model.\n\n        :return: The optimizer for the model\n        :rtype: Optimizer\n\n        \"\"\"\n        return torch.optim.Adam(self.parameters(), lr=self.lr)\n</code></pre>"},{"location":"ml_models/simple_model/#yeastdnnexplorer.ml_models.simple_model.SimpleModel.__init__","title":"<code>__init__(input_dim, output_dim, lr=0.001)</code>","text":"<p>Constructor of SimpleModel.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>The number of input features to our model, these are the binding effects for each transcription factor for a specific gene</p> required <code>output_dim</code> <code>int</code> <p>The number of output features of our model, this is the predicted gene expression value for each TF</p> required <code>lr</code> <code>float</code> <p>The learning rate for the optimizer</p> <code>0.001</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If input_dim is not an integer</p> <code>TypeError</code> <p>If output_dim is not an integer</p> <code>TypeError</code> <p>If lr is not a positive float</p> <code>ValueError</code> <p>If input_dim or output_dim are not positive</p> Source code in <code>yeastdnnexplorer/ml_models/simple_model.py</code> <pre><code>def __init__(self, input_dim: int, output_dim: int, lr: float = 0.001) -&gt; None:\n    \"\"\"\n    Constructor of SimpleModel.\n\n    :param input_dim: The number of input features to our model, these are the\n        binding effects for each transcription factor for a specific gene\n    :type input_dim: int\n    :param output_dim: The number of output features of our model, this is the\n        predicted gene expression value for each TF\n    :type output_dim: int\n    :param lr: The learning rate for the optimizer\n    :type lr: float\n    :raises TypeError: If input_dim is not an integer\n    :raises TypeError: If output_dim is not an integer\n    :raises TypeError: If lr is not a positive float\n    :raises ValueError: If input_dim or output_dim are not positive\n\n    \"\"\"\n    if not isinstance(input_dim, int):\n        raise TypeError(\"input_dim must be an integer\")\n    if not isinstance(output_dim, int):\n        raise TypeError(\"output_dim must be an integer\")\n    if not isinstance(lr, float) or lr &lt;= 0:\n        raise TypeError(\"lr must be a positive float\")\n    if input_dim &lt; 1 or output_dim &lt; 1:\n        raise ValueError(\"input_dim and output_dim must be positive integers\")\n\n    super().__init__()\n    self.input_dim = input_dim\n    self.output_dim = output_dim\n    self.lr = lr\n    self.save_hyperparameters()\n\n    self.mae = MeanAbsoluteError()\n    self.SMSE = SMSE()\n\n    # define layers for the model here\n    self.linear1 = nn.Linear(input_dim, output_dim)\n</code></pre>"},{"location":"ml_models/simple_model/#yeastdnnexplorer.ml_models.simple_model.SimpleModel.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure the optimizer for the model.</p> <p>Returns:</p> Type Description <code>Optimizer</code> <p>The optimizer for the model</p> Source code in <code>yeastdnnexplorer/ml_models/simple_model.py</code> <pre><code>def configure_optimizers(self) -&gt; Optimizer:\n    \"\"\"\n    Configure the optimizer for the model.\n\n    :return: The optimizer for the model\n    :rtype: Optimizer\n\n    \"\"\"\n    return torch.optim.Adam(self.parameters(), lr=self.lr)\n</code></pre>"},{"location":"ml_models/simple_model/#yeastdnnexplorer.ml_models.simple_model.SimpleModel.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the model (i.e. how predictions are made for a given input)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input data to the model (minus the target y values)</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The predicted y values for the input x, this is a tensor of shape (batch_size, output_dim)</p> Source code in <code>yeastdnnexplorer/ml_models/simple_model.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the model (i.e. how predictions are made for a given input)\n\n    :param x: The input data to the model (minus the target y values)\n    :type x: torch.Tensor\n    :return: The predicted y values for the input x, this is a tensor of shape\n        (batch_size, output_dim)\n    :rtype: torch.Tensor\n\n    \"\"\"\n    return self.linear1(x)\n</code></pre>"},{"location":"ml_models/simple_model/#yeastdnnexplorer.ml_models.simple_model.SimpleModel.test_step","title":"<code>test_step(batch, batch_idx)</code>","text":"<p>Test step for the model, this is called for each batch of data during testing Testing is only performed after training and validation when we have chosen a final model We want to test our final model on unseen data (which is why we use validation sets to \u201ctest\u201d during training)</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The batch of data to test on (this will have size (batch_size, input_dim)</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The loss for the test batch</p> Source code in <code>yeastdnnexplorer/ml_models/simple_model.py</code> <pre><code>def test_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Test step for the model, this is called for each batch of data during testing\n    Testing is only performed after training and validation when we have chosen a\n    final model We want to test our final model on unseen data (which is why we use\n    validation sets to \"test\" during training)\n\n    :param batch: The batch of data to test on (this will have size (batch_size,\n        input_dim)\n    :type batch: Any\n    :param batch_idx: The index of the batch\n    :type batch_idx: int\n    :return: The loss for the test batch\n    :rtype: torch.Tensor\n\n    \"\"\"\n    x, y = batch\n    y_pred = self(x)\n    loss = nn.functional.mse_loss(y_pred, y)\n    self.log(\"test_mse\", loss)\n    self.log(\"test_mae\", self.mae(y_pred, y))\n    self.log(\"test_smse\", self.SMSE(y_pred, y))\n    return loss\n</code></pre>"},{"location":"ml_models/simple_model/#yeastdnnexplorer.ml_models.simple_model.SimpleModel.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Training step for the model, this is called for each batch of data during training.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The batch of data to train on</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The loss for the training batch</p> Source code in <code>yeastdnnexplorer/ml_models/simple_model.py</code> <pre><code>def training_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Training step for the model, this is called for each batch of data during\n    training.\n\n    :param batch: The batch of data to train on\n    :type batch: Any\n    :param batch_idx: The index of the batch\n    :type batch_idx: int\n    :return: The loss for the training batch\n    :rtype: torch.Tensor\n\n    \"\"\"\n    x, y = batch\n    y_pred = self(x)\n    loss = nn.functional.mse_loss(y_pred, y)\n    self.log(\"train_mse\", loss)\n    self.log(\"train_mae\", self.mae(y_pred, y))\n    self.log(\"train_smse\", self.SMSE(y_pred, y))\n    return loss\n</code></pre>"},{"location":"ml_models/simple_model/#yeastdnnexplorer.ml_models.simple_model.SimpleModel.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Validation step for the model, this is called for each batch of data during validation.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The batch of data to validate on</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The loss for the validation batch</p> Source code in <code>yeastdnnexplorer/ml_models/simple_model.py</code> <pre><code>def validation_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Validation step for the model, this is called for each batch of data during\n    validation.\n\n    :param batch: The batch of data to validate on\n    :type batch: Any\n    :param batch_idx: The index of the batch\n    :type batch_idx: int\n    :return: The loss for the validation batch\n    :rtype: torch.Tensor\n\n    \"\"\"\n    x, y = batch\n    y_pred = self(x)\n    loss = nn.functional.mse_loss(y_pred, y)\n\n    self.log(\"val_mse\", loss)\n    self.log(\"val_mae\", self.mae(y_pred, y))\n    self.log(\"val_smse\", self.SMSE(y_pred, y))\n    return loss\n</code></pre>"},{"location":"probability_models/GenePopulation/","title":"GenePopulation","text":"<p>A simple class to hold a tensor boolean 1D vector where 0 is meant to identify genes which are unaffected by a given TF and 1 is meant to identify genes which are affected by a given TF.</p> Source code in <code>yeastdnnexplorer/probability_models/generate_data.py</code> <pre><code>class GenePopulation:\n    \"\"\"A simple class to hold a tensor boolean 1D vector where 0 is meant to identify\n    genes which are unaffected by a given TF and 1 is meant to identify genes which are\n    affected by a given TF.\"\"\"\n\n    def __init__(self, labels: torch.Tensor) -&gt; None:\n        \"\"\"\n        Constructor of GenePopulation.\n\n        :param labels: This can be any 1D tensor of boolean values. But it is meant to\n            be the output of `generate_gene_population()`\n        :type labels: torch.Tensor\n        :raises TypeError: If labels is not a tensor\n        :raises ValueError: If labels is not a 1D tensor\n        :raises TypeError: If labels is not a boolean tensor\n\n        \"\"\"\n        if not isinstance(labels, torch.Tensor):\n            raise TypeError(\"labels must be a tensor\")\n        if not labels.ndim == 1:\n            raise ValueError(\"labels must be a 1D tensor\")\n        if not labels.dtype == torch.bool:\n            raise TypeError(\"labels must be a boolean tensor\")\n        self.labels = labels\n\n    def __repr__(self):\n        return f\"&lt;GenePopulation size={len(self.labels)}&gt;\"\n</code></pre>"},{"location":"probability_models/GenePopulation/#yeastdnnexplorer.probability_models.generate_data.GenePopulation.__init__","title":"<code>__init__(labels)</code>","text":"<p>Constructor of GenePopulation.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Tensor</code> <p>This can be any 1D tensor of boolean values. But it is meant to be the output of <code>generate_gene_population()</code></p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If labels is not a tensor</p> <code>ValueError</code> <p>If labels is not a 1D tensor</p> <code>TypeError</code> <p>If labels is not a boolean tensor</p> Source code in <code>yeastdnnexplorer/probability_models/generate_data.py</code> <pre><code>def __init__(self, labels: torch.Tensor) -&gt; None:\n    \"\"\"\n    Constructor of GenePopulation.\n\n    :param labels: This can be any 1D tensor of boolean values. But it is meant to\n        be the output of `generate_gene_population()`\n    :type labels: torch.Tensor\n    :raises TypeError: If labels is not a tensor\n    :raises ValueError: If labels is not a 1D tensor\n    :raises TypeError: If labels is not a boolean tensor\n\n    \"\"\"\n    if not isinstance(labels, torch.Tensor):\n        raise TypeError(\"labels must be a tensor\")\n    if not labels.ndim == 1:\n        raise ValueError(\"labels must be a 1D tensor\")\n    if not labels.dtype == torch.bool:\n        raise TypeError(\"labels must be a boolean tensor\")\n    self.labels = labels\n</code></pre>"},{"location":"probability_models/default_perturbation_effect_adjustment_function/","title":"Probability Models","text":"<p>Default function to adjust the mean of the perturbation effect based on the enrichment score.</p> <p>All functions that are passed to generate_perturbation_effects() in the argument adjustment_function must have the same signature as this function.</p> <p>Parameters:</p> Name Type Description Default <code>binding_enrichment_data</code> <code>Tensor</code> <p>A tensor of enrichment scores for each gene with dimensions [n_genes, n_tfs, 3] where the entries in the third dimension are a matrix with columns [label, enrichment, pvalue].</p> required <code>bound_mean</code> <code>float</code> <p>The mean for bound genes.</p> required <code>unbound_mean</code> <code>float</code> <p>The mean for unbound genes.</p> required <code>max_adjustment</code> <code>float</code> <p>The maximum adjustment to the base mean based on enrichment.</p> required <code>tf_relationships</code> <code>dict[int, list[int]], optional</code> <p>Unused in this function. It is only here to match the signature of the other adjustment functions.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Adjusted mean as a tensor.</p> Source code in <code>yeastdnnexplorer/probability_models/generate_data.py</code> <pre><code>def default_perturbation_effect_adjustment_function(\n    binding_enrichment_data: torch.Tensor,\n    bound_mean: float,\n    unbound_mean: float,\n    max_adjustment: float,\n    **kwargs,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Default function to adjust the mean of the perturbation effect based on the\n    enrichment score.\n\n    All functions that are passed to generate_perturbation_effects() in the argument\n    adjustment_function must have the same signature as this function.\n\n    :param binding_enrichment_data: A tensor of enrichment scores for each gene with\n        dimensions [n_genes, n_tfs, 3] where the entries in the third dimension are a\n        matrix with columns [label, enrichment, pvalue].\n    :type binding_enrichment_data: torch.Tensor\n    :param bound_mean: The mean for bound genes.\n    :type bound_mean: float\n    :param unbound_mean: The mean for unbound genes.\n    :type unbound_mean: float\n    :param max_adjustment: The maximum adjustment to the base mean based on enrichment.\n    :type max_adjustment: float\n    :param tf_relationships: Unused in this function. It is only here to match the\n        signature of the other adjustment functions.\n    :type tf_relationships: dict[int, list[int]], optional\n    :return: Adjusted mean as a tensor.\n    :rtype: torch.Tensor\n\n    \"\"\"\n    # Extract bound/unbound labels and enrichment scores\n    bound_labels = binding_enrichment_data[:, :, 0]\n    enrichment_scores = binding_enrichment_data[:, :, 1]\n\n    adjusted_mean_matrix = torch.where(\n        bound_labels == 1, enrichment_scores, torch.zeros_like(enrichment_scores)\n    )\n\n    for gene_idx in range(bound_labels.shape[0]):\n        for tf_index in range(bound_labels.shape[1]):\n            if bound_labels[gene_idx, tf_index] == 1:\n                # divide its enrichment score by the maximum magnitude possible to\n                # create an adjustment multipler that scales with increasing enrichment\n                adjustment_multiplier = enrichment_scores[gene_idx, tf_index] / abs(\n                    enrichment_scores.max()\n                )\n\n                # randomly adjust the gene by some portion of the max adjustment\n                adjusted_mean_matrix[gene_idx, tf_index] = bound_mean + (\n                    adjustment_multiplier * max_adjustment\n                )\n            else:\n                # related tfs are not all bound, so set the enrichment\n                # score to unbound mean\n                adjusted_mean_matrix[gene_idx, tf_index] = unbound_mean\n\n    return adjusted_mean_matrix\n</code></pre>"},{"location":"probability_models/generate_binding_effects/","title":"Generate binding effects","text":"<p>Generate enrichment effects for genes using vectorized operations, based on their bound designation, with separate experiment hops ranges for unbound and bound genes.</p> <p>Note that the default values are a scaled down version of actual data. See also https://github.com/cmatKhan/callingCardsTools/blob/main/callingcardstools/PeakCalling/yeast/enrichment.py</p> <p>Parameters:</p> Name Type Description Default <code>gene_population</code> <code>GenePopulation</code> <p>A GenePopulation object. See <code>generate_gene_population()</code></p> required <code>background_hops_range</code> <code>tuple[int, int]</code> <p>The range of hops for background genes. Defaults to (1, 100)</p> <code>(1, 100)</code> <code>unbound_experiment_hops_range</code> <code>tuple[int, int]</code> <p>The range of hops for unbound genes. Defaults to (0, 1)</p> <code>(0, 1)</code> <code>bound_experiment_hops_range</code> <code>tuple[int, int]</code> <p>The range of hops for bound genes. Defaults to (1, 6)</p> <code>(1, 6)</code> <code>total_background_hops</code> <code>int</code> <p>The total number of background hops. Defaults to 1000</p> <code>1000</code> <code>total_experiment_hops</code> <code>int</code> <p>The total number of experiment hops. Defaults to 76</p> <code>76</code> <code>pseudocount</code> <code>float</code> <p>A pseudocount to avoid division by zero. Defaults to 1e-10</p> <code>1e-10</code> <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>A tensor of enrichment values for each gene.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If gene_population is not a GenePopulation object</p> <code>TypeError</code> <p>If total_background_hops is not an integer</p> <code>TypeError</code> <p>If total_experiment_hops is not an integer</p> <code>TypeError</code> <p>If pseudocount is not a float</p> <code>TypeError</code> <p>If background_hops_range is not a tuple</p> <code>TypeError</code> <p>If unbound_experiment_hops_range is not a tuple</p> <code>TypeError</code> <p>If bound_experiment_hops_range is not a tuple</p> <code>ValueError</code> <p>If background_hops_range is not a tuple of length 2</p> <code>ValueError</code> <p>If unbound_experiment_hops_range is not a tuple of length 2</p> <code>ValueError</code> <p>If bound_experiment_hops_range is not a tuple of length 2</p> Source code in <code>yeastdnnexplorer/probability_models/generate_data.py</code> <pre><code>def generate_binding_effects(\n    gene_population: GenePopulation,\n    background_hops_range: tuple[int, int] = (1, 100),\n    unbound_experiment_hops_range: tuple[int, int] = (0, 1),\n    bound_experiment_hops_range: tuple[int, int] = (1, 6),\n    total_background_hops: int = 1000,\n    total_experiment_hops: int = 76,\n    pseudocount: float = 1e-10,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Generate enrichment effects for genes using vectorized operations, based on their\n    bound designation, with separate experiment hops ranges for unbound and bound genes.\n\n    Note that the default values are a scaled down version of actual data. See also\n    https://github.com/cmatKhan/callingCardsTools/blob/main/callingcardstools/PeakCalling/yeast/enrichment.py\n\n    :param gene_population: A GenePopulation object. See `generate_gene_population()`\n    :type gene_population: GenePopulation\n    :param background_hops_range: The range of hops for background genes. Defaults to\n        (1, 100)\n    :type background_hops_range: Tuple[int, int], optional\n    :param unbound_experiment_hops_range: The range of hops for unbound genes. Defaults\n        to (0, 1)\n    :type unbound_experiment_hops_range: Tuple[int, int], optional\n    :param bound_experiment_hops_range: The range of hops for bound genes. Defaults to\n        (1, 6)\n    :type bound_experiment_hops_range: Tuple[int, int], optional\n    :param total_background_hops: The total number of background hops. Defaults to 1000\n    :type total_background_hops: int, optional\n    :param total_experiment_hops: The total number of experiment hops. Defaults to 76\n    :type total_experiment_hops: int, optional\n    :param pseudocount: A pseudocount to avoid division by zero. Defaults to 1e-10\n    :type pseudocount: float, optional\n    :return: A tensor of enrichment values for each gene.\n    :rtype: torch.Tensor\n    :raises TypeError: If gene_population is not a GenePopulation object\n    :raises TypeError: If total_background_hops is not an integer\n    :raises TypeError: If total_experiment_hops is not an integer\n    :raises TypeError: If pseudocount is not a float\n    :raises TypeError: If background_hops_range is not a tuple\n    :raises TypeError: If unbound_experiment_hops_range is not a tuple\n    :raises TypeError: If bound_experiment_hops_range is not a tuple\n    :raises ValueError: If background_hops_range is not a tuple of length 2\n    :raises ValueError: If unbound_experiment_hops_range is not a tuple of length 2\n    :raises ValueError: If bound_experiment_hops_range is not a tuple of length 2\n\n    \"\"\"\n    # NOTE: torch intervals are half open on the right, so we add 1 to the\n    # high end of the range to make it inclusive\n\n    # check input\n    if not isinstance(gene_population, GenePopulation):\n        raise TypeError(\"gene_population must be a GenePopulation object\")\n    if not isinstance(total_background_hops, int):\n        raise TypeError(\"total_background_hops must be an integer\")\n    if not isinstance(total_experiment_hops, int):\n        raise TypeError(\"total_experiment_hops must be an integer\")\n    if not isinstance(pseudocount, float):\n        raise TypeError(\"pseudocount must be a float\")\n    for arg, tup in {\n        \"background_hops_range\": background_hops_range,\n        \"unbound_experiment_hops_range\": unbound_experiment_hops_range,\n        \"bound_experiment_hops_range\": bound_experiment_hops_range,\n    }.items():\n        if not isinstance(tup, tuple):\n            raise TypeError(f\"{arg} must be a tuple\")\n        if not len(tup) == 2:\n            raise ValueError(f\"{arg} must be a tuple of length 2\")\n        if not all(isinstance(i, int) for i in tup):\n            raise TypeError(f\"{arg} must be a tuple of integers\")\n\n    # Generate background hops for all genes\n    background_hops = torch.randint(\n        low=background_hops_range[0],\n        high=background_hops_range[1] + 1,\n        size=(gene_population.labels.shape[0],),\n    )\n\n    # Generate experiment hops unbound genes\n    unbound_experiment_hops = torch.randint(\n        low=unbound_experiment_hops_range[0],\n        high=unbound_experiment_hops_range[1] + 1,\n        size=(gene_population.labels.shape[0],),\n    )\n    # Generate experiment hops bound genes\n    bound_experiment_hops = torch.randint(\n        low=bound_experiment_hops_range[0],\n        high=bound_experiment_hops_range[1] + 1,\n        size=(gene_population.labels.shape[0],),\n    )\n\n    # Use bound designation to select appropriate experiment hops\n    experiment_hops = torch.where(\n        gene_population.labels == 1, bound_experiment_hops, unbound_experiment_hops\n    )\n\n    # Calculate enrichment for all genes\n    return (experiment_hops.float() / (total_experiment_hops + pseudocount)) / (\n        (background_hops.float() / (total_background_hops + pseudocount)) + pseudocount\n    )\n</code></pre>"},{"location":"probability_models/generate_gene_population/","title":"Generate Gene Population","text":"<p>Generate two sets of genes, one of which will be considered genes which show a bound, and the other which does not. The return is a one dimensional boolean tensor where a value of \u20180\u2019 means that the gene at that index is part of the unbound group and a \u20181\u2019 means the gene at that index is part of the bound group. The length of the tensor is the number of genes in this simulated organism.</p> <p>Parameters:</p> Name Type Description Default <code>total</code> <code>int</code> <p>The total number of genes. defaults to 1000</p> <code>1000</code> <code>bound_group</code> <code>float</code> <p>The proportion of genes in the bound group. defaults to 0.3</p> <code>0.3</code> <p>Returns:</p> Type Description <code>GenePopulation</code> <p>A one dimensional tensor of boolean values where the set of indices with a value of \u20181\u2019 are the bound group and the set of indices with a value of \u20180\u2019 are the unbound group.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>if total is not an integer</p> <code>ValueError</code> <p>If bound_group is not between 0 and 1</p> Source code in <code>yeastdnnexplorer/probability_models/generate_data.py</code> <pre><code>def generate_gene_population(\n    total: int = 1000, bound_group: float = 0.3\n) -&gt; GenePopulation:\n    \"\"\"\n    Generate two sets of genes, one of which will be considered genes which show a\n    bound, and the other which does not. The return is a one dimensional boolean tensor\n    where a value of '0' means that the gene at that index is part of the unbound group\n    and a '1' means the gene at that index is part of the bound group. The length of the\n    tensor is the number of genes in this simulated organism.\n\n    :param total: The total number of genes. defaults to 1000\n    :type total: int, optional\n    :param bound_group: The proportion of genes in the bound group. defaults to 0.3\n    :type bound_group: float, optional\n    :return: A one dimensional tensor of boolean values where the set of indices with a\n        value of '1' are the bound group and the set of indices with a value of '0' are\n        the unbound group.\n    :rtype: GenePopulation\n    :raises TypeError: if total is not an integer\n    :raises ValueError: If bound_group is not between 0 and 1\n\n    \"\"\"\n    if not isinstance(total, int):\n        raise TypeError(\"total must be an integer\")\n    if not 0 &lt;= bound_group &lt;= 1:\n        raise ValueError(\"bound_group must be between 0 and 1\")\n\n    bound_group_size = int(total * bound_group)\n    logger.info(\"Generating %s genes with bound\", bound_group_size)\n\n    labels = torch.cat(\n        (\n            torch.ones(bound_group_size, dtype=torch.bool),\n            torch.zeros(total - bound_group_size, dtype=torch.bool),\n        )\n    )[torch.randperm(total)]\n\n    return GenePopulation(labels)\n</code></pre>"},{"location":"probability_models/generate_perturbation_effects/","title":"Generate perturbation effects","text":"<p>Generate perturbation effects for genes.</p> <p>If <code>max_mean_adjustment</code> is greater than 0, then the mean of the effects are adjusted based on the binding_data and the function passed in <code>adjustment_function</code>. See <code>default_perturbation_effect_adjustment_function()</code> for the default option. If <code>max_mean_adjustment</code> is 0, then the mean is not adjusted. Additional keyword arguments may be passed in that will be passed along to the adjustment function.</p> <p>Parameters:</p> Name Type Description Default <code>binding_data</code> <code>Tensor</code> <p>A tensor of binding data with dimensions [n_genes, n_tfs, 3] where the entries in the third dimension are a matrix with columns [label, enrichment, pvalue].</p> required <code>tf_index</code> <code>int | None</code> <p>The index of the TF in the binding_data tensor. Not used if we are adjusting the means (ie only used if max_mean_adjustment == 0). Defaults to None</p> <code>None</code> <code>unbound_mean</code> <code>float</code> <p>The mean for unbound genes. Defaults to 0.0</p> <code>0.0</code> <code>unbound_std</code> <code>float</code> <p>The standard deviation for unbound genes. Defaults to 1.0</p> <code>1.0</code> <code>bound_mean</code> <code>float</code> <p>The mean for bound genes. Defaults to 3.0</p> <code>3.0</code> <code>bound_std</code> <code>float</code> <p>The standard deviation for bound genes. Defaults to 1.0</p> <code>1.0</code> <code>max_mean_adjustment</code> <code>float</code> <p>The maximum adjustment to the base mean based on enrichment. Defaults to 0.0</p> <code>0.0</code> <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>A tensor of perturbation effects for each gene.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If binding_data is not a 3D tensor with the third dimension having a length of 3</p> <code>ValueError</code> <p>If unbound_mean, unbound_std, bound_mean, bound_std, or max_mean_adjustment are not floats</p> Source code in <code>yeastdnnexplorer/probability_models/generate_data.py</code> <pre><code>def generate_perturbation_effects(\n    binding_data: torch.Tensor,\n    tf_index: int | None = None,\n    unbound_mean: float = 0.0,\n    unbound_std: float = 1.0,\n    bound_mean: float = 3.0,\n    bound_std: float = 1.0,\n    max_mean_adjustment: float = 0.0,\n    adjustment_function: Callable[\n        [torch.Tensor, float, float, float], torch.Tensor\n    ] = default_perturbation_effect_adjustment_function,\n    **kwargs,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Generate perturbation effects for genes.\n\n    If `max_mean_adjustment` is greater than 0, then the mean of the\n    effects are adjusted based on the binding_data and the function passed\n    in `adjustment_function`. See `default_perturbation_effect_adjustment_function()`\n    for the default option. If `max_mean_adjustment` is 0, then the mean\n    is not adjusted. Additional keyword arguments may be passed in that will be\n    passed along to the adjustment function.\n\n    :param binding_data: A tensor of binding data with dimensions [n_genes, n_tfs, 3]\n        where the entries in the third dimension are a matrix with columns\n        [label, enrichment, pvalue].\n    :type binding_data: torch.Tensor\n    :param tf_index: The index of the TF in the binding_data tensor. Not used if we\n        are adjusting the means (ie only used if max_mean_adjustment == 0).\n        Defaults to None\n    :type tf_index: int\n    :param unbound_mean: The mean for unbound genes. Defaults to 0.0\n    :type unbound_mean: float, optional\n    :param unbound_std: The standard deviation for unbound genes. Defaults to 1.0\n    :type unbound_std: float, optional\n    :param bound_mean: The mean for bound genes. Defaults to 3.0\n    :type bound_mean: float, optional\n    :param bound_std: The standard deviation for bound genes. Defaults to 1.0\n    :type bound_std: float, optional\n    :param max_mean_adjustment: The maximum adjustment to the base mean based\n        on enrichment. Defaults to 0.0\n    :type max_mean_adjustment: float, optional\n\n    :return: A tensor of perturbation effects for each gene.\n    :rtype: torch.Tensor\n\n    :raises ValueError: If binding_data is not a 3D tensor with the third\n        dimension having a length of 3\n    :raises ValueError: If unbound_mean, unbound_std, bound_mean, bound_std,\n        or max_mean_adjustment are not floats\n\n    \"\"\"\n    # check that a valid combination of inputs has been passed in\n    if max_mean_adjustment == 0.0 and tf_index is None:\n        raise ValueError(\"If max_mean_adjustment is 0, then tf_index must be specified\")\n\n    if binding_data.ndim != 3 or binding_data.shape[2] != 3:\n        raise ValueError(\n            \"enrichment_tensor must have dimensions [num_genes, num_TFs, \"\n            \"[label, enrichment, pvalue]]\"\n        )\n    # check the rest of the inputs\n    if not all(\n        isinstance(i, float)\n        for i in (unbound_mean, unbound_std, bound_mean, bound_std, max_mean_adjustment)\n    ):\n        raise ValueError(\n            \"unbound_mean, unbound_std, bound_mean, bound_std, \"\n            \"and max_mean_adjustment must be floats\"\n        )\n    # check the Callable signature\n    if not all(\n        i in inspect.signature(adjustment_function).parameters\n        for i in (\n            \"binding_enrichment_data\",\n            \"bound_mean\",\n            \"unbound_mean\",\n            \"max_adjustment\",\n        )\n    ):\n        raise ValueError(\n            \"adjustment_function must have the signature \"\n            \"(binding_enrichment_data, bound_mean, unbound_mean, max_adjustment)\"\n        )\n\n    # Initialize an effects tensor for all genes\n    effects = torch.empty(\n        binding_data.size(0), dtype=torch.float32, device=binding_data.device\n    )\n\n    # Randomly assign signs for each gene\n    # fmt: off\n    signs = torch.randint(0, 2, (effects.size(0),),\n                          dtype=torch.float32,\n                          device=binding_data.device) * 2 - 1\n    # fmt: on\n\n    # Apply adjustments to the base mean for the bound genes, if necessary\n    if max_mean_adjustment &gt; 0 and adjustment_function is not None:\n        # Assuming adjustment_function returns a vector of means for each gene.\n        # bound genes that meet the criteria for adjustment will be affected by\n        # the status of the TFs. What TFs affect a given gene must be specified by\n        # the adjustment_function()\n        adjusted_means = adjustment_function(\n            binding_data,\n            bound_mean,\n            unbound_mean,\n            max_mean_adjustment,\n            **kwargs,\n        )\n\n        # add adjustments, ensuring they respect the original sign\n        if adjusted_means.ndim == 1:\n            effects = signs * torch.abs(\n                torch.normal(mean=adjusted_means, std=bound_std)\n            )\n        else:\n            effects = torch.zeros_like(adjusted_means)\n            for col_idx in range(effects.size(1)):\n                effects[:, col_idx] = signs * torch.abs(\n                    torch.normal(mean=adjusted_means[:, col_idx], std=bound_std)\n                )\n    else:\n        bound_mask = binding_data[:, tf_index, 0] == 1\n\n        # Generate effects based on the unbound and bound means, applying the sign\n        effects[~bound_mask] = signs[~bound_mask] * torch.abs(\n            torch.normal(\n                mean=unbound_mean, std=unbound_std, size=(torch.sum(~bound_mask),)\n            )\n        )\n        effects[bound_mask] = signs[bound_mask] * torch.abs(\n            torch.normal(mean=bound_mean, std=bound_std, size=(torch.sum(bound_mask),))\n        )\n\n    return effects\n</code></pre>"},{"location":"probability_models/generate_pvalues/","title":"Generate pvalues","text":"<p>Generate p-values for genes where larger effects are less likely to be false positives.</p> <p>Parameters:</p> Name Type Description Default <code>effects</code> <code>Tensor</code> <p>A tensor of effects</p> required <code>large_effect_percentile</code> <code>float</code> <p>The percentile of effects that are considered large effects. Defaults to 0.9</p> <code>0.9</code> <code>large_effect_upper_pval</code> <code>float</code> <p>The upper bound of the p-values for large effects. Defaults to 0.2</p> <code>0.2</code> <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>A tensor of p-values</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If effects is not a tensor or the values themselves are not numeric</p> <code>ValueError</code> <p>If large_effect_percentile is not between 0 and 1</p> <code>ValueError</code> <p>If large_effect_upper_pval is not between 0 and 1</p> Source code in <code>yeastdnnexplorer/probability_models/generate_data.py</code> <pre><code>def generate_pvalues(\n    effects: torch.Tensor,\n    large_effect_percentile: float = 0.9,\n    large_effect_upper_pval: float = 0.2,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Generate p-values for genes where larger effects are less likely to be false\n    positives.\n\n    :param effects: A tensor of effects\n    :type effects: torch.Tensor\n    :param large_effect_percentile: The percentile of effects that are considered large\n        effects. Defaults to 0.9\n    :type large_effect_percentile: float, optional\n    :param large_effect_upper_pval: The upper bound of the p-values for large effects.\n        Defaults to 0.2\n    :return: A tensor of p-values\n    :rtype: torch.Tensor\n    :raises ValueError: If effects is not a tensor or the values themselves are not\n        numeric\n    :raises ValueError: If large_effect_percentile is not between 0 and 1\n    :raises ValueError: If large_effect_upper_pval is not between 0 and 1\n\n    \"\"\"\n    # check inputs\n    if not isinstance(effects, torch.Tensor):\n        raise ValueError(\"effects must be a tensor\")\n    if not torch.is_floating_point(effects):\n        raise ValueError(\"effects must be numeric\")\n    if not 0 &lt;= large_effect_percentile &lt;= 1:\n        raise ValueError(\"large_effect_percentile must be between 0 and 1\")\n    if not 0 &lt;= large_effect_upper_pval &lt;= 1:\n        raise ValueError(\"large_effect_upper_pval must be between 0 and 1\")\n\n    # Generate p-values\n    pvalues = torch.rand(effects.shape[0])\n\n    # Draw p-values from a uniform distribution where larger abs(effects) are\n    # less likely to be false positives\n    large_effect_threshold = torch.quantile(torch.abs(effects), large_effect_percentile)\n    large_effect_mask = torch.abs(effects) &gt;= large_effect_threshold\n    pvalues[large_effect_mask] = (\n        torch.rand(torch.sum(large_effect_mask)) * large_effect_upper_pval\n    )\n\n    return pvalues\n</code></pre>"},{"location":"probability_models/perturbation_effect_adjustment_function_with_tf_relationships/","title":"Perturbation effect adjustment function with tf relationships","text":"<p>Adjust the mean of the perturbation effect based on the enrichment score and the provided relationships between TFs. For each gene, the mean of the TF-gene pair\u2019s perturbation effect will be adjusted if the TF is bound to the gene and all related TFs are also bound to the gene. The adjustment will be a random value not exceeding the maximum adjustment.</p> <p>Parameters:</p> Name Type Description Default <code>binding_enrichment_data</code> <code>Tensor</code> <p>A tensor of enrichment scores for each gene with dimensions [n_genes, n_tfs, 3] where the entries in the third dimension are a matrix with columns [label, enrichment, pvalue].</p> required <code>bound_mean</code> <code>float</code> <p>The mean for bound genes.</p> required <code>unbound_mean</code> <code>float</code> <p>The mean for unbound genes.</p> required <code>max_adjustment</code> <code>float</code> <p>The maximum adjustment to the base mean based on enrichment.</p> required <code>tf_relationships</code> <code>dict[int, list[int]]</code> <p>A dictionary where the keys are the indices of the TFs and the values are lists of indices of other TFs that are related to the key TF.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Adjusted mean as a tensor.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If tf_relationships is not a dictionary between ints and lists of ints</p> <code>ValueError</code> <p>If the tf_relationships dict does not have the same number of TFs as the binding_data tensor passed into the function</p> <code>ValueError</code> <p>If the tf_relationships dict has any TFs in the values that are not also in the keys or any key or value TFs that are out of bounds for the binding_data tensor</p> Source code in <code>yeastdnnexplorer/probability_models/generate_data.py</code> <pre><code>def perturbation_effect_adjustment_function_with_tf_relationships(\n    binding_enrichment_data: torch.Tensor,\n    bound_mean: float,\n    unbound_mean: float,\n    max_adjustment: float,\n    tf_relationships: dict[int, list[int]],\n) -&gt; torch.Tensor:\n    \"\"\"\n    Adjust the mean of the perturbation effect based on the enrichment score and the\n    provided relationships between TFs. For each gene, the mean of the TF-gene pair's\n    perturbation effect will be adjusted if the TF is bound to the gene and all related\n    TFs are also bound to the gene. The adjustment will be a random value not exceeding\n    the maximum adjustment.\n\n    :param binding_enrichment_data: A tensor of enrichment scores for each gene with\n        dimensions [n_genes, n_tfs, 3] where the entries in the third dimension are a\n        matrix with columns [label, enrichment, pvalue].\n    :type binding_enrichment_data: torch.Tensor\n    :param bound_mean: The mean for bound genes.\n    :type bound_mean: float\n    :param unbound_mean: The mean for unbound genes.\n    :type unbound_mean: float\n    :param max_adjustment: The maximum adjustment to the base mean based on enrichment.\n    :type max_adjustment: float\n    :param tf_relationships: A dictionary where the keys are the indices of the TFs and\n        the values are lists of indices of other TFs that are related to the key TF.\n    :type tf_relationships: dict[int, list[int]]\n    :return: Adjusted mean as a tensor.\n    :rtype: torch.Tensor\n    :raises ValueError: If tf_relationships is not a dictionary between ints and lists\n        of ints\n    :raises ValueError: If the tf_relationships dict does not have the same number of\n        TFs as the binding_data tensor passed into the function\n    :raises ValueError: If the tf_relationships dict has any TFs in the values that are\n        not also in the keys or any key or value TFs that are out of bounds for the\n        binding_data tensor\n\n    \"\"\"\n    if (\n        not isinstance(tf_relationships, dict)\n        or not all(isinstance(v, list) for v in tf_relationships.values())\n        or not all(isinstance(k, int) for k in tf_relationships.keys())\n        or not all(isinstance(i, int) for v in tf_relationships.values() for i in v)\n    ):\n        raise ValueError(\n            \"tf_relationships must be a dictionary between ints and lists of ints\"\n        )\n    if not all(\n        k in range(binding_enrichment_data.shape[1]) for k in tf_relationships.keys()\n    ) or not all(\n        i in range(binding_enrichment_data.shape[1])\n        for v in tf_relationships.values()\n        for i in v\n    ):\n        raise ValueError(\n            \"all keys and values in tf_relationships must be within the \\\n                  bounds of the binding_data tensor's number of TFs\"\n        )\n    if not len(tf_relationships) == binding_enrichment_data.shape[1]:\n        raise ValueError(\n            \"tf_relationships must have the same number of TFs as the \\\n                binding_data tensor passed into the function\"\n        )\n\n    # Extract bound/unbound labels and enrichment scores\n    bound_labels = binding_enrichment_data[:, :, 0]  # shape: (num_genes, num_tfs)\n    enrichment_scores = binding_enrichment_data[:, :, 1]  # shape: (num_genes, num_tfs)\n\n    # we set all unbound scores to 0, then we will go through and also\n    # set any bound scores to unbound_mean if the related tfs are not also bound\n    adjusted_mean_matrix = torch.where(\n        bound_labels == 1, enrichment_scores, torch.zeros_like(enrichment_scores)\n    )  # shape: (num_genes, num_tfs)\n\n    for gene_idx in range(bound_labels.shape[0]):\n        for tf_index, related_tfs in tf_relationships.items():\n            if bound_labels[gene_idx, tf_index] == 1 and torch.all(\n                bound_labels[gene_idx, related_tfs] == 1\n            ):\n                # draw a random value between 0 and 1 to use to\n                # control magnitude of adjustment\n                adjustment_multiplier = torch.rand(1)\n\n                # randomly adjust the gene by some portion of the max adjustment\n                adjusted_mean_matrix[gene_idx, tf_index] = bound_mean + (\n                    adjustment_multiplier * max_adjustment\n                )\n            else:\n                # related tfs are not all bound, set the enrichment score to unbound\n                # mean\n                adjusted_mean_matrix[gene_idx, tf_index] = unbound_mean\n\n    return adjusted_mean_matrix  # shape (num_genes, num_tfs)\n</code></pre>"},{"location":"probability_models/perturbation_effect_adjustment_function_with_tf_relationships_boolean_logic/","title":"Perturbation effect adjustment function with tf relationships boolean logic","text":"<p>Adjust the mean of the perturbation effect based on the enrichment score and the provided binary / boolean or unary relationships between TFs. For each gene, the mean of the TF-gene pair\u2019s perturbation effect will be adjusted if the TF is bound to the gene and all of the Relations associated with the TF are satisfied (ie they evaluate to True). These relations could be unary conditions or Ands or Ors between TFs. A TF being bound corresponds to a true value, which means And(4, 5) would be satisfied is both TF 4 and TF 5 are bound to the gene in question. The adjustment will be a random value not exceeding the maximum adjustment.</p> <p>Parameters:</p> Name Type Description Default <code>binding_enrichment_data</code> <code>Tensor</code> <p>A tensor of enrichment scores for each gene with dimensions [n_genes, n_tfs, 3] where the entries in the third dimension are a matrix with columns [label, enrichment, pvalue].</p> required <code>bound_mean</code> <code>float</code> <p>The mean for bound genes.</p> required <code>unbound_mean</code> <code>float</code> <p>The mean for unbound genes.</p> required <code>max_adjustment</code> <code>float</code> <p>The maximum adjustment to the base mean based on enrichment.</p> required <code>tf_relationships</code> <code>dict[int, list[Relation]]</code> <p>A dictionary where the keys are TF indices and the values are lists of Relation objects that represent the conditions that must be met for the mean of the perturbation effect associated with the TF-gene pair to be adjusted.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Adjusted mean as a tensor.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If tf_relationships is not a dictionary between ints and lists of Relations</p> <code>ValueError</code> <p>If the tf_relationships dict does not have the same number of TFs as the binding_data tensor passed into the function</p> <code>ValueError</code> <p>If the tf_relationships dict has any TFs in the values that are not also in the keys or any key or value TFs that are out of bounds for the binding_data tensor</p> Source code in <code>yeastdnnexplorer/probability_models/generate_data.py</code> <pre><code>def perturbation_effect_adjustment_function_with_tf_relationships_boolean_logic(\n    binding_enrichment_data: torch.Tensor,\n    bound_mean: float,\n    unbound_mean: float,\n    max_adjustment: float,\n    tf_relationships: dict[int, list[Relation]],\n) -&gt; torch.Tensor:\n    \"\"\"\n    Adjust the mean of the perturbation effect based on the enrichment score and the\n    provided binary / boolean or unary relationships between TFs. For each gene, the\n    mean of the TF-gene pair's perturbation effect will be adjusted if the TF is bound\n    to the gene and all of the Relations associated with the TF are satisfied (ie they\n    evaluate to True). These relations could be unary conditions or Ands or Ors between\n    TFs. A TF being bound corresponds to a true value, which means And(4, 5) would be\n    satisfied is both TF 4 and TF 5 are bound to the gene in question. The adjustment\n    will be a random value not exceeding the maximum adjustment.\n\n    :param binding_enrichment_data: A tensor of enrichment scores for each gene with\n        dimensions [n_genes, n_tfs, 3] where the entries in the third dimension are a\n        matrix with columns [label, enrichment, pvalue].\n    :type binding_enrichment_data: torch.Tensor\n    :param bound_mean: The mean for bound genes.\n    :type bound_mean: float\n    :param unbound_mean: The mean for unbound genes.\n    :type unbound_mean: float\n    :param max_adjustment: The maximum adjustment to the base mean based on enrichment.\n    :type max_adjustment: float\n    :param tf_relationships: A dictionary where the keys are TF indices and the values\n        are lists of Relation objects that represent the conditions that must be met for\n        the mean of the perturbation effect associated with the TF-gene pair to be\n        adjusted.\n    :type tf_relationships: dict[int, list[Relation]]\n    :return: Adjusted mean as a tensor.\n    :rtype: torch.Tensor\n    :raises ValueError: If tf_relationships is not a dictionary between ints and lists\n        of Relations\n    :raises ValueError: If the tf_relationships dict does not have the same number of\n        TFs as the binding_data tensor passed into the function\n    :raises ValueError: If the tf_relationships dict has any TFs in the values that are\n        not also in the keys or any key or value TFs that are out of bounds for the\n        binding_data tensor\n\n    \"\"\"\n    if (\n        not isinstance(tf_relationships, dict)\n        or not all(isinstance(v, list) for v in tf_relationships.values())\n        or not all(isinstance(k, int) for k in tf_relationships.keys())\n        or not all(\n            isinstance(i, Relation) for v in tf_relationships.values() for i in v\n        )\n    ):\n        raise ValueError(\n            \"tf_relationships must be a dictionary between \\\n                ints and lists of Relation objects\"\n        )\n    if not all(\n        k in range(binding_enrichment_data.shape[1]) for k in tf_relationships.keys()\n    ):\n        raise ValueError(\n            \"all TFs mentioned in tf_relationships must be within \\\n                the bounds of the binding_data tensor's number of TFs\"\n        )\n    if not len(tf_relationships) == binding_enrichment_data.shape[1]:\n        raise ValueError(\n            \"tf_relationships must have the same number of TFs as \\\n                the binding_data tensor passed into the function\"\n        )\n\n    # Extract bound/unbound labels and enrichment scores\n    bound_labels = binding_enrichment_data[:, :, 0]  # shape: (num_genes, num_tfs)\n    enrichment_scores = binding_enrichment_data[:, :, 1]  # shape: (num_genes, num_tfs)\n\n    # we set all unbound scores to 0, then we will go through and also set any\n    # bound scores to unbound_mean if the related boolean statements are not satisfied\n    adjusted_mean_matrix = torch.where(\n        bound_labels == 1, enrichment_scores, torch.zeros_like(enrichment_scores)\n    )  # shape: (num_genes, num_tfs)\n\n    for gene_idx in range(bound_labels.shape[0]):\n        for tf_index, relations in tf_relationships.items():\n            # check if all relations (boolean relationships)\n            # associated with TFs are satisfied\n            if bound_labels[gene_idx, tf_index] == 1 and all(\n                relation.evaluate(bound_labels[gene_idx].tolist())\n                for relation in relations\n            ):\n                # draw a random value between 0 and 1 to use to\n                # control magnitude of adjustment\n                adjustment_multiplier = torch.rand(1)\n\n                # randomly adjust the gene by some portion of the max adjustment\n                adjusted_mean_matrix[gene_idx, tf_index] = bound_mean + (\n                    adjustment_multiplier * max_adjustment\n                )\n            else:\n                # related tfs are not all bound, set the enrichment score to unbound\n                # mean\n                adjusted_mean_matrix[gene_idx, tf_index] = unbound_mean\n\n    return adjusted_mean_matrix  # shape (num_genes, num_tfs)\n</code></pre>"},{"location":"probability_models/relation_classes/","title":"Relation classes","text":""},{"location":"probability_models/relation_classes/#yeastdnnexplorer.probability_models.relation_classes.And","title":"<code>And</code>","text":"<p>             Bases: <code>Relation</code></p> <p>Class for representing the logical AND of multiple conditions Allows nesed conditions, i.e. And(1, Or(2, 3))</p> Source code in <code>yeastdnnexplorer/probability_models/relation_classes.py</code> <pre><code>class And(Relation):\n    \"\"\"Class for representing the logical AND of multiple conditions Allows nesed\n    conditions, i.e. And(1, Or(2, 3))\"\"\"\n\n    def __init__(self, *conditions):\n        \"\"\"\n        :param conditions: List of conditions to be evaluated\n        :type conditions: List[float | Relation]\n        \"\"\"\n        self.conditions = conditions\n\n    def evaluate(self, bound_vec):\n        \"\"\"\n        Returns true if the And() condition evaluates to true Evaluates nested\n        conditions as needed.\n\n        :param bound_vec: Vector of TF indices (0 or 1) indicating which TFs are bound\n            for the gene in question\n        :type bound_vec: List[float]\n\n        \"\"\"\n        if type(bound_vec) is not list or not all(\n            isinstance(x, float) for x in bound_vec\n        ):\n            raise ValueError(\"bound_vec must be a list of floats\")\n\n        if not self.conditions:\n            return True\n\n        # Each condition can be an index or another Relation (And/Or)\n        return all(\n            c.evaluate(bound_vec) if isinstance(c, Relation) else bound_vec[c]\n            for c in self.conditions\n        )\n\n    def __str__(self):\n        return f\"AND({', '.join(str(c) for c in self.conditions)})\"\n</code></pre>"},{"location":"probability_models/relation_classes/#yeastdnnexplorer.probability_models.relation_classes.And.__init__","title":"<code>__init__(*conditions)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>conditions</code> <code>List[float | Relation]</code> <p>List of conditions to be evaluated</p> <code>()</code> Source code in <code>yeastdnnexplorer/probability_models/relation_classes.py</code> <pre><code>def __init__(self, *conditions):\n    \"\"\"\n    :param conditions: List of conditions to be evaluated\n    :type conditions: List[float | Relation]\n    \"\"\"\n    self.conditions = conditions\n</code></pre>"},{"location":"probability_models/relation_classes/#yeastdnnexplorer.probability_models.relation_classes.And.evaluate","title":"<code>evaluate(bound_vec)</code>","text":"<p>Returns true if the And() condition evaluates to true Evaluates nested conditions as needed.</p> <p>Parameters:</p> Name Type Description Default <code>bound_vec</code> <code>List[float]</code> <p>Vector of TF indices (0 or 1) indicating which TFs are bound for the gene in question</p> required Source code in <code>yeastdnnexplorer/probability_models/relation_classes.py</code> <pre><code>def evaluate(self, bound_vec):\n    \"\"\"\n    Returns true if the And() condition evaluates to true Evaluates nested\n    conditions as needed.\n\n    :param bound_vec: Vector of TF indices (0 or 1) indicating which TFs are bound\n        for the gene in question\n    :type bound_vec: List[float]\n\n    \"\"\"\n    if type(bound_vec) is not list or not all(\n        isinstance(x, float) for x in bound_vec\n    ):\n        raise ValueError(\"bound_vec must be a list of floats\")\n\n    if not self.conditions:\n        return True\n\n    # Each condition can be an index or another Relation (And/Or)\n    return all(\n        c.evaluate(bound_vec) if isinstance(c, Relation) else bound_vec[c]\n        for c in self.conditions\n    )\n</code></pre>"},{"location":"probability_models/relation_classes/#yeastdnnexplorer.probability_models.relation_classes.Or","title":"<code>Or</code>","text":"<p>             Bases: <code>Relation</code></p> Source code in <code>yeastdnnexplorer/probability_models/relation_classes.py</code> <pre><code>class Or(Relation):\n    def __init__(self, *conditions):\n        \"\"\"\n        :param conditions: List of conditions to be evaluated\n        :type conditions: List[int | Relation]\n        \"\"\"\n        self.conditions = conditions\n\n    def evaluate(self, bound_vec):\n        \"\"\"\n        Returns true if the Or() condition evaluates to true Evaluates nested conditions\n        as needed.\n\n        :param bound_vec: Vector of TF indices (0 or 1) indicating which TFs are bound\n            for the gene in question\n        :type bound_vec: List[int]\n\n        \"\"\"\n        if type(bound_vec) is not list or not all(\n            isinstance(x, float) for x in bound_vec\n        ):\n            raise ValueError(\"bound_vec must be a list of floats\")\n\n        if not self.conditions:\n            return True\n\n        # Each condition can be an index or another Relation (And/Or)\n        return any(\n            c.evaluate(bound_vec) if isinstance(c, Relation) else bound_vec[c]\n            for c in self.conditions\n        )\n\n    def __str__(self):\n        return f\"OR({', '.join(str(c) for c in self.conditions)})\"\n</code></pre>"},{"location":"probability_models/relation_classes/#yeastdnnexplorer.probability_models.relation_classes.Or.__init__","title":"<code>__init__(*conditions)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>conditions</code> <code>List[int | Relation]</code> <p>List of conditions to be evaluated</p> <code>()</code> Source code in <code>yeastdnnexplorer/probability_models/relation_classes.py</code> <pre><code>def __init__(self, *conditions):\n    \"\"\"\n    :param conditions: List of conditions to be evaluated\n    :type conditions: List[int | Relation]\n    \"\"\"\n    self.conditions = conditions\n</code></pre>"},{"location":"probability_models/relation_classes/#yeastdnnexplorer.probability_models.relation_classes.Or.evaluate","title":"<code>evaluate(bound_vec)</code>","text":"<p>Returns true if the Or() condition evaluates to true Evaluates nested conditions as needed.</p> <p>Parameters:</p> Name Type Description Default <code>bound_vec</code> <code>List[int]</code> <p>Vector of TF indices (0 or 1) indicating which TFs are bound for the gene in question</p> required Source code in <code>yeastdnnexplorer/probability_models/relation_classes.py</code> <pre><code>def evaluate(self, bound_vec):\n    \"\"\"\n    Returns true if the Or() condition evaluates to true Evaluates nested conditions\n    as needed.\n\n    :param bound_vec: Vector of TF indices (0 or 1) indicating which TFs are bound\n        for the gene in question\n    :type bound_vec: List[int]\n\n    \"\"\"\n    if type(bound_vec) is not list or not all(\n        isinstance(x, float) for x in bound_vec\n    ):\n        raise ValueError(\"bound_vec must be a list of floats\")\n\n    if not self.conditions:\n        return True\n\n    # Each condition can be an index or another Relation (And/Or)\n    return any(\n        c.evaluate(bound_vec) if isinstance(c, Relation) else bound_vec[c]\n        for c in self.conditions\n    )\n</code></pre>"},{"location":"probability_models/relation_classes/#yeastdnnexplorer.probability_models.relation_classes.Relation","title":"<code>Relation</code>","text":"<p>Base class for relations between TF indices.</p> Source code in <code>yeastdnnexplorer/probability_models/relation_classes.py</code> <pre><code>class Relation:\n    \"\"\"Base class for relations between TF indices.\"\"\"\n\n    def evaluate(self, bound_vec: list[int]):\n        raise NotImplementedError\n</code></pre>"},{"location":"tutorials/database_interface/","title":"Database Interface","text":"<pre><code>from yeastdnnexplorer.interface import *\n</code></pre> <pre><code>regulator = RegulatorAPI()\n\nresult = await regulator.read()\nresult.get(\"metadata\")\n</code></pre> <pre>\n<code>WARNING:yeastdnnexplorer.interface.AbstractAPI:No token provided: URL un-validated\n</code>\n</pre> id uploader_id upload_date modifier_id modified_date genomicfeature_id under_development notes regulator_locus_tag regulator_symbol 0 6 1 2024-03-18 1 2024-03-18 18:33:43.005782+00:00 17 False none YAL059W ECM1 1 7 1 2024-03-18 1 2024-03-18 18:33:44.106488+00:00 20 False none YAL056W GPB2 2 8 1 2024-03-18 1 2024-03-18 18:33:44.605193+00:00 21 False none YAL055W PEX22 3 9 1 2024-03-18 1 2024-03-18 18:33:44.891194+00:00 22 False none YAL054C ACS1 4 10 1 2024-03-18 1 2024-03-18 18:33:45.111148+00:00 23 False none YAL053W FLC2 ... ... ... ... ... ... ... ... ... ... ... 1810 1816 1 2024-03-18 1 2024-03-18 23:13:22.054254+00:00 5258 False none YMR168C CEP3 1811 1817 1 2024-03-18 1 2024-03-18 23:41:58.734300+00:00 5878 False none YNR054C ESF2 1812 1818 1 2024-03-25 1 2024-03-25 20:04:21.181017+00:00 569 False none YBR267W REI1 1813 1819 1 2024-03-25 1 2024-03-25 20:04:32.036007+00:00 1171 False none YDR081C PDC2 1814 1820 1 2024-03-25 1 2024-03-25 20:05:53.691658+00:00 5667 False none YNL132W KRE33 <p>1815 rows \u00d7 10 columns</p> <pre><code># First, retrieve only the records -- you'll want to filter these results down before\n# retrieving the files most likely\npss_api = PromoterSetSigAPI()\nresult = await pss_api.read()\nresult.get(\"metadata\")\n</code></pre> <pre>\n<code>WARNING:yeastdnnexplorer.interface.AbstractAPI:No token provided: URL un-validated\n</code>\n</pre> id uploader_id upload_date modifier_id modified_date binding_id promoter_id background_id fileformat_id file 0 8419 1 2024-03-25 1 2024-03-25 20:04:11.646870+00:00 3011 NaN NaN 1 promotersetsig/8419.csv.gz 1 8420 1 2024-03-25 1 2024-03-25 20:04:12.153311+00:00 3012 NaN NaN 1 promotersetsig/8420.csv.gz 2 8421 1 2024-03-25 1 2024-03-25 20:04:12.508328+00:00 3013 NaN NaN 1 promotersetsig/8421.csv.gz 3 8422 1 2024-03-25 1 2024-03-25 20:04:12.835947+00:00 3014 NaN NaN 1 promotersetsig/8422.csv.gz 4 8423 1 2024-03-25 1 2024-03-25 20:04:13.162373+00:00 3015 NaN NaN 1 promotersetsig/8423.csv.gz ... ... ... ... ... ... ... ... ... ... ... 2178 11091 1 2024-03-26 1 2024-03-26 14:30:28.156704+00:00 4481 4.0 6.0 5 promotersetsig/11091.csv.gz 2179 11092 1 2024-03-26 1 2024-03-26 14:30:28.218468+00:00 4480 4.0 6.0 5 promotersetsig/11092.csv.gz 2180 11093 1 2024-03-26 1 2024-03-26 14:30:28.310173+00:00 4482 4.0 6.0 5 promotersetsig/11093.csv.gz 2181 11094 1 2024-03-26 1 2024-03-26 14:30:28.695849+00:00 4483 4.0 6.0 5 promotersetsig/11094.csv.gz 2182 11095 1 2024-03-26 1 2024-03-26 14:30:29.202012+00:00 4484 4.0 6.0 5 promotersetsig/11095.csv.gz <p>2183 rows \u00d7 10 columns</p> <pre><code>pss_api.push_params({\"regulator_symbol\": \"HAP5\",\n                     \"workflow\": \"nf_core_callingcards_dev\",\n                     \"data_usable\": \"pass\"})\n</code></pre> <pre><code># note that retrieve_files is set to True\nresult = await pss_api.read(retrieve_files = True)\n\n# the metadata slot is the same as before\nresult.get(\"metadata\")\n</code></pre> id uploader_id upload_date modifier_id modified_date binding_id promoter_id background_id fileformat_id file 0 10690 1 2024-03-26 1 2024-03-26 14:28:43.825628+00:00 4079 4 6 5 promotersetsig/10690.csv.gz 1 10694 1 2024-03-26 1 2024-03-26 14:28:44.739775+00:00 4083 4 6 5 promotersetsig/10694.csv.gz 2 10754 1 2024-03-26 1 2024-03-26 14:29:01.837335+00:00 4143 4 6 5 promotersetsig/10754.csv.gz 3 10929 1 2024-03-26 1 2024-03-26 14:29:45.379790+00:00 4318 4 6 5 promotersetsig/10929.csv.gz 4 10939 1 2024-03-26 1 2024-03-26 14:29:47.853980+00:00 4327 4 6 5 promotersetsig/10939.csv.gz <pre><code># but now the data slot is a dictionary where the `id` are keys and the values\n# are the files parsed into pandas dataframes\nresult.get(\"data\").get(\"10690\")\n</code></pre> name chr start end strand experiment_hops background_hops background_total_hops experiment_total_hops callingcards_enrichment poisson_pval hypergeometric_pval 0 1 chrI 0 335 + 0 2 103922 8579 0.0 0.305876 1.0 1 2 chrI 0 538 + 0 4 103922 8579 0.0 0.411518 1.0 2 3 chrI 2169 2480 - 0 1 103922 8579 0.0 0.246143 1.0 3 4 chrI 2169 2480 + 0 1 103922 8579 0.0 0.246143 1.0 4 5 chrI 9017 9717 - 0 11 103922 8579 0.0 0.669806 1.0 ... ... ... ... ... ... ... ... ... ... ... ... ... 6703 7118 chrM 66174 66874 - 0 0 103922 8579 0.0 0.181269 1.0 6704 7132 chrM 74513 75213 - 0 0 103922 8579 0.0 0.181269 1.0 6705 7133 chrM 75984 76684 - 0 0 103922 8579 0.0 0.181269 1.0 6706 7137 chrM 80022 80722 - 0 0 103922 8579 0.0 0.181269 1.0 6707 7140 chrM 85709 85779 - 0 0 103922 8579 0.0 0.181269 1.0 <p>6708 rows \u00d7 12 columns</p> <p>Parameters can be removed one by one</p> <pre><code>print(pss_api.params)\n</code></pre> <pre>\n<code>regulator_symbol: HAP5, workflow: nf_core_callingcards_dev, data_usable: pass\n</code>\n</pre> <pre><code>pss_api.pop_params('data_usable')\n\nprint(pss_api.params)\n</code></pre> <pre>\n<code>regulator_symbol: HAP5, workflow: nf_core_callingcards_dev\n</code>\n</pre> <p>or cleared entirely</p> <pre><code>pss_api.pop_params(None)\n\npss_api.params == {}\n</code></pre> <pre>\n<code>True</code>\n</pre> <pre><code>rr_api = RankResponseAPI()\n\nrr_api.push_params({\"promotersetsig_id\": \"8783\",\n                    \"expression_id\": \"4563\"})\n\nresult = await rr_api.read()\n</code></pre> <pre>\n<code>WARNING:yeastdnnexplorer.interface.AbstractAPI:No token provided: URL un-validated\n</code>\n</pre> <pre><code>result.get(\"metadata\")\n</code></pre> id promotersetsig_id n_responsive total_expression_genes filename expression_id 0 8783_4563 8783 1074 6175 8783_4563.csv.gz 4563 <pre><code>result.get(\"data\").get(\"8783_4563\")\n</code></pre> rank_bin n_responsive_in_rank random n_successes response_ratio pvalue ci_lower ci_upper 0 5 2 0.173913 2 0.400000 0.210341 0.052745 0.853367 1 10 1 0.173913 3 0.300000 0.393247 0.066740 0.652453 2 15 1 0.173913 4 0.266667 0.313515 0.077872 0.551003 3 20 0 0.173913 4 0.200000 0.767009 0.057334 0.436614 4 25 1 0.173913 5 0.200000 0.790608 0.068311 0.407037 5 30 2 0.173913 7 0.233333 0.344051 0.099338 0.422837 6 35 0 0.173913 7 0.200000 0.656334 0.084406 0.369379 7 40 1 0.173913 8 0.200000 0.675722 0.090522 0.356478 8 45 0 0.173913 8 0.177778 1.000000 0.080018 0.320534 9 50 0 0.173913 8 0.160000 1.000000 0.071701 0.291126"},{"location":"tutorials/database_interface/#the-database-interface-classes","title":"The Database Interface Classes","text":"<p>For each API endpoint exposed in the Django app, there is a corresponding class that provide methods to execute CRUD operations asynchronously.</p> <p>There are two types of API endpoints \u2013 those that contain only records data, and  those that store both records and pointers to files.</p>"},{"location":"tutorials/database_interface/#connecting-to-the-database","title":"Connecting to the Database","text":"<p>The database currently runs on HTCF service partition. This is a single node with 8 CPU and 30 GB that is meant for long running low resource jobs. The components that need to run are a postgres database, a redis instance and the django app. As long as these components are running on the service partition,  you can connect via an ssh tunnel with:</p> <pre><code>ssh username@login.htcf.wustl.edu -N -L 8001:n240:8000\n</code></pre> <p>where the <code>8001:n240:8000</code> takes the form of <code>local_port:cluster_node:app_port</code>. The django app will always be served on port <code>8000</code>, and <code>n240</code> is the only service partition node. You may choose a different local port.</p> <p>If you do this and cannot connect, let me know and I\u2019ll check the status of the jobs  on the cluster.</p>"},{"location":"tutorials/database_interface/#database-username-and-password","title":"Database username and password","text":"<p>Once you have a tunnel, you can access the database frontend at <code>127.0.0.1:8001</code> (or a different local port, if you changed that number). If you haven\u2019t already signed up, you\u2019ll need to click the \u2018sign up\u2019 button and follow the instructions. The e-mail server is not hooked up at the moment, so when it says \u201csee the e-mail\u201d, send a slack message and let me know. I\u2019ll give you a link to complete the sign up process. After that, you can just use the \u201csign in\u201d button.</p> <p>For computational tasks, including generating rank response data, celery workers must be launched on the HTCF general partition. There is currently a script that is meant to monitor the redis queue and launch/kill these workers automatically, but this functionality is new and largely untested. You can monitor the workers/tasks if you create another tunnel with:</p> <pre><code>ssh username@login.htcf.wustl.edu -N -L 8002:n240:5555\n</code></pre> <p>You\u2019d access this dashboard at <code>127.0.0.1:5555</code></p> <p>The login is currently:</p> <pre><code>username: \"flower\"\npassword: \"daisy\"\n</code></pre> <p>(yes, really \u2013 the security comes from the fact that you need to login with HTCF)</p>"},{"location":"tutorials/database_interface/#configuring-the-database-interface-classes","title":"Configuring the Database Interface Classes","text":"<p>The database classes expect the following environmental variables to be set. </p> <pre><code>BASE_URL='http://127.0.0.1:8001'\nTOKEN='&lt;your token=\"\"&gt;'\nBINDING_URL='http://127.0.0.1:8001/api/binding'\nBINDINGMANUALQC_URL='http://127.0.0.1:8001/api/bindingmanualqc'\nCALLINGCARDSBACKGROUND_URL='http://127.0.0.1:8001/api/callingcardsbackground'\nDATASOURCE_URL='http://127.0.0.1:8001/api/datasource'\nEXPRESSION_URL='http://127.0.0.1:8001/api/expression'\nEXPRESSIONMANUALQC_URL='http://127.0.0.1:8001/api/expressionmanualqc'\nFILEFORMAT_URL='http://127.0.0.1:8001/api/fileformat'\nGENOMICFEATURE_URL='http://127.0.0.1:8001/api/genomicfeature'\nPROMOTERSET_URL='http://127.0.0.1:8001/api/promoterset'\nPROMOTERSETSIG_URL='http://127.0.0.1:8001/api/promotersetsig'\nREGULATOR_URL='http://127.0.0.1:8001/api/regulator'\n</code></pre> <p>This can be achieved in the package during development with a <code>.env</code> file at the top most level of the package. The <code>.env</code> file is loaded in the package <code>__init__.py</code>.</p> <p>If you are importing <code>yeastdnnexplorer</code> into a different environment, then you\u2019ll  need to add the package <code>dotenv</code> and execute <code>load_dotenv(dotenv_path=env_path)</code>. If the <code>.env</code> file is in the same <code>PWD</code> in which you execute that command, there is no need to specify a path.</p>"},{"location":"tutorials/database_interface/#token-authentication","title":"Token Authentication","text":"<p>Once you have a username and password to the database, you can retrieve your token.  Make sure that you put this token, at least, in a <code>.env</code> file, and make sure that  <code>.env</code> file is in your <code>.gitignore</code>.</p> <p>Alternatively, you could retrieve and store in memory the token at the beginning of  each session \u2013 this is more secure if you are not using a <code>.env</code> file.  </p> <p>The <code>.env</code> file is already in the <code>yeastddnexplorer</code> <code>.gitignore</code></p> <pre><code>curl -X 'POST' \\\n  'http://127.0.0.1:8001/auth-token/' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"username\": \"username\",\n  \"password\": \"password\"\n}'\n</code></pre> <p>Or with python:</p> <pre><code>import requests\n\nurl = \"http://127.0.0.1:8001/auth-token/\"\nheaders = {\n    \"accept\": \"application/json\",\n    \"Content-Type\": \"application/json\",\n}\ndata = {\n    \"username\": \"username\",\n    \"password\": \"password\",\n}\n\nresponse = requests.post(url, json=data, headers=headers)\nprint(response.text)\n</code></pre> <p></p>"},{"location":"tutorials/database_interface/#using-the-interface-classes","title":"Using the Interface Classes","text":""},{"location":"tutorials/database_interface/#records-only-endpoints","title":"Records Only Endpoints","text":"<p>The records only endpoints are:</p> <ul> <li> <p>BindingManualQC</p> </li> <li> <p>DataSource</p> </li> <li> <p>ExpressionManualQC</p> </li> <li> <p>FileFormat</p> </li> <li> <p>GenomicFeature</p> </li> <li> <p>PromoterSetSig</p> </li> <li> <p>Regulator</p> </li> </ul> <p>When the <code>read()</code> method is called on the corresponding API classes, a dataframe will be returned in the response.</p> <p>All of the <code>read()</code> methods, for both types of API endpoints, return the result of a callable. By default, the callable returns a dictionary with two keys: <code>metadata</code> and <code>data</code>. For response only tables, the <code>metadata</code> value will be the records from the database as a pandas dataframe and the <code>data</code> will be `None.</p>"},{"location":"tutorials/database_interface/#example-regulatorapi","title":"Example \u2013 RegulatorAPI","text":""},{"location":"tutorials/database_interface/#record-and-file-endpoints","title":"Record and File Endpoints","text":"<p>The record and file endpoints are the following:</p> <ul> <li> <p>CallingCardsBackground</p> </li> <li> <p>Expression</p> </li> <li> <p>PromoterSet</p> </li> <li> <p>PromoterSetSig</p> </li> <li> <p>RankResponse *</p> </li> </ul> <p>The default <code>read()</code> method is the same as the Records only Endpoint API classes. However, there is an additional argument, <code>retrieve_files</code> which if set to <code>True</code> will retrieve the file for which each record provides metadata. The return value of <code>read()</code> is again a callable, and by default the <code>data</code> key will store a dictionary where the keys correspond to the <code>id</code> column in the <code>metadata</code>.</p>"},{"location":"tutorials/database_interface/#filtering","title":"Filtering","text":"<p>All API classes have a <code>params</code> attribute which stores the filtering parameters which will be applied to the HTTP requests.</p>"},{"location":"tutorials/database_interface/#retrieving-files-from-a-records-and-files-object","title":"Retrieving files from a Records and Files Object","text":"<p>To retrieve files from a Records and Files endpoint object, do the following:</p>"},{"location":"tutorials/database_interface/#rank-response","title":"Rank Response","text":"<p>The rank response endpoint is slightly different than the others.</p> <ol> <li> <p>It is an additional action of the promotersetsig table, and so does not have    a \u2018records_only\u2019 option. Data files will always be retrieved/returned.</p> </li> <li> <p>It currently requires that the parameters <code>promotersetsig_id</code> and <code>expression_id</code>    be set.</p> </li> <li> <p>It requires that celery workers are available. On the cluster, this means     submitting additional jobs. At the time of writing, this hasn\u2019t yet been automated.</p> </li> <li> <p>The <code>id</code> is the concatenation of the <code>promotersetsig_id</code>_<code>expression_id</code></p> </li> </ol>"},{"location":"tutorials/database_interface/#caveats","title":"Caveats","text":"<ol> <li> <p>I have written the scripts to automatically check the redis queue for work and to     both launch celery worker nodes, and kill them when they are finished. But, though    they work if I run them manually, they have not worked when scheduled through a    cronjob. I\u2019ll work with Brian and Eric next week to figure out why.</p> </li> <li> <p>I haven\u2019t tested each of the endpoint APIs individually. Help is welcome.</p> </li> </ol>"},{"location":"tutorials/generate_in_silico_data/","title":"Generate In-silico Data","text":"<pre><code>from yeastdnnexplorer.probability_models.relation_classes import And, Or\nfrom yeastdnnexplorer.probability_models.generate_data import (generate_gene_population, \n                                                               generate_binding_effects,\n                                                               generate_pvalues,\n                                                               generate_perturbation_effects,\n                                                               perturbation_effect_adjustment_function_with_tf_relationships,\n                                                               perturbation_effect_adjustment_function_with_tf_relationships_boolean_logic)\n\nimport torch\nimport matplotlib.pyplot as plt\n\ntorch.manual_seed(42)  # For CPU\ntorch.cuda.manual_seed_all(42)  # For all CUDA devices\n\n</code></pre> <pre>\n<code>Matplotlib is building the font cache; this may take a moment.\n</code>\n</pre> <pre><code>n_genes = 1000\nbound = [0.1, 0.15, 0.2, 0.25, 0.3]\nn_sample = [1, 1, 2, 2, 4]\n\n# this will be a list of length 10 with a GenePopulation object in each element\ngene_populations_list = []\nfor bound_proportion, n_draws in zip(bound, n_sample):\n    for _ in range(n_draws):\n        gene_populations_list.append(generate_gene_population(n_genes, bound_proportion))\n\n</code></pre> <pre><code># Generate binding data for each gene population\nbinding_effect_list = [generate_binding_effects(gene_population)\n                     for gene_population in gene_populations_list]\n\n\n# Calculate p-values for binding data\nbinding_pvalue_list = [generate_pvalues(binding_data) for binding_data in binding_effect_list]\n\nbinding_data_combined = [torch.stack((gene_population.labels, binding_effect, binding_pval), dim=1)\n                         for gene_population, binding_effect, binding_pval\n                         in zip (gene_populations_list, binding_effect_list, binding_pvalue_list)]\n\n# Stack along a new dimension (dim=1) to create a tensor of shape [num_genes, num_TFs, 3]\nbinding_data_tensor = torch.stack(binding_data_combined, dim=1)\n\n# Verify the shape\nprint(\"Shape of the binding data tensor:\", binding_data_tensor.shape)\n\n</code></pre> <pre>\n<code>Shape of the binding data tensor: torch.Size([1000, 10, 3])\n</code>\n</pre> <pre><code># See `generate_perturbation_effects()` in the help or the documentation for more details.\nperturbation_effects_list_no_mean_adjustment = [generate_perturbation_effects(binding_data_tensor[:, tf_index, :].unsqueeze(1), tf_index=0) \n                                                        for tf_index in range(sum(n_sample))]\nperturbation_pvalue_list_no_mean_adjustment = [generate_pvalues(perturbation_effects) for perturbation_effects in perturbation_effects_list_no_mean_adjustment]\n</code></pre> <pre><code># if you want to modify the default mean for bound genes, you can pass in the 'bound_mean' parameter\nperturbation_effects_list_normal_mean_adjustment = generate_perturbation_effects(\n    binding_data_tensor, \n    max_mean_adjustment=10.0\n)\n\n# since the p-value generation function operates on one column at a time, we must iterate over the columns of our perturb effects\n# list and generate p-values for each column\nperturbation_effects_list_normal_mean_adjustment_pvalues = torch.zeros_like(perturbation_effects_list_normal_mean_adjustment)\nfor col_idx in range(perturbation_effects_list_normal_mean_adjustment.shape[1]):\n    col = perturbation_effects_list_normal_mean_adjustment[:, col_idx]\n    col_pvals = generate_pvalues(col)\n    perturbation_effects_list_normal_mean_adjustment_pvalues[:, col_idx] = col_pvals\n</code></pre> <pre><code># define our dictionary of TF relationships\n# For each gene, if TF 0 is bound, then we only adjust its mean if TF 1 is also bound\n# similarly, if TF 7 is bound, we still only adjust its mean if TFs 1 and 4 are bound\ntf_relationships = {\n    0: [1],\n    1: [8],\n    2: [5, 6],\n    3: [4],\n    4: [5],\n    5: [9],\n    6: [4],\n    7: [1, 4],\n    8: [6],\n    9: [4],\n}\n\nperturbation_effects_list_dep_mean_adjustment = generate_perturbation_effects(\n    binding_data_tensor, \n    tf_relationships=tf_relationships,\n    adjustment_function=perturbation_effect_adjustment_function_with_tf_relationships,\n    max_mean_adjustment=10.0,\n)\nperturbation_effects_list_dep_mean_adjustment_pvalues = torch.zeros_like(perturbation_effects_list_dep_mean_adjustment)\nfor col_idx in range(perturbation_effects_list_dep_mean_adjustment.shape[1]):\n    col = perturbation_effects_list_dep_mean_adjustment[:, col_idx]\n    col_pvals = generate_pvalues(col)\n    perturbation_effects_list_dep_mean_adjustment_pvalues[:, col_idx] = col_pvals\n</code></pre> <pre><code># note that Or(1,1) is used to enforce a unary contraint\ntf_relationships_dict_boolean_logic = {\n    0: [And(3, 4, 8), Or(3, 7), Or(1, 1)],\n    1: [And(5, Or(7, 8))],\n    2: [],\n    3: [Or(7, 9), And(6, 7)],\n    4: [And(1, 2)],\n    5: [Or(0, 1, 2, 8, 9)],\n    6: [And(0, Or(1, 2))],\n    7: [Or(2, And(5, 6, 9))],\n    8: [],\n    9: [And(6, And(3, Or(0, 9)))],\n}\n\nperturbation_effects_list_boolean_logic = generate_perturbation_effects(\n    binding_data_tensor, \n    adjustment_function=perturbation_effect_adjustment_function_with_tf_relationships_boolean_logic,\n    tf_relationships=tf_relationships_dict_boolean_logic,\n    max_mean_adjustment=10.0,\n)\nperturbation_effects_list_boolean_logic_pvalues = torch.zeros_like(perturbation_effects_list_boolean_logic)\nfor col_idx in range(perturbation_effects_list_boolean_logic.shape[1]):\n    col = perturbation_effects_list_boolean_logic[:, col_idx]\n    col_pvals = generate_pvalues(col)\n    perturbation_effects_list_boolean_logic_pvalues[:, col_idx] = col_pvals\n</code></pre> <pre><code># Convert lists to tensors if they are not already\nperturbation_effects_tensor = torch.stack(perturbation_effects_list_no_mean_adjustment, dim=1)\nperturbation_pvalues_tensor = torch.stack(perturbation_pvalue_list_no_mean_adjustment, dim=1)\n\n# Ensure perturbation data is reshaped to match [n_genes, n_tfs]\n# This step might need adjustment based on the actual shapes of your tensors.\nperturbation_effects_tensor = perturbation_effects_tensor.unsqueeze(-1)  # Adds an extra dimension for concatenation\nperturbation_pvalues_tensor = perturbation_pvalues_tensor.unsqueeze(-1)  # Adds an extra dimension for concatenation\n\n# Concatenate along the last dimension to form a [n_genes, n_tfs, 5] tensor\nfinal_data_tensor = torch.cat((binding_data_tensor, perturbation_effects_tensor, perturbation_pvalues_tensor), dim=2)\n\n# Verify the shape\nprint(\"Shape of the final data tensor:\", final_data_tensor.shape)\n</code></pre> <pre>\n<code>Shape of the final data tensor: torch.Size([1000, 10, 5])\n</code>\n</pre> <p>As an aside, I choose to structure the data this way by looking at the result of strides, which describes how the data is stored in memory:</p> <pre><code>tensor_continuous = torch.empty(100, 1000, 3)\nstrides_continuous = tensor_continuous.stride()\nprint(strides_continuous)\n\n\ntensor_continuous = torch.empty(1000, 100, 3)\nstrides_continuous = tensor_continuous.stride()\nprint(strides_continuous)\n</code></pre> <pre>\n<code>(3000, 3, 1)\n(300, 3, 1)\n</code>\n</pre> <pre><code>tolerance = 1e-5\nare_equal = torch.isclose(\n    torch.sum(final_data_tensor[:, :, 0] == 1, axis=0),\n    torch.tensor([val * n_genes for val, count in zip(bound, n_sample) for _ in range(count)],\n                 dtype=torch.long),\n    atol=tolerance)\n\nprint(f\"bound/nosie ratio is correct: {are_equal.all()}\")\n</code></pre> <pre>\n<code>bound/nosie ratio is correct: True\n</code>\n</pre> <pre><code>labels = final_data_tensor[:, :, 0].flatten()\nunbound_binding = final_data_tensor[:, :, 1].flatten()[labels == 0]\nbound_binding = final_data_tensor[:, :, 1].flatten()[labels == 1]\n\nprint(f\"The unbound binding max is {unbound_binding.max()} and the min is {unbound_binding.min()}\")\nprint(f\"the unbound min is {unbound_binding.min()}\")\nprint(f\"the unbound mean is {unbound_binding.mean()} and the std is {unbound_binding.std()}\")\nprint(f\"The bound binding max is {bound_binding.max()} and the min is {bound_binding.min()}\")\nprint(f\"the bound min is {bound_binding.min()}\")\nprint(f\"the bound mean is {bound_binding.mean()} and the std is {bound_binding.std()}\")\n</code></pre> <pre>\n<code>The unbound binding max is 13.157892227172852 and the min is 0.0\nthe unbound min is 0.0\nthe unbound mean is 0.3589712679386139 and the std is 1.1559306383132935\nThe bound binding max is 78.94734954833984 and the min is 0.1315789520740509\nthe bound min is 0.1315789520740509\nthe bound mean is 2.4840002059936523 and the std is 6.374814510345459\n</code>\n</pre> <pre><code>\n# Plotting\nplt.figure(figsize=(10, 6))\nplt.hist(unbound_binding, bins=30, alpha=0.5, label='Label 0', color='orange')\nplt.hist(bound_binding, bins=30, alpha=0.5, label='Label 1', color='blue')\nplt.xlim(0,5)\nplt.title('Histogram of Values in the 2nd Column')\nplt.xlabel('Values')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()\n</code></pre> <pre><code>unbound_perturbation = final_data_tensor[:, :, 3].flatten()[labels == 0]\nbound_perturbation = final_data_tensor[:, :, 3].flatten()[labels == 1]\n\nprint(f\"The unbound binding max is {unbound_perturbation.max()} and the min is {unbound_perturbation.min()}\")\nprint(f\"the unbound min is {unbound_perturbation.min()}\")\nprint(f\"the unbound mean is {unbound_perturbation.mean()} and the std is {unbound_perturbation.std()}\")\nprint(f\"The bound binding max is {bound_perturbation.max()} and the min is {bound_perturbation.min()}\")\nprint(f\"the bound min is {bound_perturbation.min()}\")\nprint(f\"the bound mean is {bound_perturbation.mean()} and the std is {bound_perturbation.std()}\")\n</code></pre> <pre>\n<code>The unbound binding max is 3.423511505126953 and the min is -3.506139039993286\nthe unbound min is -3.506139039993286\nthe unbound mean is 0.010617653839290142 and the std is 0.988001823425293\nThe bound binding max is 6.107701301574707 and the min is -6.406703948974609\nthe bound min is -6.406703948974609\nthe bound mean is -0.011303802020847797 and the std is 3.136451482772827\n</code>\n</pre> <pre><code># Plotting\nplt.figure(figsize=(10, 6))\nplt.hist(unbound_perturbation, bins=30, alpha=0.5, label='Label 0', color='orange')\nplt.hist(bound_perturbation, bins=30, alpha=0.5, label='Label 1', color='blue')\nplt.title('Histogram of Values in the 2nd Column')\nplt.xlabel('Values')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()\n</code></pre> <pre><code># plot the binding effects vs the perturbation effects\n# color the points by the label\n# make sure the labels are categorical\n# label 0 should be blue while label 1 should be orange\n\n# Plotting\nplt.figure(figsize=(10, 6))\nplt.scatter(final_data_tensor[:, :, 1].flatten(), final_data_tensor[:, :, 3].flatten().abs(), c=['orange' if x == 0 else 'blue' for x in labels])\nplt.title('Binding Effects vs Perturbation Effects')\nplt.xlabel('Binding Effects')\nplt.ylabel('Perturbation Effects')\nplt.xlim(0,100)\nplt.show()\n</code></pre> <pre><code># in this case, select the TF binding data that corresponds with the effect data\n# which we wish to produce. use the .unsqueeze(1) method to add the TF dimension\n# after selecting the TF\nperturbation_effects_tf_influenced = generate_perturbation_effects(\n    binding_data_tensor, \n    max_mean_adjustment=3.0, # try 0.1, 3.0, and 10.0\n    bound_mean=5.0, # try 3.0, 5.0, or 10.0\n    unbound_mean=0.0, # try adjusting this\n)\nperturbation_pvalue_tf_influenced = torch.zeros_like(perturbation_effects_tf_influenced)\nfor col_idx in range(perturbation_effects_tf_influenced.shape[1]):\n    col = perturbation_effects_tf_influenced[:, col_idx]\n    col_pvals = generate_pvalues(col)\n    perturbation_pvalue_tf_influenced[:, col_idx] = col_pvals\n\nperturbation_effects_tensor_tf_influened = perturbation_effects_tf_influenced.unsqueeze(-1)\nperturbation_pvalues_tensor_tf_influenced = perturbation_pvalue_tf_influenced.unsqueeze(-1)\n\nfinal_data_tensor_tf_influenced = torch.cat(\n    (binding_data_tensor,\n     perturbation_effects_tensor_tf_influened,\n     perturbation_pvalues_tensor_tf_influenced), \n    dim=2)\n\n# Verify the shape\nprint(\"Shape of the final data tensor:\", final_data_tensor.shape)\n</code></pre> <pre>\n<code>Shape of the final data tensor: torch.Size([1000, 10, 5])\n</code>\n</pre> <pre><code># Plotting. Note that the 'unbound' group effects are still range from 0 to 3\n\nplt.figure(figsize=(10, 6))\nplt.scatter(final_data_tensor_tf_influenced[:, :, 1].flatten(), final_data_tensor_tf_influenced[:, :, 3].flatten().abs(), c=['orange' if x == 0 else 'blue' for x in labels])\nplt.title('Binding Effects vs Perturbation Effects')\nplt.xlabel('Binding Effects')\nplt.ylabel('Perturbation Effects')\n\nlegend_labels = ['Bound', 'Unbound']\ncolors = ['blue', 'orange']\nlegend_handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10) for color in colors]\nplt.legend(legend_handles, legend_labels)\n\nplt.show()\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"tutorials/generate_in_silico_data/#generating-in-silico-data","title":"Generating in silico data","text":""},{"location":"tutorials/generate_in_silico_data/#step-1","title":"Step 1:","text":"<p>The first step is to generate a gene population, or set of gene populations. A gene population is simply a class that stores a 1D tensor called <code>labels</code>. <code>labels</code> is a boolean vector where 1 means the gene is part of the bound group (a gene which is both bound and responsive to the TF) while 0 means the gene is part of the background or unbound group. The length of <code>labels</code> is the number of genes in the population, and the index should be considered the unique gene identifier. In other words, the indicies should never change.</p>"},{"location":"tutorials/generate_in_silico_data/#step-2","title":"Step 2:","text":"<p>The second step is to generate binding data from the gene population(s).</p>"},{"location":"tutorials/generate_in_silico_data/#step-3-generate-perturbation-data","title":"Step 3: Generate perturbation data.","text":"<p>It is important to understand that there are four possible ways we provide for you to generate perturbation data. 1. No Mean Adjustment 2. Standard Mean Adjustment 3. Mean adjustment dependent on all TFs bound to gene in question 4. Mean adjustment dependent on binary relationships between bound and unbound TFs to gene in question.</p>"},{"location":"tutorials/generate_in_silico_data/#method-1-generating-perturbation-data-with-no-mean-adjustment","title":"Method 1: Generating perturbation data with no mean adjustment","text":"<p>If you don\u2019t pass in a value for <code>max_mean_adjustment</code> to <code>generate_perturbation_effects</code> it will default to zero, meaning the means of the perturbation effects will not be adjusted in any way and will all be equal to <code>bound_mean</code> (deault is 3.0) for bound TF-gene pairs and <code>unbound_mean</code> (default is 0.0) for unbound TF-gene pairs.</p>"},{"location":"tutorials/generate_in_silico_data/#method-2-generating-perturbation-data-with-a-simple-mean-adjustment","title":"Method 2: Generating perturbation data with a simple mean adjustment","text":"<p>If you do pass in a nonzero value for <code>max_mean_adjustment</code>, the means of bound gene-TF pairs will be adjusted by up to a maximum of <code>max_mean_adjustment</code>. Note that instead of passing in one column (corresponding to one TF) of the binding data tensor at a time, we instead pass in the entire binding data tensor at once. This syntactic difference is just a result of how our mean adjustment functions requires the entire matrix of all genes and TFs as opposed to being able to operate on one column at once. Using this data generation method, we adust the mean of any TF that is bound to a gene.</p>"},{"location":"tutorials/generate_in_silico_data/#method-3-generating-perturbation-data-with-a-mean-adjustment-dependent-on-which-tfs-are-bound-to-gene","title":"Method 3: Generating Perturbation Data with a mean adjustment dependent on which TFs are bound to gene","text":"<p>You are also able to specify a dictionary of TF relationships. Passing in this dictionary in combination with using our <code>perturbation_effect_adjustment_function_with_tf_relationships</code> mean adjustment function alows for you to only adjust the means of perturbation effects if the TF in the TF-gene pair in question is bound AND all other TFs associated with that TF are bound to the same gene. To associate a TF with another TF, put its index in the list of TFs corresponding to the other TF\u2019s index in the tf_relationships dictionary.</p>"},{"location":"tutorials/generate_in_silico_data/#method-4-generating-perturbation-data-with-a-mean-adjustment-dependent-on-boolean-relationships-between-tfs","title":"Method 4: Generating Perturbation Data with a mean adjustment dependent on boolean relationships between TFs","text":"<p>(see the documentation in <code>yeastdnnexplorer/probability_models/relation_classes.py</code> for more information on <code>And()</code> and <code>Or()</code>) </p> <p>This is a more advanced version of method 3 where instead of only specifying direct dependencies you can specify logical relations that must be satisfied for a gene-TF pair\u2019s perturbation effect value to be adjusted. For example, in the below example we only adjust the mean of TF 3 for each gene if TF 3 is bound and (7 || 9) &amp;&amp; (6 &amp;&amp; 7) are bound. </p>"},{"location":"tutorials/generate_in_silico_data/#step-4-assemble","title":"Step 4: Assemble","text":"<p>The final step is to assemble the data into a single tensor. Here is one way. The order of the matrix in the last dimension is:</p> <ol> <li>bound/unbound label</li> <li>binding effect</li> <li>binding pvalue</li> <li>perturbation effect</li> <li>perturbation pvalue</li> </ol> <p>For simplicity\u2019s sake, we will use the perturbation effect data we generated with no mean adjustment. However you can assemble the data using perturbation effect data generated from any of the 4 methods we covered above.</p>"},{"location":"tutorials/generate_in_silico_data/#sanity-checks","title":"Sanity checks","text":"<p>Ensure that the generated data matches expectations.</p>"},{"location":"tutorials/generate_in_silico_data/#the-boundunbound-ratios-should-match-exactly-the-initial-bound-ratio","title":"The bound/unbound ratios should match exactly the initial bound ratio","text":""},{"location":"tutorials/generate_in_silico_data/#binding-effect-distributions-should-match-expectations","title":"Binding effect distributions should match expectations","text":""},{"location":"tutorials/generate_in_silico_data/#perturbation-effect-distribtuions-should-match-expectations","title":"Perturbation effect distribtuions should match expectations","text":""},{"location":"tutorials/generate_in_silico_data/#the-binding-effects-should-be-positively-correlated-with-the-perturbaiton-effects","title":"The binding effects should be positively correlated with the perturbaiton effects","text":""},{"location":"tutorials/generate_in_silico_data/#re-generate-data-with-an-explicit-relationship-between-a-give-tfs-binding-and-perturbation-effects","title":"Re-generate data with an explicit relationship between a give TF\u2019s binding and perturbation effects","text":""},{"location":"tutorials/hyperparameter_sweep/","title":"Hyperparameter Sweep","text":"<p>This notebook introduces how to perform a hyperparameter sweep to find the best hyperparameters for our model using the Optuna library. Feel free to modify the objective function if you would like to test other hyperparameters or values.</p> <pre><code># imports \nimport argparse\nfrom argparse import Namespace\n\nfrom pytorch_lightning import Trainer, LightningModule, seed_everything\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\nfrom torchsummary import summary\n\nfrom yeastdnnexplorer.data_loaders.synthetic_data_loader import SyntheticDataLoader\nfrom yeastdnnexplorer.ml_models.simple_model import SimpleModel\nfrom yeastdnnexplorer.ml_models.customizable_model import CustomizableModel\n\nimport optuna\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# set random seed for reproducability\nseed_everything(42)\n</code></pre> <pre>\n<code>Seed set to 42\n</code>\n</pre> <pre>\n<code>42</code>\n</pre> <p>Here we define loggers and checkpoints for our model. Checkpoints tell pytorch when to save instances of the model (that can be loaded and inspected later) and loggers tell pytorch how to format the metrics that the model logs during its training. </p> <pre><code># Checkpoint to save the best version of model (during the entire training process) based on the metric passed into \"monitor\"\nbest_model_checkpoint = ModelCheckpoint(\n    monitor=\"val_mse\",  # You can modify this to save the best model based on any other metric that the model you're testing tracks and reports\n    mode=\"min\",\n    filename=\"best-model-{epoch:02d}-{val_loss:.2f}.ckpt\",\n    save_top_k=1,  # Can modify this to save the top k models\n)\n\n# Callback to save checkpoints every 2 epochs, regardless of performance\nperiodic_checkpoint = ModelCheckpoint(\n    filename=\"periodic-{epoch:02d}.ckpt\",\n    every_n_epochs=2,\n    save_top_k=-1,  # Setting -1 saves all checkpoints\n)\n\n# define loggers for the model\ntb_logger = TensorBoardLogger(\"logs/tensorboard_logs\")\ncsv_logger = CSVLogger(\"logs/csv_logs\")\n</code></pre> <p>Now we perform our hyperparameter sweep using the Optuna library. To do this, we need to define an objective function that returns a scalar value. This scalar value will be the value that our sweep is attempting to minimize. We train one instance of our model inside each call to the objective function (each model on each iteration will use a different selection of hyperparameters). In our objective function, we return the validation mse associated with the instance of the model. This is because we would like to find the combination of hyperparameters that leads to the lowest validation mse. We use validation mse instead of test mse since we do not want to risk fitting to the test data at all while tuning hyperparameters.</p> <p>If you\u2019d like to try different hyperparameters, you just need to modify the list of possible values corresponding to the hyperparameter in question.</p> <p>If you\u2019d like to run the hyperparamter sweep on real data instead of synthetic data, simply swap out the synthetic data loader for the real data loader.</p> <pre><code># on each call to the objective function, it will choose a hyperparameter value from each of the suggest_categorical arrays and pass them into the model\n    # this allows us to test many different hyperparameter configurations during our sweep\n\ndef objective(trial):\n    # model hyperparameters\n    lr = trial.suggest_categorical(\"lr\", [0.01])\n    hidden_layer_num = trial.suggest_categorical(\"hidden_layer_num\", [1, 2, 3, 5])\n    activation = trial.suggest_categorical(\n        \"activation\", [\"ReLU\", \"Sigmoid\", \"Tanh\", \"LeakyReLU\"]\n    )\n    optimizer = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\", \"RMSprop\"])\n    L2_regularization_term = trial.suggest_categorical(\n        \"L2_regularization_term\", [0.0, 0.1]\n    )\n    dropout_rate = trial.suggest_categorical(\n        \"dropout_rate\", [0.0, 0.5]\n    )\n\n    # data module hyperparameters\n    batch_size = trial.suggest_categorical(\"batch_size\", [32])\n\n    # training hyperparameters\n    max_epochs = trial.suggest_categorical(\n        \"max_epochs\", [1]\n    ) # default is 10\n\n    # defining what to pass in for the hidden layer sizes list based on the number of hidden layers\n    hidden_layer_sizes_configurations = {\n        1: [[64], [256]],\n        2: [[64, 32], [256, 64]],\n        3: [[256, 128, 32], [512, 256, 64]],\n        5: [[512, 256, 128, 64, 32]],\n    }\n    hidden_layer_sizes = trial.suggest_categorical(\n        f\"hidden_layer_sizes_{hidden_layer_num}_layers\",\n        hidden_layer_sizes_configurations[hidden_layer_num],\n    )\n\n    print(\"=\" * 70)\n    print(\"About to create model with the following hyperparameters:\")\n    print(f\"lr: {lr}\")\n    print(f\"hidden_layer_num: {hidden_layer_num}\")\n    print(f\"hidden_layer_sizes: {hidden_layer_sizes}\")\n    print(f\"activation: {activation}\")\n    print(f\"optimizer: {optimizer}\")\n    print(f\"L2_regularization_term: {L2_regularization_term}\")\n    print(f\"dropout_rate: {dropout_rate}\")\n    print(f\"batch_size: {batch_size}\")\n    print(f\"max_epochs: {max_epochs}\")\n    print(\"\")\n\n    # create data module\n    data_module = SyntheticDataLoader(\n        batch_size=batch_size,\n        num_genes=4000,\n        bound_mean=3.0,\n        bound=[0.5] * 10,\n        n_sample=[1, 2, 2, 4, 4],\n        val_size=0.1,\n        test_size=0.1,\n        random_state=42,\n        max_mean_adjustment=3.0,\n    )\n\n    num_tfs = sum(data_module.n_sample)  # sum of all n_sample is the number of TFs\n\n    # create model\n    model = CustomizableModel(\n        input_dim=num_tfs,\n        output_dim=num_tfs,\n        lr=lr,\n        hidden_layer_num=hidden_layer_num,\n        hidden_layer_sizes=hidden_layer_sizes,\n        activation=activation,\n        optimizer=optimizer,\n        L2_regularization_term=L2_regularization_term,\n        dropout_rate=dropout_rate,\n    )\n\n    # create trainer\n    trainer = Trainer(\n        max_epochs=max_epochs,\n        deterministic=True,\n        accelerator=\"cpu\",\n        # callbacks and loggers are commented out for now since running a large sweep would generate an unnecessarily huge amount of checkpoints and logs\n        # callbacks=[best_model_checkpoint, periodic_checkpoint],\n        # logger=[tb_logger, csv_logger],\n    )\n\n    # train model\n    trainer.fit(model, data_module)\n\n    # get best validation loss from the model\n    return trainer.callback_metrics[\"val_mse\"]\n</code></pre> <p>Now we define an optuna study, which represents our hyperparameter sweep. It will run the objective function n_trials times and choose the model that gave the best val_mse across all of those trials with different hyperparameters. Note that this will create a very large amount of output as it will show training stats for every model. This is why we print out the best params and loss in a separate cell.</p> <pre><code>STUDY_NAME = \"CustomizableModelHyperparameterSweep3\"\nNUM_TRIALS = 5 # you will need a lot more than 5 trials if you have many possible combinations of hyperparams\n\n# Perform hyperparameter optimization using Optuna\nstudy = optuna.create_study(\n    direction=\"minimize\", # we want to minimize the val_mse\n    study_name=STUDY_NAME,\n    # storage=\"sqlite:///db.sqlite3\", # you can save the study results in a database if you'd like, this is needed if you want to try and use the optuna dashboard library to dispaly results\n)\nstudy.optimize(objective, n_trials=NUM_TRIALS)\n\n# Get the best hyperparameters and their corresponding values\nbest_params = study.best_params\nbest_loss = study.best_value\n</code></pre> <pre>\n<code>[I 2024-05-29 13:18:03,548] A new study created in memory with name: CustomizableModelHyperparameterSweep3\n/Users/ericjia/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [64] which is of type list.\n  warnings.warn(message)\n/Users/ericjia/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [256] which is of type list.\n  warnings.warn(message)\n</code>\n</pre> <pre>\n<code>======================================================================\nAbout to create model with the following hyperparameters:\nlr: 0.01\nhidden_layer_num: 1\nhidden_layer_sizes: [256]\nactivation: Tanh\noptimizer: RMSprop\nL2_regularization_term: 0.1\ndropout_rate: 0.5\nbatch_size: 32\nmax_epochs: 1\n\n</code>\n</pre> <pre>\n<code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n/Users/ericjia/yeastdnnexplorer/yeastdnnexplorer/data_loaders/synthetic_data_loader.py:260: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_train, Y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(\n/Users/ericjia/yeastdnnexplorer/yeastdnnexplorer/data_loaders/synthetic_data_loader.py:263: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_val, Y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(\n/Users/ericjia/yeastdnnexplorer/yeastdnnexplorer/data_loaders/synthetic_data_loader.py:266: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_test, Y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(\n\n  | Name          | Type              | Params\n----------------------------------------------------\n0 | activation    | Tanh              | 0     \n1 | input_layer   | Linear            | 3.6 K \n2 | hidden_layers | ModuleList        | 0     \n3 | output_layer  | Linear            | 3.3 K \n4 | dropout       | Dropout           | 0     \n5 | mae           | MeanAbsoluteError | 0     \n6 | SMSE          | SMSE              | 0     \n----------------------------------------------------\n6.9 K     Trainable params\n0         Non-trainable params\n6.9 K     Total params\n0.028     Total estimated model params size (MB)\n</code>\n</pre> <pre>\n<code>Sanity Checking: |                                                                                            \u2026</code>\n</pre> <pre>\n<code>/Users/ericjia/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n</code>\n</pre> <pre>\n<code>Training: |                                                                                                   \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>`Trainer.fit` stopped: `max_epochs=1` reached.\n[I 2024-05-29 13:18:26,417] Trial 0 finished with value: 4.489274501800537 and parameters: {'lr': 0.01, 'hidden_layer_num': 1, 'activation': 'Tanh', 'optimizer': 'RMSprop', 'L2_regularization_term': 0.1, 'dropout_rate': 0.5, 'batch_size': 32, 'max_epochs': 1, 'hidden_layer_sizes_1_layers': [256]}. Best is trial 0 with value: 4.489274501800537.\n/Users/ericjia/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [64] which is of type list.\n  warnings.warn(message)\n/Users/ericjia/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [256] which is of type list.\n  warnings.warn(message)\nGPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n</code>\n</pre> <pre>\n<code>======================================================================\nAbout to create model with the following hyperparameters:\nlr: 0.01\nhidden_layer_num: 1\nhidden_layer_sizes: [256]\nactivation: LeakyReLU\noptimizer: SGD\nL2_regularization_term: 0.1\ndropout_rate: 0.5\nbatch_size: 32\nmax_epochs: 1\n\n</code>\n</pre> <pre>\n<code>\n  | Name          | Type              | Params\n----------------------------------------------------\n0 | activation    | LeakyReLU         | 0     \n1 | input_layer   | Linear            | 3.6 K \n2 | hidden_layers | ModuleList        | 0     \n3 | output_layer  | Linear            | 3.3 K \n4 | dropout       | Dropout           | 0     \n5 | mae           | MeanAbsoluteError | 0     \n6 | SMSE          | SMSE              | 0     \n----------------------------------------------------\n6.9 K     Trainable params\n0         Non-trainable params\n6.9 K     Total params\n0.028     Total estimated model params size (MB)\n</code>\n</pre> <pre>\n<code>Sanity Checking: |                                                                                            \u2026</code>\n</pre> <pre>\n<code>Training: |                                                                                                   \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>`Trainer.fit` stopped: `max_epochs=1` reached.\n[I 2024-05-29 13:18:45,320] Trial 1 finished with value: 6.033911228179932 and parameters: {'lr': 0.01, 'hidden_layer_num': 1, 'activation': 'LeakyReLU', 'optimizer': 'SGD', 'L2_regularization_term': 0.1, 'dropout_rate': 0.5, 'batch_size': 32, 'max_epochs': 1, 'hidden_layer_sizes_1_layers': [256]}. Best is trial 0 with value: 4.489274501800537.\n/Users/ericjia/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [64, 32] which is of type list.\n  warnings.warn(message)\n/Users/ericjia/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [256, 64] which is of type list.\n  warnings.warn(message)\nGPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n</code>\n</pre> <pre>\n<code>======================================================================\nAbout to create model with the following hyperparameters:\nlr: 0.01\nhidden_layer_num: 2\nhidden_layer_sizes: [256, 64]\nactivation: ReLU\noptimizer: SGD\nL2_regularization_term: 0.0\ndropout_rate: 0.5\nbatch_size: 32\nmax_epochs: 1\n\n</code>\n</pre> <pre>\n<code>\n  | Name          | Type              | Params\n----------------------------------------------------\n0 | activation    | ReLU              | 0     \n1 | input_layer   | Linear            | 3.6 K \n2 | hidden_layers | ModuleList        | 16.4 K\n3 | output_layer  | Linear            | 845   \n4 | dropout       | Dropout           | 0     \n5 | mae           | MeanAbsoluteError | 0     \n6 | SMSE          | SMSE              | 0     \n----------------------------------------------------\n20.9 K    Trainable params\n0         Non-trainable params\n20.9 K    Total params\n0.084     Total estimated model params size (MB)\n</code>\n</pre> <pre>\n<code>Sanity Checking: |                                                                                            \u2026</code>\n</pre> <pre>\n<code>Training: |                                                                                                   \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>`Trainer.fit` stopped: `max_epochs=1` reached.\n[I 2024-05-29 13:19:02,993] Trial 2 finished with value: 6.900921821594238 and parameters: {'lr': 0.01, 'hidden_layer_num': 2, 'activation': 'ReLU', 'optimizer': 'SGD', 'L2_regularization_term': 0.0, 'dropout_rate': 0.5, 'batch_size': 32, 'max_epochs': 1, 'hidden_layer_sizes_2_layers': [256, 64]}. Best is trial 0 with value: 4.489274501800537.\nGPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n</code>\n</pre> <pre>\n<code>======================================================================\nAbout to create model with the following hyperparameters:\nlr: 0.01\nhidden_layer_num: 2\nhidden_layer_sizes: [64, 32]\nactivation: Tanh\noptimizer: Adam\nL2_regularization_term: 0.1\ndropout_rate: 0.0\nbatch_size: 32\nmax_epochs: 1\n\n</code>\n</pre> <pre>\n<code>\n  | Name          | Type              | Params\n----------------------------------------------------\n0 | activation    | Tanh              | 0     \n1 | input_layer   | Linear            | 896   \n2 | hidden_layers | ModuleList        | 2.1 K \n3 | output_layer  | Linear            | 429   \n4 | dropout       | Dropout           | 0     \n5 | mae           | MeanAbsoluteError | 0     \n6 | SMSE          | SMSE              | 0     \n----------------------------------------------------\n3.4 K     Trainable params\n0         Non-trainable params\n3.4 K     Total params\n0.014     Total estimated model params size (MB)\n</code>\n</pre> <pre>\n<code>Sanity Checking: |                                                                                            \u2026</code>\n</pre> <pre>\n<code>Training: |                                                                                                   \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>`Trainer.fit` stopped: `max_epochs=1` reached.\n[I 2024-05-29 13:19:19,976] Trial 3 finished with value: 4.5260910987854 and parameters: {'lr': 0.01, 'hidden_layer_num': 2, 'activation': 'Tanh', 'optimizer': 'Adam', 'L2_regularization_term': 0.1, 'dropout_rate': 0.0, 'batch_size': 32, 'max_epochs': 1, 'hidden_layer_sizes_2_layers': [64, 32]}. Best is trial 0 with value: 4.489274501800537.\n/Users/ericjia/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [512, 256, 128, 64, 32] which is of type list.\n  warnings.warn(message)\nGPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n</code>\n</pre> <pre>\n<code>======================================================================\nAbout to create model with the following hyperparameters:\nlr: 0.01\nhidden_layer_num: 5\nhidden_layer_sizes: [512, 256, 128, 64, 32]\nactivation: Tanh\noptimizer: RMSprop\nL2_regularization_term: 0.1\ndropout_rate: 0.5\nbatch_size: 32\nmax_epochs: 1\n\n</code>\n</pre> <pre>\n<code>\n  | Name          | Type              | Params\n----------------------------------------------------\n0 | activation    | Tanh              | 0     \n1 | input_layer   | Linear            | 7.2 K \n2 | hidden_layers | ModuleList        | 174 K \n3 | output_layer  | Linear            | 429   \n4 | dropout       | Dropout           | 0     \n5 | mae           | MeanAbsoluteError | 0     \n6 | SMSE          | SMSE              | 0     \n----------------------------------------------------\n182 K     Trainable params\n0         Non-trainable params\n182 K     Total params\n0.729     Total estimated model params size (MB)\n</code>\n</pre> <pre>\n<code>Sanity Checking: |                                                                                            \u2026</code>\n</pre> <pre>\n<code>Training: |                                                                                                   \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>`Trainer.fit` stopped: `max_epochs=1` reached.\n[I 2024-05-29 13:19:37,861] Trial 4 finished with value: 4.612905502319336 and parameters: {'lr': 0.01, 'hidden_layer_num': 5, 'activation': 'Tanh', 'optimizer': 'RMSprop', 'L2_regularization_term': 0.1, 'dropout_rate': 0.5, 'batch_size': 32, 'max_epochs': 1, 'hidden_layer_sizes_5_layers': [512, 256, 128, 64, 32]}. Best is trial 0 with value: 4.489274501800537.\n</code>\n</pre> <p>Print out the best hyperparameters and the val_mse assocaited with the model with the best hyperparameters.</p> <pre><code>print(\"RESULTS\" + (\"=\" * 70))\nprint(f\"Best hyperparameters: {best_params}\")\nprint(f\"Best loss: {best_loss}\")\n</code></pre> <pre>\n<code>RESULTS======================================================================\nBest hyperparameters: {'lr': 0.01, 'hidden_layer_num': 1, 'activation': 'Tanh', 'optimizer': 'RMSprop', 'L2_regularization_term': 0.1, 'dropout_rate': 0.5, 'batch_size': 32, 'max_epochs': 1, 'hidden_layer_sizes_1_layers': [256]}\nBest loss: 4.489274501800537\n</code>\n</pre> <p>And that\u2019s it! Now you could take what you found to be the best hyperparameters and train a model with them for many more epochs. The Optuna Documentation will be a helpful resource if you\u2019d like to add more to this notebook or the hyperparam sweep functions</p> <pre><code>\n</code></pre>"},{"location":"tutorials/lightning_crash_course/","title":"Lightning Crash Course","text":"<pre><code># imports\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\n\nfrom yeastdnnexplorer.data_loaders.synthetic_data_loader import SyntheticDataLoader\nfrom yeastdnnexplorer.data_loaders.real_data_loader import RealDataLoader\nfrom yeastdnnexplorer.ml_models.simple_model import SimpleModel\nfrom yeastdnnexplorer.ml_models.customizable_model import CustomizableModel\n</code></pre> <p>In Pytorch Lightning, the data is kept completely separate from the models. This allows for you to easy train a model using different datasets or train different models on the same dataset. <code>DataModules</code> encapsulate all the logic of loading in a specific dataset and splitting into training, testing, and validation sets. In this project, we have two data loaders defined: <code>SyntheticDataLoader</code> for the in silico data (which takes in many parameters that allow you to specify how the data is generated) and <code>RealDataLoader</code> which contains all of the logic for loading in the real experiment data and putting it into a form that the models expect.</p> <p>Once you decide what model you want to train and what dataModule you want to use, you can bundle these with a <code>Trainer</code> object to train the model on the dataset.</p> <p>If you\u2019d like to learn more about the models and dataModules we\u2019ve defined, there is extensive documentation in each of the files that explains each method\u2019s purpose.</p> <pre><code># define an instance of our simple linear baseline model\nmodel = SimpleModel(\n    input_dim=10,\n    output_dim=10,\n    lr=1e-2,\n)\n\n# define an instance of the synthetic data loader\n# see the constructor for the full list of params and their explanations\ndata_module = SyntheticDataLoader(\n    batch_size=32,\n    num_genes=3000,\n    bound=[0.5] * 5,\n    n_sample=[1, 1, 2, 2, 4],\n    val_size=0.1,\n    test_size=0.1,\n    bound_mean=3.0,\n)\n\n# define a trainer instance\ntrainer = Trainer(\n    max_epochs=10,\n    deterministic=True,\n    accelerator=\"cpu\", # change to \"gpu\" if you have access to one\n)\n\n# train the model\ntrainer.fit(model, data_module)\n\n# test the model (recall that data_module specifies the train / test split, we don't need to do it explicitly here)\ntest_results = trainer.test(model, data_module)\nprint(test_results)\n\n</code></pre> <pre>\n<code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nMissing logger folder: /Users/ericjia/yeastdnnexplorer/docs/tutorials/lightning_logs\n/Users/ericjia/yeastdnnexplorer/yeastdnnexplorer/data_loaders/synthetic_data_loader.py:260: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_train, Y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(\n/Users/ericjia/yeastdnnexplorer/yeastdnnexplorer/data_loaders/synthetic_data_loader.py:263: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_val, Y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(\n/Users/ericjia/yeastdnnexplorer/yeastdnnexplorer/data_loaders/synthetic_data_loader.py:266: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_test, Y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(\n\n  | Name    | Type              | Params\n----------------------------------------------\n0 | mae     | MeanAbsoluteError | 0     \n1 | SMSE    | SMSE              | 0     \n2 | linear1 | Linear            | 110   \n----------------------------------------------\n110       Trainable params\n0         Non-trainable params\n110       Total params\n0.000     Total estimated model params size (MB)\n</code>\n</pre> <pre>\n<code>Sanity Checking: |                                                                                            \u2026</code>\n</pre> <pre>\n<code>/Users/ericjia/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n</code>\n</pre> <pre>\n<code>Training: |                                                                                                   \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>`Trainer.fit` stopped: `max_epochs=10` reached.\n</code>\n</pre> <pre>\n<code>Testing: |                                                                                                    \u2026</code>\n</pre> <pre>\n<code>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        test_mae            1.1637259721755981\n        test_mse            1.8661913871765137\n        test_smse           10.101052284240723\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n[{'test_mse': 1.8661913871765137, 'test_mae': 1.1637259721755981, 'test_smse': 10.101052284240723}]\n</code>\n</pre> <p>It\u2019s very easy to train the same model on a different dataset, for example if we want to use real world data we can just swap to the data module that we\u2019ve defined for the real world data.</p> <pre><code># we need to redefine a new instance with the same params unless we want it to pick up where it left off\nnew_model = SimpleModel(\n    input_dim=30,  # note that the input and output dims are equal to the num TFs in the dataset\n    output_dim=30,\n    lr=1e-2,\n)\n\nreal_data_module = RealDataLoader(\n    batch_size=32,\n    val_size=0.1,\n    test_size=0.1,\n    data_dir_path=\"../../data/init_analysis_data_20240409/\", # note that this is relative to where real_data_loader.py is\n    perturbation_dataset_title=\"hu_reimann_tfko\",\n)\n\n# we also have to define a new trainer instance, not really sure why but it seems to be necessary\ntrainer = Trainer(\n    max_epochs=10,\n    deterministic=True,\n    accelerator=\"cpu\", # change to \"gpu\" if you have access to one\n)\n\ntrainer.fit(new_model, real_data_module)\ntest_results = trainer.test(new_model, real_data_module)\nprint(test_results)\n</code></pre> <pre>\n<code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n</code>\n</pre> <pre>\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In[3], line 23\n     16 # we also have to define a new trainer instance, not really sure why but it seems to be necessary\n     17 trainer = Trainer(\n     18     max_epochs=10,\n     19     deterministic=True,\n     20     accelerator=\"cpu\", # change to \"gpu\" if you have access to one\n     21 )\n---&gt; 23 trainer.fit(new_model, real_data_module)\n     24 test_results = trainer.test(new_model, real_data_module)\n     25 print(test_results)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:544, in Trainer.fit(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\n    542 self.state.status = TrainerStatus.RUNNING\n    543 self.training = True\n--&gt; 544 call._call_and_handle_interrupt(\n    545     self, self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n    546 )\n\nFile ~/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:44, in _call_and_handle_interrupt(trainer, trainer_fn, *args, **kwargs)\n     42     if trainer.strategy.launcher is not None:\n     43         return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n---&gt; 44     return trainer_fn(*args, **kwargs)\n     46 except _TunerExitException:\n     47     _call_teardown_hook(trainer)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:580, in Trainer._fit_impl(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\n    573 assert self.state.fn is not None\n    574 ckpt_path = self._checkpoint_connector._select_ckpt_path(\n    575     self.state.fn,\n    576     ckpt_path,\n    577     model_provided=True,\n    578     model_connected=self.lightning_module is not None,\n    579 )\n--&gt; 580 self._run(model, ckpt_path=ckpt_path)\n    582 assert self.state.stopped\n    583 self.training = False\n\nFile ~/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:947, in Trainer._run(self, model, ckpt_path)\n    944 self.__setup_profiler()\n    946 log.debug(f\"{self.__class__.__name__}: preparing data\")\n--&gt; 947 self._data_connector.prepare_data()\n    949 call._call_setup_hook(self)  # allow user to set up LightningModule in accelerator environment\n    950 log.debug(f\"{self.__class__.__name__}: configuring model\")\n\nFile ~/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:94, in _DataConnector.prepare_data(self)\n     92     dm_prepare_data_per_node = datamodule.prepare_data_per_node\n     93     if (dm_prepare_data_per_node and local_rank_zero) or (not dm_prepare_data_per_node and global_rank_zero):\n---&gt; 94         call._call_lightning_datamodule_hook(trainer, \"prepare_data\")\n     95 # handle lightning module prepare data:\n     96 # check for prepare_data_per_node before calling lightning_module.prepare_data\n     97 if lightning_module is not None:\n\nFile ~/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:179, in _call_lightning_datamodule_hook(trainer, hook_name, *args, **kwargs)\n    177 if callable(fn):\n    178     with trainer.profiler.profile(f\"[LightningDataModule]{trainer.datamodule.__class__.__name__}.{hook_name}\"):\n--&gt; 179         return fn(*args, **kwargs)\n    180 return None\n\nFile ~/yeastdnnexplorer/yeastdnnexplorer/data_loaders/real_data_loader.py:118, in RealDataLoader.prepare_data(self)\n    106 \"\"\"\n    107 This function reads in the binding data and perturbation data from the CSV files\n    108 that we have for these datasets.\n   (...)\n    113 \n    114 \"\"\"\n    116 brent_cc_path = os.path.join(self.data_dir_path, \"binding/brent_nf_cc\")\n    117 brent_nf_csv_files = [\n--&gt; 118     f for f in os.listdir(brent_cc_path) if f.endswith(\".csv\")\n    119 ]\n    120 perturb_dataset_path = os.path.join(\n    121     self.data_dir_path, f\"perturbation/{self.perturbation_dataset_title}\"\n    122 )\n    123 perturb_dataset_csv_files = [\n    124     f for f in os.listdir(perturb_dataset_path) if f.endswith(\".csv\")\n    125 ]\n\nFileNotFoundError: [Errno 2] No such file or directory: '../../data/init_analysis_data_20240409/binding/brent_nf_cc'</pre> <p>If we wanted to do the same thing with our more complex and customizable <code>CustomizableModel</code> (which allows you to pass in many params like the number of hidden layers, dropout rate, choice of optimizer, etc) the code would look identical to above except that we would be initializing a <code>CustomizableModel</code> instead of a <code>SimpleModel</code>. See the documentation in <code>customizable_model.py</code> for more</p> <pre><code># this will be used to save the model checkpoint that performs the best on the validation set\nbest_model_checkpoint = ModelCheckpoint(\n    monitor=\"val_mse\", # we can depend on any metric we want\n    mode=\"min\",\n    filename=\"best-model-{epoch:02d}-{val_loss:.2f}\",\n    save_top_k=1, # we can save more than just the top model if we want\n)\n\n# Callback to save checkpoints every 2 epochs, regardless of model performance\nperiodic_checkpoint = ModelCheckpoint(\n    filename=\"periodic-{epoch:02d}\",\n    every_n_epochs=2,\n    save_top_k=-1,  # Setting -1 saves all checkpoints  \n)\n\n# csv logger is a very basic logger that will create a csv file with our metrics as we train\ncsv_logger = CSVLogger(\"logs/csv_logs\")  # we define the directory we want the logs to be saved in\n\n# tensorboard logger is a more advanced logger that will create a directory with a bunch of files that can be visualized with tensorboard\n# tensorboard is a library that can be ran via the command line, and will create a local server that can be accessed via a web browser\n# that displays the training metrics in a more interactive way (on a dashboard)\n# you can run tensorboard by running the command `tensorboard --logdir=path/to/log/dir` in the terminal\ntb_logger = TensorBoardLogger(\"logs/tensorboard_logs\", name=\"test-run-2\")\n\n# If we wanted to use these checkpoints and loggers, we would pass them to the trainer like so:\ntrainer_with_checkpoints_and_loggers = Trainer(\n    max_epochs=10,\n    deterministic=True,\n    accelerator=\"cpu\",\n    callbacks=[best_model_checkpoint, periodic_checkpoint],\n    logger=[csv_logger, tb_logger],\n)\n</code></pre> <pre>\n<code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n</code>\n</pre> <pre><code># Load a model from a checkpoint\n# We can load a model from a checkpoint like so:\npath_to_checkpoint = \"example/path/not/real.ckpt\"\n\n# note that we need to use the same model class that was used to save the checkpoint\nmodel = SimpleModel.load_from_checkpoint(path_to_checkpoint)\n\n# we can load the model and continue training from where it left off\ntrainer.fit(model, data_module)\n\n# we could also load the model and test it\ntest_results = trainer.test(model, data_module)\n\n# we could also load the model and make predictions\npredictions = model(data_module.test_dataloader())\n</code></pre> <pre>\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In[5], line 6\n      3 path_to_checkpoint = \"example/path/not/real.ckpt\"\n      5 # note that we need to use the same model class that was used to save the checkpoint\n----&gt; 6 model = SimpleModel.load_from_checkpoint(path_to_checkpoint)\n      8 # we can load the model and continue training from where it left off\n      9 trainer.fit(model, data_module)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/pytorch_lightning/utilities/model_helpers.py:125, in _restricted_classmethod_impl.__get__.&lt;locals&gt;.wrapper(*args, **kwargs)\n    120 if instance is not None and not is_scripting:\n    121     raise TypeError(\n    122         f\"The classmethod `{cls.__name__}.{self.method.__name__}` cannot be called on an instance.\"\n    123         \" Please call it on the class type and make sure the return value is used.\"\n    124     )\n--&gt; 125 return self.method(cls, *args, **kwargs)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/pytorch_lightning/core/module.py:1581, in LightningModule.load_from_checkpoint(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\n   1492 @_restricted_classmethod\n   1493 def load_from_checkpoint(\n   1494     cls,\n   (...)\n   1499     **kwargs: Any,\n   1500 ) -&gt; Self:\n   1501     r\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\n   1502     passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\n   1503 \n   (...)\n   1579 \n   1580     \"\"\"\n-&gt; 1581     loaded = _load_from_checkpoint(\n   1582         cls,  # type: ignore[arg-type]\n   1583         checkpoint_path,\n   1584         map_location,\n   1585         hparams_file,\n   1586         strict,\n   1587         **kwargs,\n   1588     )\n   1589     return cast(Self, loaded)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/pytorch_lightning/core/saving.py:63, in _load_from_checkpoint(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\n     61 map_location = map_location or _default_map_location\n     62 with pl_legacy_patch():\n---&gt; 63     checkpoint = pl_load(checkpoint_path, map_location=map_location)\n     65 # convert legacy checkpoints to the new format\n     66 checkpoint = _pl_migrate_checkpoint(\n     67     checkpoint, checkpoint_path=(checkpoint_path if isinstance(checkpoint_path, (str, Path)) else None)\n     68 )\n\nFile ~/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/lightning_fabric/utilities/cloud_io.py:56, in _load(path_or_url, map_location)\n     51     return torch.hub.load_state_dict_from_url(\n     52         str(path_or_url),\n     53         map_location=map_location,  # type: ignore[arg-type]\n     54     )\n     55 fs = get_filesystem(path_or_url)\n---&gt; 56 with fs.open(path_or_url, \"rb\") as f:\n     57     return torch.load(f, map_location=map_location)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/fsspec/spec.py:1298, in AbstractFileSystem.open(self, path, mode, block_size, cache_options, compression, **kwargs)\n   1296 else:\n   1297     ac = kwargs.pop(\"autocommit\", not self._intrans)\n-&gt; 1298     f = self._open(\n   1299         path,\n   1300         mode=mode,\n   1301         block_size=block_size,\n   1302         autocommit=ac,\n   1303         cache_options=cache_options,\n   1304         **kwargs,\n   1305     )\n   1306     if compression is not None:\n   1307         from fsspec.compression import compr\n\nFile ~/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/fsspec/implementations/local.py:191, in LocalFileSystem._open(self, path, mode, block_size, **kwargs)\n    189 if self.auto_mkdir and \"w\" in mode:\n    190     self.makedirs(self._parent(path), exist_ok=True)\n--&gt; 191 return LocalFileOpener(path, mode, fs=self, **kwargs)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/fsspec/implementations/local.py:355, in LocalFileOpener.__init__(self, path, mode, autocommit, fs, compression, **kwargs)\n    353 self.compression = get_compression(path, compression)\n    354 self.blocksize = io.DEFAULT_BUFFER_SIZE\n--&gt; 355 self._open()\n\nFile ~/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/fsspec/implementations/local.py:360, in LocalFileOpener._open(self)\n    358 if self.f is None or self.f.closed:\n    359     if self.autocommit or \"w\" not in self.mode:\n--&gt; 360         self.f = open(self.path, mode=self.mode)\n    361         if self.compression:\n    362             compress = compr[self.compression]\n\nFileNotFoundError: [Errno 2] No such file or directory: '/Users/ericjia/yeastdnnexplorer/docs/tutorials/example/path/not/real.ckpt'</pre> <pre><code>\n</code></pre>"},{"location":"tutorials/lightning_crash_course/#lightning-crash-course","title":"Lightning Crash Course","text":"<p>This project uses the PyTorch Lightning Library to define and train the machine learning models. PyTorch Lightning is built on top of pytorch, and it abstracts away some of the setup and biolerplate for models (such as writing out training loops). In this notebook, we provide a brief introduction to how to use the models and dataModules we\u2019ve defined to train models.</p>"},{"location":"tutorials/lightning_crash_course/#checkpointing-logging","title":"Checkpointing &amp; Logging","text":"<p>PyTorch lightning gives us the power to define checkpoints and loggers that will be used during training. Checkpoints will save checkpoints of your model during training. In the following code, we define a checkpoint that saves the model\u2019s state when it produced the lowest validation mean squared error on the validation set during training. We also define another checkpoint to periodically save a checkpoint of the model after every 2 training epochs. These checkpoints are powerful because they can be reloaded later. You can continue training a model after loading its checkpoint or you can test the model checkpoint on new data.</p> <p>Loggers are responsible for saving metrics about the model as it is training for us to look at later. We define several loggers to track this data. See the comments above the Tensorboard logger to see how to use Tensorboard to visualize the metrics as the model trains</p> <p>To use checkpoints and loggers, we have to pass them into the Trainer object that we use to train the model with a dataModule. </p> <p>There are many more types of checkpoints and loggers you can create and use, PyTorch Lightning\u2019s documentation is very helpful here</p>"},{"location":"tutorials/lightning_crash_course/#loading-in-and-using-a-checkpoint","title":"Loading in and using a Checkpoint","text":""},{"location":"tutorials/testing_model_metrics/","title":"Testing Model Metrics","text":"<p>In this notebook, we run several simple experiments to gain a deeper understanding of the metrics that we use to evaluate our models and how they respond to changes in the parameters we use for generating our in silico data.</p> <pre><code># imports\nimport torch\n\nfrom pytorch_lightning import Trainer, LightningModule\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\n\nfrom yeastdnnexplorer.data_loaders.synthetic_data_loader import SyntheticDataLoader\nfrom yeastdnnexplorer.ml_models.simple_model import SimpleModel\n\nimport matplotlib.pyplot as plt\n\ntorch.manual_seed(42)  # For CPU\ntorch.cuda.manual_seed_all(42)  # For all CUDA devices\n</code></pre> <p>Define checkpoints and loggers for the models</p> <pre><code># define checkpoints for the model\n# tells it when to save snapshots of the model during training\n# Callback to save the best model based on validation loss\nbest_model_checkpoint = ModelCheckpoint(\n    monitor=\"val_mse\",\n    mode=\"min\",\n    filename=\"best-model-{epoch:02d}-{val_loss:.2f}\",\n    save_top_k=1,\n)\n\n# Callback to save checkpoints every 5 epochs, regardless of performance\nperiodic_checkpoint = ModelCheckpoint(\n    filename=\"periodic-{epoch:02d}\",\n    every_n_epochs=2,\n    save_top_k=-1,  # Setting -1 saves all checkpoints\n)\n\n# configure loggers\ntb_logger = TensorBoardLogger(\"logs/tensorboard_logs\")\ncsv_logger = CSVLogger(\"logs/csv_logs\")\n</code></pre> <p>Here we define a helper function that will generate data and train a simple linear model with all of the given parameters, print the test results of the model, and return the trained model and its test results. This will allow us to easily run experiments where we compare model performance while tweaking data or model parameters.</p> <pre><code>def train_simple_model_with_params(\n    batch_size: int,\n    lr: float,\n    max_epochs: int,\n    using_random_seed: bool,\n    accelerator: str,\n    num_genes: int,\n    bound_mean: float,\n    val_size: float,\n    test_size: float,\n    bound: list[float],\n    n_sample: list[int],\n    max_mean_adjustment: float,\n) -&amp;gt; LightningModule:\n    data_module = SyntheticDataLoader(\n        batch_size=batch_size,\n        num_genes=num_genes,\n        bound_mean=bound_mean,\n        bound=bound,  # old: [0.1, 0.15, 0.2, 0.25, 0.3],\n        n_sample=n_sample,  # sum of this is num of tfs\n        val_size=val_size,\n        test_size=test_size,\n        random_state=42,\n        max_mean_adjustment=max_mean_adjustment,\n    )\n\n    num_tfs = sum(data_module.n_sample)  # sum of all n_sample is the number of TFs\n\n    model = SimpleModel(input_dim=num_tfs, output_dim=num_tfs, lr=lr)\n    trainer = Trainer(\n        max_epochs=max_epochs,\n        deterministic=using_random_seed,\n        accelerator=accelerator,\n        # callbacks=[best_model_checkpoint, periodic_checkpoint],\n        # logger=[tb_logger, csv_logger],\n    )\n    trainer.fit(model, data_module)\n    test_results = trainer.test(model, datamodule=data_module)\n    print(\"Printing test results...\")\n    print(\n        test_results\n    )  # this prints all metrics that were logged during the test phase\n\n    return model, test_results\n</code></pre> <pre><code>bound_means = [0.5, 1.0, 2.0, 3.0, 5.0]\ntest_mses = []\nfor bound_mean in bound_means:\n    model, test_results = train_simple_model_with_params(\n        batch_size=32,\n        lr=0.01,\n        max_epochs=10,\n        using_random_seed=True,\n        accelerator=\"cpu\",\n        num_genes=1000,\n        val_size=0.1,\n        test_size=0.1,\n        bound=[0.5] * 5,\n        n_sample=[1, 1, 2, 2, 4],  # sum of this is num of tfs\n        bound_mean=bound_mean,\n        max_mean_adjustment=0.0\n    )\n    test_mses.append(test_results[0][\"test_mse\"])\n</code></pre> <pre>\n<code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n  | Name    | Type              | Params\n----------------------------------------------\n0 | mae     | MeanAbsoluteError | 0     \n1 | SMSE    | SMSE              | 0     \n2 | linear1 | Linear            | 110   \n----------------------------------------------\n110       Trainable params\n0         Non-trainable params\n110       Total params\n0.000     Total estimated model params size (MB)\n</code>\n</pre> <pre>\n<code>Sanity Checking: |                                                                                            \u2026</code>\n</pre> <pre>\n<code>Training: |                                                                                                   \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>`Trainer.fit` stopped: `max_epochs=10` reached.\n</code>\n</pre> <pre>\n<code>Testing: |                                                                                                    \u2026</code>\n</pre> <pre>\n<code>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        test_mae            0.5135628581047058\n        test_mse             0.416797935962677\n        test_smse           10.241324424743652\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</code>\n</pre> <pre>\n<code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n  | Name    | Type              | Params\n----------------------------------------------\n0 | mae     | MeanAbsoluteError | 0     \n1 | SMSE    | SMSE              | 0     \n2 | linear1 | Linear            | 110   \n----------------------------------------------\n110       Trainable params\n0         Non-trainable params\n110       Total params\n0.000     Total estimated model params size (MB)\n</code>\n</pre> <pre>\n<code>Printing test results...\n[{'test_mse': 0.416797935962677, 'test_mae': 0.5135628581047058, 'test_smse': 10.241324424743652}]\n</code>\n</pre> <pre>\n<code>Sanity Checking: |                                                                                            \u2026</code>\n</pre> <pre>\n<code>Training: |                                                                                                   \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>`Trainer.fit` stopped: `max_epochs=10` reached.\n</code>\n</pre> <pre>\n<code>Testing: |                                                                                                    \u2026</code>\n</pre> <pre>\n<code>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        test_mae            0.5821905136108398\n        test_mse            0.5283595323562622\n        test_smse           10.348736763000488\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</code>\n</pre> <pre>\n<code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n  | Name    | Type              | Params\n----------------------------------------------\n0 | mae     | MeanAbsoluteError | 0     \n1 | SMSE    | SMSE              | 0     \n2 | linear1 | Linear            | 110   \n----------------------------------------------\n110       Trainable params\n0         Non-trainable params\n110       Total params\n0.000     Total estimated model params size (MB)\n</code>\n</pre> <pre>\n<code>Printing test results...\n[{'test_mse': 0.5283595323562622, 'test_mae': 0.5821905136108398, 'test_smse': 10.348736763000488}]\n</code>\n</pre> <pre>\n<code>Sanity Checking: |                                                                                            \u2026</code>\n</pre> <pre>\n<code>Training: |                                                                                                   \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>`Trainer.fit` stopped: `max_epochs=10` reached.\n</code>\n</pre> <pre>\n<code>Testing: |                                                                                                    \u2026</code>\n</pre> <pre>\n<code>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        test_mae            0.8307084441184998\n        test_mse             1.050934910774231\n        test_smse           10.213595390319824\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</code>\n</pre> <pre>\n<code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n</code>\n</pre> <pre>\n<code>Printing test results...\n[{'test_mse': 1.050934910774231, 'test_mae': 0.8307084441184998, 'test_smse': 10.213595390319824}]\n</code>\n</pre> <pre>\n<code>\n  | Name    | Type              | Params\n----------------------------------------------\n0 | mae     | MeanAbsoluteError | 0     \n1 | SMSE    | SMSE              | 0     \n2 | linear1 | Linear            | 110   \n----------------------------------------------\n110       Trainable params\n0         Non-trainable params\n110       Total params\n0.000     Total estimated model params size (MB)\n</code>\n</pre> <pre>\n<code>Sanity Checking: |                                                                                            \u2026</code>\n</pre> <pre>\n<code>Training: |                                                                                                   \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>`Trainer.fit` stopped: `max_epochs=10` reached.\n</code>\n</pre> <pre>\n<code>Testing: |                                                                                                    \u2026</code>\n</pre> <pre>\n<code>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        test_mae            1.1858488321304321\n        test_mse             2.014770984649658\n        test_smse           10.195466995239258\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</code>\n</pre> <pre>\n<code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n  | Name    | Type              | Params\n----------------------------------------------\n0 | mae     | MeanAbsoluteError | 0     \n1 | SMSE    | SMSE              | 0     \n2 | linear1 | Linear            | 110   \n----------------------------------------------\n110       Trainable params\n0         Non-trainable params\n110       Total params\n0.000     Total estimated model params size (MB)\n</code>\n</pre> <pre>\n<code>Printing test results...\n[{'test_mse': 2.014770984649658, 'test_mae': 1.1858488321304321, 'test_smse': 10.195466995239258}]\n</code>\n</pre> <pre>\n<code>Sanity Checking: |                                                                                            \u2026</code>\n</pre> <pre>\n<code>Training: |                                                                                                   \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>`Trainer.fit` stopped: `max_epochs=10` reached.\n</code>\n</pre> <pre>\n<code>Testing: |                                                                                                    \u2026</code>\n</pre> <pre>\n<code>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        test_mae             2.091959238052368\n        test_mse              6.157958984375\n        test_smse           11.987293243408203\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nPrinting test results...\n[{'test_mse': 6.157958984375, 'test_mae': 2.091959238052368, 'test_smse': 11.987293243408203}]\n</code>\n</pre> <p>Plot Results</p> <pre><code>plt.plot(bound_means, test_mses, marker=\"o\")\nplt.xlabel(\"bound Mean\")\nplt.xticks(bound_means, rotation=45)\nplt.yticks(test_mses)\nplt.ylabel(\"Test MSE\")\nplt.title(\"Test MSE as a function of bound Mean\")\nplt.show()\n</code></pre> <pre><code>bound_unbound_ratios = [0.05, 0.1, 0.25, 0.5, 0.75, 0.9]\ntest_mses = []\n\nfor bound_unbound_ratio in bound_unbound_ratios:\n    model, test_results = train_simple_model_with_params(\n        batch_size=32,\n        lr=0.01,\n        max_epochs=10,\n        using_random_seed=True,\n        accelerator=\"cpu\",\n        num_genes=1000,\n        val_size=0.1,\n        test_size=0.1,\n        bound=[bound_unbound_ratio] * 5,\n        n_sample=[1, 1, 2, 2, 4],\n        bound_mean=3.0,\n        max_mean_adjustment=0.0\n    )\n    print(test_results)\n    test_mses.append(test_results[0][\"test_mse\"])\n</code></pre> <pre><code>plt.plot(bound_unbound_ratios, test_mses, marker=\"o\")\nplt.xlabel(\"Percentage of Data in bound Group\")\nplt.ylabel(\"Test MSE\")\nplt.xticks(bound_unbound_ratios, rotation=45)\nplt.yticks(test_mses)\nplt.title(\"Test MSE as a function of bound/unbound ratio (bound mean = 3.0)\")\nplt.show()\n</code></pre> <pre><code># these params will be consistent for both datasets\nnum_genes = 3000\nval_size = 0.1\ntest_size = 0.1\nbound = [0.5] * 5\nn_sample = [1, 1, 2, 2, 4]\nrandom_state = 42\n\n# the first data loader will load a dataset with a small scale and a small bound mean\nsmall_scale_and_mean_dataloader = SyntheticDataLoader(\n    num_genes=num_genes,\n    bound=bound, \n    n_sample=n_sample,\n    val_size=val_size,\n    test_size=test_size,\n    random_state=random_state,\n    bound_mean=1.0,\n    max_mean_adjustment=1.0\n)\n\n# the second data loader will generate a dataset with a large scale and a large bound mean\nlarge_scale_and_mean_dataloader = SyntheticDataLoader(\n    num_genes=num_genes,\n    bound=bound, \n    n_sample=n_sample,\n    val_size=val_size,\n    test_size=test_size,\n    random_state=random_state,\n    bound_mean=10.0,\n    max_mean_adjustment=10.0\n)\n\nnum_tfs = sum(n_sample)  # sum of all n_sample is the number of TFs\n\nmodel = SimpleModel(input_dim=num_tfs, output_dim=num_tfs, lr=0.01)\ntrainer = Trainer(\n    max_epochs=10,\n    deterministic=True,\n    accelerator='cpu',\n    # callbacks=[best_model_checkpoint, periodic_checkpoint],\n    # logger=[tb_logger, csv_logger],\n)\n\ntrainer.fit(model, small_scale_and_mean_dataloader)\nsmall_test_results = trainer.test(model, datamodule=small_scale_and_mean_dataloader)\nprint(\"Printing small test results...\")\nprint(small_test_results)\n\n\ntrainer.fit(model, large_scale_and_mean_dataloader)\nlarge_test_results = trainer.test(model, datamodule=large_scale_and_mean_dataloader)\nprint(\"Printing large test results...\")\nprint(large_test_results)\n</code></pre>"},{"location":"tutorials/testing_model_metrics/#experiment-1","title":"Experiment 1","text":"<p>Now we can use this function to run simple experiments, like testing how the model\u2019s test mse changes when we tweak the mean of the bound genes while holding all other parameters the same. For simplicity, we will not be performing any mean adjustments while generating the data, but we could modify this in the future by incresing the max_mean_adjustment (to use a normal mean adjustment) or adding onto our experiment function to take in our special mean adjustment functions (to use either of the special dependent mean adjustment logic that we\u2019ve defined, see <code>generate_in_silico_data.ipynb</code> for more about this).</p> <p>Note that this will create a lot of output since we are training several models, so we create the plot in a separate cell.</p>"},{"location":"tutorials/testing_model_metrics/#experiment-2","title":"Experiment 2","text":"<p>We can run a similar experiment where we test the effect of the bound / unbound ratio (aka bound / unbound ratio) on the model\u2019s MSE</p>"},{"location":"tutorials/testing_model_metrics/#experiment-3","title":"Experiment 3","text":"<p>Here we run a little experiment to verify that our smse (standardized mean squared error) metric is actually scale and mean invariant (ie doesn\u2019t depend on the scale or mean of the data so long as the variance is roughly the same). Note that this isn\u2019t a perfect experiment, as increasing the max mean adjustment (and therefore the scale) will increase the variance by a factor as a result of how our in silico data generation functions work, so there will definitely be a little difference in smse values, but the difference in mse and mae should be a much larger percentage.</p> <p>We will train and test two models that are exactly the same except that one is trained on a dataset with a small bound mean and mean adjustment and one is trained on a dataset with a large bound mean adn mean adjustment. This will give the two datasets drastically different scales and means. Unfortunately, it will also give them slightly different variances which should cause a slight difference in smse. But again it should be a much smaller percentage difference than the difference between mses and maes</p>"},{"location":"tutorials/visualizing_and_testing_data_generation_methods/","title":"Visualizing and Testing Data Generation Methods","text":"<p>In this notebook, we will run an experiment to display the average perturbation effect values that we generate with the 4 different methods we have for perturbation effect generation (other than the method for generating the perturbation effect values, we will be holding everything else the same). </p> <p>Recall that we have 4 methods for generating perturbation effect data (see <code>generate_in_silico_data.ipynb</code> for more information on these): 1. No Mean Adjustment 2. Standard Mean Adjustment 3. Mean adjustment dependent on all TFs bound to gene in question 4. Mean adjustment dependent on binary relationships between bound and unbound TFs to gene in question.</p> <p>After understanding what the generated data looks like for each of these methods, we will perform another experiment where we train the same model on data generated with each of these methods and compare the model\u2019s performance to a simple linear model.</p> <pre><code># imports\nfrom yeastdnnexplorer.probability_models.generate_data import (generate_gene_population, \n                                                               generate_binding_effects,\n                                                               generate_pvalues,\n                                                               generate_perturbation_effects)\n\nimport torch\nimport matplotlib.pyplot as plt\n\nfrom yeastdnnexplorer.probability_models.relation_classes import Relation, And, Or\nfrom yeastdnnexplorer.probability_models.generate_data import (\n    default_perturbation_effect_adjustment_function,\n    perturbation_effect_adjustment_function_with_tf_relationships,\n    perturbation_effect_adjustment_function_with_tf_relationships_boolean_logic\n)\n\nfrom pytorch_lightning import Trainer, LightningModule, seed_everything\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\nfrom torchsummary import summary\n\nfrom yeastdnnexplorer.data_loaders.synthetic_data_loader import SyntheticDataLoader\nfrom yeastdnnexplorer.ml_models.simple_model import SimpleModel\nfrom yeastdnnexplorer.ml_models.customizable_model import CustomizableModel\n\ntorch.manual_seed(42)  # For CPU\ntorch.cuda.manual_seed_all(42)  # For all CUDA devices\n</code></pre> <p>Generating the binding data will be the same as always, see <code>generate_in_silico_data.ipynb</code></p> <pre><code>n_genes = 3000\n\nbound = [0.5, 0.5, 0.5, 0.5, 0.5]\nn_sample = [1, 1, 2, 2, 4]\n\n# this will be a list of length 10 with a GenePopulation object in each element\ngene_populations_list = []\nfor bound_proportion, n_draws in zip(bound, n_sample):\n    for _ in range(n_draws):\n        gene_populations_list.append(generate_gene_population(n_genes, bound_proportion))\n\n# Generate binding data for each gene population\nbinding_effect_list = [generate_binding_effects(gene_population)\n                     for gene_population in gene_populations_list]\n\n# Calculate p-values for binding data\nbinding_pvalue_list = [generate_pvalues(binding_data) for binding_data in binding_effect_list]\n\nbinding_data_combined = [torch.stack((gene_population.labels, binding_effect, binding_pval), dim=1)\n                         for gene_population, binding_effect, binding_pval\n                         in zip (gene_populations_list, binding_effect_list, binding_pvalue_list)]\n\n# Stack along a new dimension (dim=1) to create a tensor of shape [num_genes, num_TFs, 3]\nbinding_data_tensor = torch.stack(binding_data_combined, dim=1)\n</code></pre> <p>Now we define our experiment, this function will return the average perturbation effects (across n_iterations iterations) for each TF for a specific gene for each of the 4 data generation method we have at our disposal. Due to the randomness in the generated data, we need to find the averages over a number of iterations to get the true common values.</p> <p>We also need to define dictionaries of TF relationships for our third and fourth methods of generating perturbation data, see <code>generate_in_silico_data.ipynb</code> for an explanation of what these represent and how they are used / structured. The documentation in <code>generate_data.py</code> may be helpful as well.</p> <pre><code>tf_relationships = {\n    0: [1],\n    1: [8],\n    2: [5, 6],\n    3: [4],\n    4: [5],\n    5: [9],\n    6: [4],\n    7: [1, 4],\n    8: [6],\n    9: [4],\n}\n\ntf_relationships_dict_boolean_logic = {\n    0: [And(3, 4, 8), Or(3, 7), Or(1, 1)],\n    1: [And(5, Or(7, 8))],\n    2: [],\n    3: [Or(7, 9), And(6, 7)],\n    4: [And(1, 2)],\n    5: [Or(0, 1, 2, 8, 9)],\n    6: [And(0, Or(1, 2))],\n    7: [Or(2, And(5, 6, 9))],\n    8: [],\n    9: [And(6, And(3, Or(0, 9)))],\n}\n\ndef experiment(n_iterations = 10, GENE_IDX = 0):\n    print(\"Bound (1) and Unbound (0) Labels for gene \" + str(GENE_IDX) + \":\")\n    print(binding_data_tensor[GENE_IDX, :, 0])\n\n    num_tfs = sum(n_sample)\n\n    no_mean_adjustment_scores = torch.zeros(num_tfs)\n    normal_mean_adjustment_scores = torch.zeros(num_tfs)\n    dep_mean_adjustment_scores = torch.zeros(num_tfs)\n    boolean_logic_scores = torch.zeros(num_tfs)\n\n    # we generate perturbation effects for each TF on each iteration and then add them to the running totals\n    for i in range(n_iterations):\n        # Method 1: Generate perturbation effects without mean adjustment\n        perturbation_effects_list_no_mean_adjustment = [generate_perturbation_effects(binding_data_tensor[:, tf_index, :].unsqueeze(1), tf_index=0) \n                                                        for tf_index in range(sum(n_sample))]\n        perturbation_effects_list_no_mean_adjustment = torch.stack(perturbation_effects_list_no_mean_adjustment, dim=1)\n\n        # Method 2: Generate perturbation effects with normal mean adjustment\n        perturbation_effects_list_normal_mean_adjustment = generate_perturbation_effects(\n            binding_data_tensor, \n            max_mean_adjustment=10.0\n        )\n\n        # Method 3: Generate perturbation effects with dependent mean adjustment\n        perturbation_effects_list_dep_mean_adjustment = generate_perturbation_effects(\n            binding_data_tensor, \n            tf_relationships=tf_relationships,\n            adjustment_function=perturbation_effect_adjustment_function_with_tf_relationships,\n            max_mean_adjustment=10.0,\n        )\n\n        # Method 4: Generate perturbation effects with binary relations between the TFs\n        perturbation_effects_list_boolean_logic = generate_perturbation_effects(\n            binding_data_tensor, \n            adjustment_function=perturbation_effect_adjustment_function_with_tf_relationships_boolean_logic,\n            tf_relationships=tf_relationships_dict_boolean_logic,\n            max_mean_adjustment=10.0,\n        )\n\n        # take absolute values since we only care about the magnitude of the effects\n        no_mean_adjustment_scores += abs(perturbation_effects_list_no_mean_adjustment[GENE_IDX, :])\n        normal_mean_adjustment_scores += abs(perturbation_effects_list_normal_mean_adjustment[GENE_IDX, :])\n        dep_mean_adjustment_scores += abs(perturbation_effects_list_dep_mean_adjustment[GENE_IDX, :])\n        boolean_logic_scores += abs(perturbation_effects_list_boolean_logic[GENE_IDX, :])\n\n        if (i + 1) % 5 == 0:\n            print(f\"iteration {i+1} completed\")\n\n    # divide by the number of iterations to get the averages\n    no_mean_adjustment_scores /= n_iterations\n    normal_mean_adjustment_scores /= n_iterations\n    dep_mean_adjustment_scores /= n_iterations\n    boolean_logic_scores /= n_iterations\n\n    return no_mean_adjustment_scores, normal_mean_adjustment_scores, dep_mean_adjustment_scores, boolean_logic_scores\n</code></pre> <p>Now we can run the experiment for n_iterations, I find that you should iterate at least 30 times, but closer to 100 is most ideal. This could take 1-5 minutes depending on your computer.</p> <pre><code>GENE_IDX = 0\nexperiment_results = experiment(n_iterations=50, GENE_IDX=GENE_IDX)\n</code></pre> <pre>\n<code>Bound (1) and Unbound (0) Labels for gene 0:\ntensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 1.])\niteration 5 completed\niteration 10 completed\niteration 15 completed\niteration 20 completed\niteration 25 completed\niteration 30 completed\niteration 35 completed\niteration 40 completed\niteration 45 completed\niteration 50 completed\n</code>\n</pre> <p>We now plot our results.</p> <pre><code>x_vals = list(range(sum(n_sample)))\nprint(\"Bound (bound) TFs for gene \" + str(GENE_IDX) + \" are: \" + str(binding_data_tensor[GENE_IDX, :, 0].nonzero().flatten().tolist()))\nprint(\"Unbound (unbound) TFs for gene \" + str(GENE_IDX) + \" are: \" + str((1 - binding_data_tensor[GENE_IDX, :, 0]).nonzero().flatten().tolist()))\nprint(binding_data_tensor[GENE_IDX, :, 0])\nplt.figure(figsize=(10, 6))\n\n# Plot each set of experiment results with a different color\ncolors = ['red', 'green', 'blue', 'orange']\nfor index, results in enumerate(experiment_results):\n    plt.scatter(x_vals, results, color=colors[index])\n\nplt.title('Pertubation Effects for Gene ' + str(GENE_IDX) + ' with Different Adjustment Functions (averaged across 100 trials)')\nplt.xlabel('TF Index')\nplt.ylabel('Perturbation Effect Val')\nplt.xticks(x_vals)\nplt.grid(True)\nplt.legend(['No Mean Adjustment', 'Normal (non-dependent) Mean Adjust', 'Dependent Mean Adjustment', 'Boolean Logic Adjustment'])\nplt.show()\n</code></pre> <pre>\n<code>Bound (bound) TFs for gene 0 are: [3, 4, 5, 6, 7, 9]\nUnbound (unbound) TFs for gene 0 are: [0, 1, 2, 8]\ntensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 1.])\n</code>\n</pre> <p>Recall that for the dependent mean adjustment, the TF in question must be bound and all of the TFs in its dependency array (in the tf_relationships dictionary) must be bound as well. This is why we do not adjust the mean for TF 7 despite it being bound, it depends on TF 1 and TF 4 both being bound, and TF1 is not bound.</p> <p>Similarly, for the boolean logic adjustment, we do not adjust the mean for 6 despite it being bound because it depends on (TF0 &amp;&amp; (TF1 || TF2)) being bound, and none of those 3 TFs are bound to the gene we are studying.</p> <p>Note that if you change GENE_IDX, the random seed, or any of the relationship dictionaris that this explanation will no longer apply to the data you are seeing in the plot.</p> <pre><code># define checkpoints and loggers\nbest_model_checkpoint = ModelCheckpoint(\n    monitor=\"val_mse\",\n    mode=\"min\",\n    filename=\"best-model-{epoch:02d}-{val_loss:.2f}\",\n    save_top_k=1,\n)\n\n# Callback to save checkpoints every 5 epochs, regardless of performance\nperiodic_checkpoint = ModelCheckpoint(\n    filename=\"periodic-{epoch:02d}\",\n    every_n_epochs=2,\n    save_top_k=-1,  # Setting -1 saves all checkpoints\n)\n\n# define loggers for the model\ntb_logger = TensorBoardLogger(\"logs/tensorboard_logs\")\ncsv_logger = CSVLogger(\"logs/csv_logs\")\n</code></pre> <p>We define a few helper functions to run our experiment. We make helper functions for things that will mostly be the same across each training loop so that we don\u2019t have to keep redefining them.</p> <pre><code>def get_data_module(max_mean_adjustment, adjustment_function = default_perturbation_effect_adjustment_function, tf_relationships_dict = {}):\n    return SyntheticDataLoader(\n        batch_size=32,\n        num_genes=4000,\n        bound_mean=3.0,\n        bound=[0.5] * 5,\n        n_sample=[1, 1, 2, 2, 4],  # sum of this is num of tfs\n        val_size=0.1,\n        test_size=0.1,\n        random_state=42,\n        max_mean_adjustment=max_mean_adjustment,\n        adjustment_function=adjustment_function,\n        tf_relationships=tf_relationships_dict,\n    )\n\ndef get_model(num_tfs):\n    return CustomizableModel(\n        input_dim=num_tfs,\n        output_dim=num_tfs,\n        lr=0.01,\n        hidden_layer_num=2,\n        hidden_layer_sizes=[64, 32],\n        activation=\"LeakyReLU\",\n        optimizer=\"RMSprop\",\n        L2_regularization_term=0.0,\n        dropout_rate=0.0,\n    )\n\ndef get_linear_model(num_tfs):\n    return SimpleModel(\n        input_dim=num_tfs,\n        output_dim=num_tfs,\n        lr=0.01\n    )\n\ndef get_trainer():\n    # uncomment callbacks or logggers if you would like checkpoints / logs\n    return Trainer(\n        max_epochs=10,\n        deterministic=True,\n        accelerator=\"cpu\",\n        # callbacks=[best_model_checkpoint, periodic_checkpoint],\n        # logger=[tb_logger, csv_logger],\n    )\n</code></pre> <pre><code># These lists will store the test results for different models and data generation methods\nmodel_mses = []\nlinear_model_test_mses = []\n</code></pre> <p>Train models on data generated with no mean adjustment</p> <pre><code>data_module = get_data_module(0.0)\nnum_tfs = sum(data_module.n_sample)\n\n# nonlinear model\nmodel = get_model(num_tfs)\ntrainer = get_trainer()\ntrainer.fit(model, data_module)\ntest_results = trainer.test(model, datamodule=data_module)\nprint(\"Printing test results...\")\nprint(test_results)\nmodel_mses.append(test_results[0][\"test_mse\"])\n\n# linear model\nlinear_model = get_linear_model(num_tfs)\ntrainer = get_trainer()\ntrainer.fit(linear_model, data_module)\ntest_results = trainer.test(linear_model, datamodule=data_module)\nprint(\"Printing linear model test results\")\nprint(test_results)\nlinear_model_test_mses.append(test_results[0][\"test_mse\"])\n</code></pre> <pre>\n<code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n/Users/ericjia/yeastdnnexplorer/yeastdnnexplorer/data_loaders/synthetic_data_loader.py:260: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_train, Y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(\n/Users/ericjia/yeastdnnexplorer/yeastdnnexplorer/data_loaders/synthetic_data_loader.py:263: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_val, Y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(\n/Users/ericjia/yeastdnnexplorer/yeastdnnexplorer/data_loaders/synthetic_data_loader.py:266: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_test, Y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(\n\n  | Name          | Type              | Params\n----------------------------------------------------\n0 | activation    | LeakyReLU         | 0     \n1 | input_layer   | Linear            | 704   \n2 | hidden_layers | ModuleList        | 2.1 K \n3 | output_layer  | Linear            | 330   \n4 | dropout       | Dropout           | 0     \n5 | mae           | MeanAbsoluteError | 0     \n6 | SMSE          | SMSE              | 0     \n----------------------------------------------------\n3.1 K     Trainable params\n0         Non-trainable params\n3.1 K     Total params\n0.012     Total estimated model params size (MB)\n</code>\n</pre> <pre>\n<code>Sanity Checking: |                                                                                            \u2026</code>\n</pre> <pre>\n<code>/Users/ericjia/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n</code>\n</pre> <pre>\n<code>Training: |                                                                                                   \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>`Trainer.fit` stopped: `max_epochs=10` reached.\n</code>\n</pre> <pre>\n<code>Testing: |                                                                                                    \u2026</code>\n</pre> <pre>\n<code>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        test_mae            0.9961513876914978\n        test_mse            1.5293521881103516\n        test_smse            8.054170608520508\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</code>\n</pre> <pre>\n<code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n  | Name    | Type              | Params\n----------------------------------------------\n0 | mae     | MeanAbsoluteError | 0     \n1 | SMSE    | SMSE              | 0     \n2 | linear1 | Linear            | 110   \n----------------------------------------------\n110       Trainable params\n0         Non-trainable params\n110       Total params\n0.000     Total estimated model params size (MB)\n</code>\n</pre> <pre>\n<code>Printing test results...\n[{'test_mse': 1.5293521881103516, 'test_mae': 0.9961513876914978, 'test_smse': 8.054170608520508}]\n</code>\n</pre> <pre>\n<code>Sanity Checking: |                                                                                            \u2026</code>\n</pre> <pre>\n<code>/Users/ericjia/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (9) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n</code>\n</pre> <pre>\n<code>Training: |                                                                                                   \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>`Trainer.fit` stopped: `max_epochs=10` reached.\n</code>\n</pre> <pre>\n<code>Testing: |                                                                                                    \u2026</code>\n</pre> <pre>\n<code>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        test_mae            1.3378851413726807\n        test_mse            3.5256669521331787\n        test_smse           18.614917755126953\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nPrinting linear model test results\n[{'test_mse': 3.5256669521331787, 'test_mae': 1.3378851413726807, 'test_smse': 18.614917755126953}]\n</code>\n</pre> <p>Train models on data generated with normal mean adjustments</p> <pre><code>data_module = get_data_module(3.0)\nnum_tfs = sum(data_module.n_sample)\n\n# nonlinear model\nmodel = get_model(num_tfs)\ntrainer = get_trainer()\ntrainer.fit(model, data_module)\ntest_results = trainer.test(model, datamodule=data_module)\nprint(\"Printing test results...\")\nprint(test_results)\nmodel_mses.append(test_results[0][\"test_mse\"])\n\n# linear model\nlinear_model = get_linear_model(num_tfs)\ntrainer = get_trainer()\ntrainer.fit(linear_model, data_module)\ntest_results = trainer.test(linear_model, datamodule=data_module)\nprint(\"Printing linear model test results\")\nprint(test_results)\nlinear_model_test_mses.append(test_results[0][\"test_mse\"])\n</code></pre> <pre>\n<code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n  | Name          | Type              | Params\n----------------------------------------------------\n0 | activation    | LeakyReLU         | 0     \n1 | input_layer   | Linear            | 704   \n2 | hidden_layers | ModuleList        | 2.1 K \n3 | output_layer  | Linear            | 330   \n4 | dropout       | Dropout           | 0     \n5 | mae           | MeanAbsoluteError | 0     \n6 | SMSE          | SMSE              | 0     \n----------------------------------------------------\n3.1 K     Trainable params\n0         Non-trainable params\n3.1 K     Total params\n0.012     Total estimated model params size (MB)\n</code>\n</pre> <pre>\n<code>Sanity Checking: |                                                                                            \u2026</code>\n</pre> <pre>\n<code>Training: |                                                                                                   \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>`Trainer.fit` stopped: `max_epochs=10` reached.\n</code>\n</pre> <pre>\n<code>Testing: |                                                                                                    \u2026</code>\n</pre> <pre>\n<code>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        test_mae            1.4062790870666504\n        test_mse            3.1310036182403564\n        test_smse            7.031686305999756\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</code>\n</pre> <pre>\n<code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n</code>\n</pre> <pre>\n<code>Printing test results...\n[{'test_mse': 3.1310036182403564, 'test_mae': 1.4062790870666504, 'test_smse': 7.031686305999756}]\n</code>\n</pre> <pre>\n<code>\n  | Name    | Type              | Params\n----------------------------------------------\n0 | mae     | MeanAbsoluteError | 0     \n1 | SMSE    | SMSE              | 0     \n2 | linear1 | Linear            | 110   \n----------------------------------------------\n110       Trainable params\n0         Non-trainable params\n110       Total params\n0.000     Total estimated model params size (MB)\n</code>\n</pre> <pre>\n<code>Sanity Checking: |                                                                                            \u2026</code>\n</pre> <pre>\n<code>Training: |                                                                                                   \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>`Trainer.fit` stopped: `max_epochs=10` reached.\n</code>\n</pre> <pre>\n<code>Testing: |                                                                                                    \u2026</code>\n</pre> <pre>\n<code>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        test_mae             2.076347827911377\n        test_mse             8.463006973266602\n        test_smse           19.093679428100586\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nPrinting linear model test results\n[{'test_mse': 8.463006973266602, 'test_mae': 2.076347827911377, 'test_smse': 19.093679428100586}]\n</code>\n</pre> <p>Train model on data generated with dependent mean adjustments (method 3)</p> <pre><code># define dictionary of relations between TFs (see generate_in_silico_data.ipynb for an explanation of how this dict is defined / used)\ntf_relationships_dict = {\n    0: [1],\n    1: [8],\n    2: [5, 6],\n    3: [4],\n    4: [5],\n    5: [9],\n    6: [4],\n    7: [1, 4],\n    8: [6],\n    9: [4],\n}\n\ndata_module = get_data_module(\n    3.0, \n    adjustment_function=perturbation_effect_adjustment_function_with_tf_relationships, \n    tf_relationships_dict=tf_relationships_dict\n)\nnum_tfs = sum(data_module.n_sample)\n\nprint(\"Number of TFs: \", num_tfs)\n\n# nonlinear model\nmodel = get_model(num_tfs)\ntrainer = get_trainer()\ntrainer.fit(model, data_module)\ntest_results = trainer.test(model, datamodule=data_module)\nprint(\"Printing test results...\")\nprint(test_results)\nmodel_mses.append(test_results[0][\"test_mse\"])\n\n# linear model\nlinear_model = get_linear_model(num_tfs)\ntrainer = get_trainer()\ntrainer.fit(linear_model, data_module)\ntest_results = trainer.test(linear_model, datamodule=data_module)\nprint(\"Printing linear model test results\")\nprint(test_results)\nlinear_model_test_mses.append(test_results[0][\"test_mse\"])\n</code></pre> <pre>\n<code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n</code>\n</pre> <pre>\n<code>Number of TFs:  10\n</code>\n</pre> <pre>\n<code>\n  | Name          | Type              | Params\n----------------------------------------------------\n0 | activation    | LeakyReLU         | 0     \n1 | input_layer   | Linear            | 704   \n2 | hidden_layers | ModuleList        | 2.1 K \n3 | output_layer  | Linear            | 330   \n4 | dropout       | Dropout           | 0     \n5 | mae           | MeanAbsoluteError | 0     \n6 | SMSE          | SMSE              | 0     \n----------------------------------------------------\n3.1 K     Trainable params\n0         Non-trainable params\n3.1 K     Total params\n0.012     Total estimated model params size (MB)\n</code>\n</pre> <pre>\n<code>Sanity Checking: |                                                                                            \u2026</code>\n</pre> <pre>\n<code>Training: |                                                                                                   \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>`Trainer.fit` stopped: `max_epochs=10` reached.\n</code>\n</pre> <pre>\n<code>Testing: |                                                                                                    \u2026</code>\n</pre> <pre>\n<code>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        test_mae             1.116166114807129\n        test_mse            2.3565773963928223\n        test_smse           7.7063517570495605\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</code>\n</pre> <pre>\n<code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n</code>\n</pre> <pre>\n<code>Printing test results...\n[{'test_mse': 2.3565773963928223, 'test_mae': 1.116166114807129, 'test_smse': 7.7063517570495605}]\n</code>\n</pre> <pre>\n<code>\n  | Name    | Type              | Params\n----------------------------------------------\n0 | mae     | MeanAbsoluteError | 0     \n1 | SMSE    | SMSE              | 0     \n2 | linear1 | Linear            | 110   \n----------------------------------------------\n110       Trainable params\n0         Non-trainable params\n110       Total params\n0.000     Total estimated model params size (MB)\n</code>\n</pre> <pre>\n<code>Sanity Checking: |                                                                                            \u2026</code>\n</pre> <pre>\n<code>Training: |                                                                                                   \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>`Trainer.fit` stopped: `max_epochs=10` reached.\n</code>\n</pre> <pre>\n<code>Testing: |                                                                                                    \u2026</code>\n</pre> <pre>\n<code>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        test_mae             1.30489981174469\n        test_mse            3.8554797172546387\n        test_smse           12.853811264038086\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nPrinting linear model test results\n[{'test_mse': 3.8554797172546387, 'test_mae': 1.30489981174469, 'test_smse': 12.853811264038086}]\n</code>\n</pre> <p>Train models on data generated using the binary relations between TFs (method 4)</p> <pre><code>tf_relationships_dict_boolean_logic = {\n    0: [And(3, 4, 8), Or(3, 7), Or(1, 1)],\n    1: [And(5, Or(7, 8))],\n    2: [],\n    3: [Or(7, 9), And(6, 7)],\n    4: [And(1, 2)],\n    5: [Or(0, 1, 2, 8, 9)],\n    6: [And(0, Or(1, 2))],\n    7: [Or(2, And(5, 6, 9))],\n    8: [],\n    9: [And(6, And(3, Or(0, 9)))],\n}\n\ndata_module = get_data_module(\n    3.0, \n    adjustment_function=perturbation_effect_adjustment_function_with_tf_relationships_boolean_logic, \n    tf_relationships_dict=tf_relationships_dict_boolean_logic\n)\n\n# nonlinear model\nmodel = get_model(num_tfs)\ntrainer = get_trainer()\ntrainer.fit(model, data_module)\ntest_results = trainer.test(model, datamodule=data_module)\nprint(\"Printing test results...\")\nprint(test_results)\nmodel_mses.append(test_results[0][\"test_mse\"])\n\n# linear model\nlinear_model = get_linear_model(num_tfs)\ntrainer = get_trainer()\ntrainer.fit(linear_model, data_module)\ntest_results = trainer.test(linear_model, datamodule=data_module)\nprint(\"Printing linear model test results\")\nprint(test_results)\nlinear_model_test_mses.append(test_results[0][\"test_mse\"])\n</code></pre> <pre>\n<code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n  | Name          | Type              | Params\n----------------------------------------------------\n0 | activation    | LeakyReLU         | 0     \n1 | input_layer   | Linear            | 704   \n2 | hidden_layers | ModuleList        | 2.1 K \n3 | output_layer  | Linear            | 330   \n4 | dropout       | Dropout           | 0     \n5 | mae           | MeanAbsoluteError | 0     \n6 | SMSE          | SMSE              | 0     \n----------------------------------------------------\n3.1 K     Trainable params\n0         Non-trainable params\n3.1 K     Total params\n0.012     Total estimated model params size (MB)\n</code>\n</pre> <pre>\n<code>Sanity Checking: |                                                                                            \u2026</code>\n</pre> <pre>\n<code>Training: |                                                                                                   \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>`Trainer.fit` stopped: `max_epochs=10` reached.\n</code>\n</pre> <pre>\n<code>Testing: |                                                                                                    \u2026</code>\n</pre> <pre>\n<code>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        test_mae            1.1678574085235596\n        test_mse             2.400193452835083\n        test_smse            7.260862827301025\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</code>\n</pre> <pre>\n<code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n</code>\n</pre> <pre>\n<code>Printing test results...\n[{'test_mse': 2.400193452835083, 'test_mae': 1.1678574085235596, 'test_smse': 7.260862827301025}]\n</code>\n</pre> <pre>\n<code>\n  | Name    | Type              | Params\n----------------------------------------------\n0 | mae     | MeanAbsoluteError | 0     \n1 | SMSE    | SMSE              | 0     \n2 | linear1 | Linear            | 110   \n----------------------------------------------\n110       Trainable params\n0         Non-trainable params\n110       Total params\n0.000     Total estimated model params size (MB)\n</code>\n</pre> <pre>\n<code>Sanity Checking: |                                                                                            \u2026</code>\n</pre> <pre>\n<code>Training: |                                                                                                   \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>`Trainer.fit` stopped: `max_epochs=10` reached.\n</code>\n</pre> <pre>\n<code>Testing: |                                                                                                    \u2026</code>\n</pre> <pre>\n<code>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        test_mae            1.3436788320541382\n        test_mse             4.040103912353516\n        test_smse            12.06108570098877\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nPrinting linear model test results\n[{'test_mse': 4.040103912353516, 'test_mae': 1.3436788320541382, 'test_smse': 12.06108570098877}]\n</code>\n</pre> <p>Now we can plot the results of our experiment. TODO add explantion for plot here? Probably not the right place to put it (I feel like that belongs in the presentation or something, because this notebook could be modified and the explanation wouldn\u2019t make sense)</p> <pre><code>data_gen_methods = [\"No Mean Adjustment\", \"Dependent Mean Adjustment\", \"TF Dependent Mean Adjustment\", \"TF Dependent Mean Adjust with Boolean Logic\"]\nplt.figure(figsize=(10, 6))\nplt.scatter(data_gen_methods, model_mses, color='blue')\nplt.scatter(data_gen_methods, linear_model_test_mses, color='orange')\nplt.title('Model MSE Comparison (bound mean = 3.0)')\nplt.xlabel('Model')\nplt.ylabel('MSE')\nplt.grid(True)\nplt.xticks(rotation=45, ha=\"right\")\nplt.legend(['Complex (Customizable) Model', 'Linear Model'])\nplt.tight_layout()  # Adjust layout to make room for the rotated x-axis labels\nplt.show()\n</code></pre> <pre><code>\n</code></pre>"},{"location":"tutorials/visualizing_and_testing_data_generation_methods/#training-models-on-data-generated-from-the-4-different-methods","title":"Training models on data generated from the 4 different methods","text":"<p>In the next experiment, we will be training the exact same model on data generated from each of these 4 methods. We will also train a simple linear model on all four methods to use as a baseline to compare to. Other than the method used to generate the data, everything else will be held the same.</p>"}]}