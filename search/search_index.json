{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"yeastdnnexplorer","text":""},{"location":"#introduction","title":"Introduction","text":"<p><code>yeastdnnexplorer</code> is intended to serve as a development environment for exploring different DNN models to infer the relationship between transcription factors and target genes using binding and perturbation expression data.</p>"},{"location":"#installation","title":"Installation","text":"<p>This repo has not yet been added to PyPI. See the developer installation below.</p>"},{"location":"#development","title":"Development","text":"<ol> <li>git clone the repo</li> <li><code>cd</code> into the local version of the repo</li> <li>choose one (or more) of the following (only poetry currently supported)</li> </ol>"},{"location":"#poetry","title":"poetry","text":"<p>You can also install the dependencies using poetry. I prefer setting the following:</p> <pre><code>poetry config virtualenvs.in-project true\n</code></pre> <p>So that the virtual environments are installed in the project directory as <code>.venv</code></p> <p>After cloning and <code>cd</code>ing into the repo, you can install the dependencies with:</p> <pre><code>poetry install\n</code></pre>"},{"location":"#mkdocs","title":"mkdocs","text":"<p>The documentation is build with mkdocs:</p>"},{"location":"#commands","title":"Commands","text":"<p>After building the environment with poetry, you can use <code>poetry run</code> or a poetry shell to execute the following:</p> <ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre> <p>To update the gh-pages documentation, use <code>poetry run mkdocs gh-deply</code></p>"},{"location":"data_loaders/real_data_loader/","title":"Real Data Loader","text":"<p>             Bases: <code>LightningDataModule</code></p> <p>A class to load in data from the CSV data for various binding and perturbation experiments.</p> <p>After loading in the data, the data loader will parse the data into the form expected by our models. It will also split the data into training, testing, and validation sets for the model to use.</p> <p>NOTE: Right now the only binding dataset this works with is the brent_nf_cc dataset because it has the same set of genes in each CSV file. This is the case for all of the perturbation datasets, but not for the other 2 binding datasets. In the future we would like to write a dataModule that handles the other 2 binding datasets. For now, you can only pass in a parameter for the title of the perturb response dataset that you want to use, and brent_nf_cc is hardcoded as the binding dataset.</p> Source code in <code>yeastdnnexplorer/data_loaders/real_data_loader.py</code> <pre><code>class RealDataLoader(LightningDataModule):\n    \"\"\"\n    A class to load in data from the CSV data for various binding and perturbation\n    experiments.\n\n    After loading in the data, the data loader will parse the data into the form\n    expected by our models. It will also split the data into training, testing, and\n    validation sets for the model to use.\n\n    NOTE: Right now the only binding dataset this works with is the brent_nf_cc dataset\n    because it has the same set of genes in each CSV file. This is the case for all of\n    the perturbation datasets, but not for the other 2 binding datasets. In the future\n    we would like to write a dataModule that handles the other 2 binding datasets. For\n    now, you can only pass in a parameter for the title of the perturb response\n    dataset that you want to use, and brent_nf_cc is hardcoded as the binding dataset.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        batch_size: int = 32,\n        val_size: float = 0.1,\n        test_size: float = 0.1,\n        random_state: int = 42,\n        data_dir_path: str | None = None,\n        perturbation_dataset_title: str = \"hu_reimann_tfko\",\n    ) -&gt; None:\n        \"\"\"\n        Constructor of RealDataLoader.\n\n        :param batch_size: The number of samples in each mini-batch\n        :type batch_size: int\n        :param val_size: The proportion of the dataset to include in the validation\n            split\n        :type val_size: float\n        :param test_size: The proportion of the dataset to include in the test split\n        :type test_size: float\n        :param random_state: The random seed to use for splitting the data (keep this\n            consistent to ensure reproduceability)\n        :type random_state: int\n        :param data_dir_path: The path to the directory containing the CSV files for the\n            binding and perturbation data\n        :type data_dir_path: str\n        :param perturbation_dataset_title: The title of the perturbation dataset to use\n            (one of 'hu_reimann_tfko', 'kemmeren_tfko', or 'mcisaac_oe')\n        :type perturbation_dataset_title: str\n        :raises TypeError: If batch_size is not an positive integer\n        :raises TypeError: If val_size is not a float between 0 and 1 (inclusive)\n        :raises TypeError: If test_size is not a float between 0 and 1 (inclusive)\n        :raises TypeError: If random_state is not an integer\n        :raises ValueError: If val_size + test_size is greater than 1 (i.e. the splits\n            are too large)\n        :raises ValueError: if no data_dir is provided\n        :raises AssertinoError: if the dataset sizes do not match up after reading in\n            the data from the CSV files\n\n        \"\"\"\n        if not isinstance(batch_size, int) or batch_size &lt; 1:\n            raise TypeError(\"batch_size must be a positive integer\")\n        if not isinstance(val_size, (int, float)) or val_size &lt;= 0 or val_size &gt;= 1:\n            raise TypeError(\"val_size must be a float between 0 and 1 (inclusive)\")\n        if not isinstance(test_size, (int, float)) or test_size &lt;= 0 or test_size &gt;= 1:\n            raise TypeError(\"test_size must be a float between 0 and 1 (inclusive)\")\n        if not isinstance(random_state, int):\n            raise TypeError(\"random_state must be an integer\")\n        if data_dir_path is None:\n            raise ValueError(\"data_dir_path must be provided\")\n        if test_size + val_size &gt; 1:\n            raise ValueError(\"val_size + test_size must be less than or equal to 1\")\n        if not isinstance(\n            perturbation_dataset_title, str\n        ) and perturbation_dataset_title in [\n            \"hu_reimann_tfko\",\n            \"kemmeren_tfko\",\n            \"mcisaac_oe\",\n        ]:\n            raise TypeError(\n                \"perturbation_dataset_title must be a string and must be one\"\n                \" of 'hu_reimann_tfko', 'kemmeren_tfko', or 'mcisaac_oe'\"\n            )\n\n        super().__init__()\n        self.batch_size = batch_size\n        self.val_size = val_size\n        self.test_size = test_size\n        self.random_state = random_state\n        self.data_dir_path = data_dir_path\n        self.perturbation_dataset_title = perturbation_dataset_title\n\n        self.final_data_tensor: torch.Tensor = None\n        self.binding_effect_matrix: torch.Tensor | None = None\n        self.perturbation_effect_matrix: torch.Tensor | None = None\n        self.val_dataset: TensorDataset | None = None\n        self.test_dataset: TensorDataset | None = None\n\n    def prepare_data(self) -&gt; None:\n        \"\"\"\n        This function reads in the binding data and perturbation data from the CSV files\n        that we have for these datasets.\n\n        It throws out any genes that are not present in both the binding and\n        perturbation sets, and then structures the data in a way that the model expects\n        and can use\n\n        \"\"\"\n\n        brent_cc_path = os.path.join(self.data_dir_path, \"binding/brent_nf_cc\")\n        brent_nf_csv_files = [\n            f for f in os.listdir(brent_cc_path) if f.endswith(\".csv\")\n        ]\n        perturb_dataset_path = os.path.join(\n            self.data_dir_path, f\"perturbation/{self.perturbation_dataset_title}\"\n        )\n        perturb_dataset_csv_files = [\n            f for f in os.listdir(perturb_dataset_path) if f.endswith(\".csv\")\n        ]\n\n        # get a list of the genes in the binding data csvs\n        # for brent_cc (and the 3 perturb response datasets) the genes are\n        # in the same order in each csv, so it suffices to grab the target_locus_tag\n        # column from the first one\n        brent_cc_genes_ids = pd.read_csv(\n            os.path.join(brent_cc_path, brent_nf_csv_files[0])\n        )[\"target_locus_tag\"]\n        perturb_dataset_genes_ids = pd.read_csv(\n            os.path.join(perturb_dataset_path, perturb_dataset_csv_files[0])\n        )[\"target_locus_tag\"]\n\n        # Get the intersection of the genes in the binding and perturbation data\n        common_genes = set(brent_cc_genes_ids).intersection(perturb_dataset_genes_ids)\n\n        # Read in binding data from csv files\n        binding_data_effects = pd.DataFrame()\n        binding_data_pvalues = pd.DataFrame()\n        for i, file in enumerate(brent_nf_csv_files):\n            file_path = os.path.join(brent_cc_path, file)\n            df = pd.read_csv(file_path)\n\n            # only keep the genes that are in the intersection\n            # of the genes in the binding and perturbation data\n            df = df[df[\"target_locus_tag\"].isin(common_genes)]\n\n            # we need to handle duplicates now\n            # (some datasets have multiple occurrences of the same gene)\n            # we will keep the occurrence with the highest value in the 'effect' column\n            # we can do this by sorting the dataframe by the 'effect' column\n            # in descending order and keeping the fist occurrence of each gene\n            # this does require us to do some additional work later (see how we\n            # are consistently setting the index to 'target_locus_tag',\n            # this ensures all of our datasets are in the same order)\n            df = df.sort_values(\"effect\", ascending=False).drop_duplicates(\n                subset=\"target_locus_tag\", keep=\"first\"\n            )\n\n            # on the first iteration, add target_locus_tag column to the binding data\n            if i == 0:\n                binding_data_effects[\"target_locus_tag\"] = df[\"target_locus_tag\"]\n                binding_data_pvalues[\"target_locus_tag\"] = df[\"target_locus_tag\"]\n                binding_data_effects.set_index(\"target_locus_tag\", inplace=True)\n                binding_data_pvalues.set_index(\"target_locus_tag\", inplace=True)\n\n            binding_data_effects[file] = df.set_index(\"target_locus_tag\")[\"effect\"]\n            binding_data_pvalues[file] = df.set_index(\"target_locus_tag\")[\"pvalue\"]\n\n        # Read in perturbation data from csv files\n        perturbation_effects = pd.DataFrame()\n        perturbation_pvalues = pd.DataFrame()\n        for i, file in enumerate(perturb_dataset_csv_files):\n            file_path = os.path.join(perturb_dataset_path, file)\n            df = pd.read_csv(file_path)\n\n            # only keep the genes that are in the\n            # intersection of the genes in the binding and perturbation data\n            df = df[df[\"target_locus_tag\"].isin(common_genes)]\n\n            # handle duplicates\n            df = df.sort_values(\"effect\", ascending=False).drop_duplicates(\n                subset=\"target_locus_tag\", keep=\"first\"\n            )\n\n            # on the first iteration, add the target_locus_tag\n            # column to the perturbation data\n            if i == 0:\n                perturbation_effects[\"target_locus_tag\"] = df[\"target_locus_tag\"]\n                perturbation_pvalues[\"target_locus_tag\"] = df[\"target_locus_tag\"]\n                perturbation_effects.set_index(\"target_locus_tag\", inplace=True)\n                perturbation_pvalues.set_index(\"target_locus_tag\", inplace=True)\n\n            perturbation_effects[file] = df.set_index(\"target_locus_tag\")[\"effect\"]\n            perturbation_pvalues[file] = df.set_index(\"target_locus_tag\")[\"pvalue\"]\n\n        # shapes should be equal at this point\n        assert binding_data_effects.shape == perturbation_effects.shape\n        assert binding_data_pvalues.shape == perturbation_pvalues.shape\n\n        # reindex so that the rows in binding and perturb data match up\n        # (we need genes to be in the same order)\n        perturbation_effects = perturbation_effects.reindex(binding_data_effects.index)\n        perturbation_pvalues = perturbation_pvalues.reindex(binding_data_pvalues.index)\n\n        # concat the data into the shape expected by the model\n        # we need to first convert the data to tensors\n        binding_data_effects_tensor = torch.tensor(\n            binding_data_effects.values, dtype=torch.float64\n        )\n        binding_data_pvalues_tensor = torch.tensor(\n            binding_data_pvalues.values, dtype=torch.float64\n        )\n        perturbation_effects_tensor = torch.tensor(\n            perturbation_effects.values, dtype=torch.float64\n        )\n        perturbation_pvalues_tensor = torch.tensor(\n            perturbation_pvalues.values, dtype=torch.float64\n        )\n\n        # note that we no longer have a bound / unbound tensor\n        # (like for the synthetic data)\n        self.final_data_tensor = torch.stack(\n            [\n                binding_data_effects_tensor,\n                binding_data_pvalues_tensor,\n                perturbation_effects_tensor,\n                perturbation_pvalues_tensor,\n            ],\n            dim=-1,\n        )\n\n    def setup(self, stage: str | None = None) -&gt; None:\n        \"\"\"\n        This function runs after prepare_data finishes and is used to split the data\n        into train, validation, and test sets It ensures that these datasets are of the\n        correct dimensionality and size to be used by the model.\n\n        :param stage: The stage of the data setup (either 'fit' for training, 'validate'\n            for validation, or 'test' for testing), unused for now as the model is not\n            complicated enough to necessitate this\n        :type stage: Optional[str]\n\n        \"\"\"\n        self.binding_effect_matrix = self.final_data_tensor[:, :, 0]\n        self.perturbation_effect_matrix = self.final_data_tensor[:, :, 2]\n\n        # split into train, val, and test\n        X_train, X_temp, Y_train, Y_temp = train_test_split(\n            self.binding_effect_matrix,\n            self.perturbation_effect_matrix,\n            test_size=(self.val_size + self.test_size),\n            random_state=self.random_state,\n        )\n\n        # normalize test_size so that it is a percentage of the remaining data\n        self.test_size = self.test_size / (self.val_size + self.test_size)\n        X_val, X_test, Y_val, Y_test = train_test_split(\n            X_temp, Y_temp, test_size=self.test_size, random_state=self.random_state\n        )\n\n        # Convert to tensors\n        X_train, Y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(\n            Y_train, dtype=torch.float32\n        )\n        X_val, Y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(\n            Y_val, dtype=torch.float32\n        )\n        X_test, Y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(\n            Y_test, dtype=torch.float32\n        )\n\n        # Set our datasets\n        self.train_dataset = TensorDataset(X_train, Y_train)\n        self.val_dataset = TensorDataset(X_val, Y_val)\n        self.test_dataset = TensorDataset(X_test, Y_test)\n\n    def train_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Function to return the training dataloader, we shuffle to avoid learning based\n        on the order of the data.\n\n        :return: The training dataloader\n        :rtype: DataLoader\n\n        \"\"\"\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            num_workers=15,\n            shuffle=True,\n            persistent_workers=True,\n        )\n\n    def val_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Function to return the validation dataloader.\n\n        :return: The validation dataloader\n        :rtype: DataLoader\n\n        \"\"\"\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            num_workers=15,\n            shuffle=False,\n            persistent_workers=True,\n        )\n\n    def test_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Function to return the testing dataloader.\n\n        :return: The testing dataloader\n        :rtype: DataLoader\n\n        \"\"\"\n        return DataLoader(\n            self.test_dataset,\n            batch_size=self.batch_size,\n            num_workers=15,\n            shuffle=False,\n            persistent_workers=True,\n        )\n</code></pre>"},{"location":"data_loaders/real_data_loader/#yeastdnnexplorer.data_loaders.real_data_loader.RealDataLoader.__init__","title":"<code>__init__(batch_size=32, val_size=0.1, test_size=0.1, random_state=42, data_dir_path=None, perturbation_dataset_title='hu_reimann_tfko')</code>","text":"<p>Constructor of RealDataLoader.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The number of samples in each mini-batch</p> <code>32</code> <code>val_size</code> <code>float</code> <p>The proportion of the dataset to include in the validation split</p> <code>0.1</code> <code>test_size</code> <code>float</code> <p>The proportion of the dataset to include in the test split</p> <code>0.1</code> <code>random_state</code> <code>int</code> <p>The random seed to use for splitting the data (keep this consistent to ensure reproduceability)</p> <code>42</code> <code>data_dir_path</code> <code>str | None</code> <p>The path to the directory containing the CSV files for the binding and perturbation data</p> <code>None</code> <code>perturbation_dataset_title</code> <code>str</code> <p>The title of the perturbation dataset to use (one of \u2018hu_reimann_tfko\u2019, \u2018kemmeren_tfko\u2019, or \u2018mcisaac_oe\u2019)</p> <code>'hu_reimann_tfko'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If batch_size is not an positive integer</p> <code>TypeError</code> <p>If val_size is not a float between 0 and 1 (inclusive)</p> <code>TypeError</code> <p>If test_size is not a float between 0 and 1 (inclusive)</p> <code>TypeError</code> <p>If random_state is not an integer</p> <code>ValueError</code> <p>If val_size + test_size is greater than 1 (i.e. the splits are too large)</p> <code>ValueError</code> <p>if no data_dir is provided</p> <code>AssertinoError</code> <p>if the dataset sizes do not match up after reading in the data from the CSV files</p> Source code in <code>yeastdnnexplorer/data_loaders/real_data_loader.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 32,\n    val_size: float = 0.1,\n    test_size: float = 0.1,\n    random_state: int = 42,\n    data_dir_path: str | None = None,\n    perturbation_dataset_title: str = \"hu_reimann_tfko\",\n) -&gt; None:\n    \"\"\"\n    Constructor of RealDataLoader.\n\n    :param batch_size: The number of samples in each mini-batch\n    :type batch_size: int\n    :param val_size: The proportion of the dataset to include in the validation\n        split\n    :type val_size: float\n    :param test_size: The proportion of the dataset to include in the test split\n    :type test_size: float\n    :param random_state: The random seed to use for splitting the data (keep this\n        consistent to ensure reproduceability)\n    :type random_state: int\n    :param data_dir_path: The path to the directory containing the CSV files for the\n        binding and perturbation data\n    :type data_dir_path: str\n    :param perturbation_dataset_title: The title of the perturbation dataset to use\n        (one of 'hu_reimann_tfko', 'kemmeren_tfko', or 'mcisaac_oe')\n    :type perturbation_dataset_title: str\n    :raises TypeError: If batch_size is not an positive integer\n    :raises TypeError: If val_size is not a float between 0 and 1 (inclusive)\n    :raises TypeError: If test_size is not a float between 0 and 1 (inclusive)\n    :raises TypeError: If random_state is not an integer\n    :raises ValueError: If val_size + test_size is greater than 1 (i.e. the splits\n        are too large)\n    :raises ValueError: if no data_dir is provided\n    :raises AssertinoError: if the dataset sizes do not match up after reading in\n        the data from the CSV files\n\n    \"\"\"\n    if not isinstance(batch_size, int) or batch_size &lt; 1:\n        raise TypeError(\"batch_size must be a positive integer\")\n    if not isinstance(val_size, (int, float)) or val_size &lt;= 0 or val_size &gt;= 1:\n        raise TypeError(\"val_size must be a float between 0 and 1 (inclusive)\")\n    if not isinstance(test_size, (int, float)) or test_size &lt;= 0 or test_size &gt;= 1:\n        raise TypeError(\"test_size must be a float between 0 and 1 (inclusive)\")\n    if not isinstance(random_state, int):\n        raise TypeError(\"random_state must be an integer\")\n    if data_dir_path is None:\n        raise ValueError(\"data_dir_path must be provided\")\n    if test_size + val_size &gt; 1:\n        raise ValueError(\"val_size + test_size must be less than or equal to 1\")\n    if not isinstance(\n        perturbation_dataset_title, str\n    ) and perturbation_dataset_title in [\n        \"hu_reimann_tfko\",\n        \"kemmeren_tfko\",\n        \"mcisaac_oe\",\n    ]:\n        raise TypeError(\n            \"perturbation_dataset_title must be a string and must be one\"\n            \" of 'hu_reimann_tfko', 'kemmeren_tfko', or 'mcisaac_oe'\"\n        )\n\n    super().__init__()\n    self.batch_size = batch_size\n    self.val_size = val_size\n    self.test_size = test_size\n    self.random_state = random_state\n    self.data_dir_path = data_dir_path\n    self.perturbation_dataset_title = perturbation_dataset_title\n\n    self.final_data_tensor: torch.Tensor = None\n    self.binding_effect_matrix: torch.Tensor | None = None\n    self.perturbation_effect_matrix: torch.Tensor | None = None\n    self.val_dataset: TensorDataset | None = None\n    self.test_dataset: TensorDataset | None = None\n</code></pre>"},{"location":"data_loaders/real_data_loader/#yeastdnnexplorer.data_loaders.real_data_loader.RealDataLoader.prepare_data","title":"<code>prepare_data()</code>","text":"<p>This function reads in the binding data and perturbation data from the CSV files that we have for these datasets.</p> <p>It throws out any genes that are not present in both the binding and perturbation sets, and then structures the data in a way that the model expects and can use</p> Source code in <code>yeastdnnexplorer/data_loaders/real_data_loader.py</code> <pre><code>def prepare_data(self) -&gt; None:\n    \"\"\"\n    This function reads in the binding data and perturbation data from the CSV files\n    that we have for these datasets.\n\n    It throws out any genes that are not present in both the binding and\n    perturbation sets, and then structures the data in a way that the model expects\n    and can use\n\n    \"\"\"\n\n    brent_cc_path = os.path.join(self.data_dir_path, \"binding/brent_nf_cc\")\n    brent_nf_csv_files = [\n        f for f in os.listdir(brent_cc_path) if f.endswith(\".csv\")\n    ]\n    perturb_dataset_path = os.path.join(\n        self.data_dir_path, f\"perturbation/{self.perturbation_dataset_title}\"\n    )\n    perturb_dataset_csv_files = [\n        f for f in os.listdir(perturb_dataset_path) if f.endswith(\".csv\")\n    ]\n\n    # get a list of the genes in the binding data csvs\n    # for brent_cc (and the 3 perturb response datasets) the genes are\n    # in the same order in each csv, so it suffices to grab the target_locus_tag\n    # column from the first one\n    brent_cc_genes_ids = pd.read_csv(\n        os.path.join(brent_cc_path, brent_nf_csv_files[0])\n    )[\"target_locus_tag\"]\n    perturb_dataset_genes_ids = pd.read_csv(\n        os.path.join(perturb_dataset_path, perturb_dataset_csv_files[0])\n    )[\"target_locus_tag\"]\n\n    # Get the intersection of the genes in the binding and perturbation data\n    common_genes = set(brent_cc_genes_ids).intersection(perturb_dataset_genes_ids)\n\n    # Read in binding data from csv files\n    binding_data_effects = pd.DataFrame()\n    binding_data_pvalues = pd.DataFrame()\n    for i, file in enumerate(brent_nf_csv_files):\n        file_path = os.path.join(brent_cc_path, file)\n        df = pd.read_csv(file_path)\n\n        # only keep the genes that are in the intersection\n        # of the genes in the binding and perturbation data\n        df = df[df[\"target_locus_tag\"].isin(common_genes)]\n\n        # we need to handle duplicates now\n        # (some datasets have multiple occurrences of the same gene)\n        # we will keep the occurrence with the highest value in the 'effect' column\n        # we can do this by sorting the dataframe by the 'effect' column\n        # in descending order and keeping the fist occurrence of each gene\n        # this does require us to do some additional work later (see how we\n        # are consistently setting the index to 'target_locus_tag',\n        # this ensures all of our datasets are in the same order)\n        df = df.sort_values(\"effect\", ascending=False).drop_duplicates(\n            subset=\"target_locus_tag\", keep=\"first\"\n        )\n\n        # on the first iteration, add target_locus_tag column to the binding data\n        if i == 0:\n            binding_data_effects[\"target_locus_tag\"] = df[\"target_locus_tag\"]\n            binding_data_pvalues[\"target_locus_tag\"] = df[\"target_locus_tag\"]\n            binding_data_effects.set_index(\"target_locus_tag\", inplace=True)\n            binding_data_pvalues.set_index(\"target_locus_tag\", inplace=True)\n\n        binding_data_effects[file] = df.set_index(\"target_locus_tag\")[\"effect\"]\n        binding_data_pvalues[file] = df.set_index(\"target_locus_tag\")[\"pvalue\"]\n\n    # Read in perturbation data from csv files\n    perturbation_effects = pd.DataFrame()\n    perturbation_pvalues = pd.DataFrame()\n    for i, file in enumerate(perturb_dataset_csv_files):\n        file_path = os.path.join(perturb_dataset_path, file)\n        df = pd.read_csv(file_path)\n\n        # only keep the genes that are in the\n        # intersection of the genes in the binding and perturbation data\n        df = df[df[\"target_locus_tag\"].isin(common_genes)]\n\n        # handle duplicates\n        df = df.sort_values(\"effect\", ascending=False).drop_duplicates(\n            subset=\"target_locus_tag\", keep=\"first\"\n        )\n\n        # on the first iteration, add the target_locus_tag\n        # column to the perturbation data\n        if i == 0:\n            perturbation_effects[\"target_locus_tag\"] = df[\"target_locus_tag\"]\n            perturbation_pvalues[\"target_locus_tag\"] = df[\"target_locus_tag\"]\n            perturbation_effects.set_index(\"target_locus_tag\", inplace=True)\n            perturbation_pvalues.set_index(\"target_locus_tag\", inplace=True)\n\n        perturbation_effects[file] = df.set_index(\"target_locus_tag\")[\"effect\"]\n        perturbation_pvalues[file] = df.set_index(\"target_locus_tag\")[\"pvalue\"]\n\n    # shapes should be equal at this point\n    assert binding_data_effects.shape == perturbation_effects.shape\n    assert binding_data_pvalues.shape == perturbation_pvalues.shape\n\n    # reindex so that the rows in binding and perturb data match up\n    # (we need genes to be in the same order)\n    perturbation_effects = perturbation_effects.reindex(binding_data_effects.index)\n    perturbation_pvalues = perturbation_pvalues.reindex(binding_data_pvalues.index)\n\n    # concat the data into the shape expected by the model\n    # we need to first convert the data to tensors\n    binding_data_effects_tensor = torch.tensor(\n        binding_data_effects.values, dtype=torch.float64\n    )\n    binding_data_pvalues_tensor = torch.tensor(\n        binding_data_pvalues.values, dtype=torch.float64\n    )\n    perturbation_effects_tensor = torch.tensor(\n        perturbation_effects.values, dtype=torch.float64\n    )\n    perturbation_pvalues_tensor = torch.tensor(\n        perturbation_pvalues.values, dtype=torch.float64\n    )\n\n    # note that we no longer have a bound / unbound tensor\n    # (like for the synthetic data)\n    self.final_data_tensor = torch.stack(\n        [\n            binding_data_effects_tensor,\n            binding_data_pvalues_tensor,\n            perturbation_effects_tensor,\n            perturbation_pvalues_tensor,\n        ],\n        dim=-1,\n    )\n</code></pre>"},{"location":"data_loaders/real_data_loader/#yeastdnnexplorer.data_loaders.real_data_loader.RealDataLoader.setup","title":"<code>setup(stage=None)</code>","text":"<p>This function runs after prepare_data finishes and is used to split the data into train, validation, and test sets It ensures that these datasets are of the correct dimensionality and size to be used by the model.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str | None</code> <p>The stage of the data setup (either \u2018fit\u2019 for training, \u2018validate\u2019 for validation, or \u2018test\u2019 for testing), unused for now as the model is not complicated enough to necessitate this</p> <code>None</code> Source code in <code>yeastdnnexplorer/data_loaders/real_data_loader.py</code> <pre><code>def setup(self, stage: str | None = None) -&gt; None:\n    \"\"\"\n    This function runs after prepare_data finishes and is used to split the data\n    into train, validation, and test sets It ensures that these datasets are of the\n    correct dimensionality and size to be used by the model.\n\n    :param stage: The stage of the data setup (either 'fit' for training, 'validate'\n        for validation, or 'test' for testing), unused for now as the model is not\n        complicated enough to necessitate this\n    :type stage: Optional[str]\n\n    \"\"\"\n    self.binding_effect_matrix = self.final_data_tensor[:, :, 0]\n    self.perturbation_effect_matrix = self.final_data_tensor[:, :, 2]\n\n    # split into train, val, and test\n    X_train, X_temp, Y_train, Y_temp = train_test_split(\n        self.binding_effect_matrix,\n        self.perturbation_effect_matrix,\n        test_size=(self.val_size + self.test_size),\n        random_state=self.random_state,\n    )\n\n    # normalize test_size so that it is a percentage of the remaining data\n    self.test_size = self.test_size / (self.val_size + self.test_size)\n    X_val, X_test, Y_val, Y_test = train_test_split(\n        X_temp, Y_temp, test_size=self.test_size, random_state=self.random_state\n    )\n\n    # Convert to tensors\n    X_train, Y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(\n        Y_train, dtype=torch.float32\n    )\n    X_val, Y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(\n        Y_val, dtype=torch.float32\n    )\n    X_test, Y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(\n        Y_test, dtype=torch.float32\n    )\n\n    # Set our datasets\n    self.train_dataset = TensorDataset(X_train, Y_train)\n    self.val_dataset = TensorDataset(X_val, Y_val)\n    self.test_dataset = TensorDataset(X_test, Y_test)\n</code></pre>"},{"location":"data_loaders/real_data_loader/#yeastdnnexplorer.data_loaders.real_data_loader.RealDataLoader.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Function to return the testing dataloader.</p> <p>Returns:</p> Type Description <code>DataLoader</code> <p>The testing dataloader</p> Source code in <code>yeastdnnexplorer/data_loaders/real_data_loader.py</code> <pre><code>def test_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Function to return the testing dataloader.\n\n    :return: The testing dataloader\n    :rtype: DataLoader\n\n    \"\"\"\n    return DataLoader(\n        self.test_dataset,\n        batch_size=self.batch_size,\n        num_workers=15,\n        shuffle=False,\n        persistent_workers=True,\n    )\n</code></pre>"},{"location":"data_loaders/real_data_loader/#yeastdnnexplorer.data_loaders.real_data_loader.RealDataLoader.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Function to return the training dataloader, we shuffle to avoid learning based on the order of the data.</p> <p>Returns:</p> Type Description <code>DataLoader</code> <p>The training dataloader</p> Source code in <code>yeastdnnexplorer/data_loaders/real_data_loader.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Function to return the training dataloader, we shuffle to avoid learning based\n    on the order of the data.\n\n    :return: The training dataloader\n    :rtype: DataLoader\n\n    \"\"\"\n    return DataLoader(\n        self.train_dataset,\n        batch_size=self.batch_size,\n        num_workers=15,\n        shuffle=True,\n        persistent_workers=True,\n    )\n</code></pre>"},{"location":"data_loaders/real_data_loader/#yeastdnnexplorer.data_loaders.real_data_loader.RealDataLoader.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Function to return the validation dataloader.</p> <p>Returns:</p> Type Description <code>DataLoader</code> <p>The validation dataloader</p> Source code in <code>yeastdnnexplorer/data_loaders/real_data_loader.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Function to return the validation dataloader.\n\n    :return: The validation dataloader\n    :rtype: DataLoader\n\n    \"\"\"\n    return DataLoader(\n        self.val_dataset,\n        batch_size=self.batch_size,\n        num_workers=15,\n        shuffle=False,\n        persistent_workers=True,\n    )\n</code></pre>"},{"location":"data_loaders/synthetic_data_loader/","title":"Synthetic Data Loader","text":"<p>             Bases: <code>LightningDataModule</code></p> <p>A class for a synthetic data loader that generates synthetic bindiing &amp; perturbation effect data for training, validation, and testing a model This class contains all of the logic for generating and parsing the synthetic data, as well as splitting it into train, validation, and test sets It is a subclass of pytorch_lightning.LightningDataModule, which is similar to a regular PyTorch DataLoader but with added functionality for data loading.</p> Source code in <code>yeastdnnexplorer/data_loaders/synthetic_data_loader.py</code> <pre><code>class SyntheticDataLoader(LightningDataModule):\n    \"\"\"A class for a synthetic data loader that generates synthetic bindiing &amp;\n    perturbation effect data for training, validation, and testing a model This class\n    contains all of the logic for generating and parsing the synthetic data, as well as\n    splitting it into train, validation, and test sets It is a subclass of\n    pytorch_lightning.LightningDataModule, which is similar to a regular PyTorch\n    DataLoader but with added functionality for data loading.\"\"\"\n\n    def __init__(\n        self,\n        batch_size: int = 32,\n        num_genes: int = 1000,\n        bound: list[float] = [0.1, 0.2, 0.2, 0.4, 0.5],\n        bound_mean: float = 3.0,\n        n_sample: list[int] = [1, 2, 2, 4, 4],\n        val_size: float = 0.1,\n        test_size: float = 0.1,\n        random_state: int = 42,\n        max_mean_adjustment: float = 0.0,\n        adjustment_function: Callable[\n            [torch.Tensor, float, float, float], torch.Tensor\n        ] = default_perturbation_effect_adjustment_function,\n        tf_relationships: dict[int, list[int] | list[Relation]] = {},\n    ) -&gt; None:\n        \"\"\"\n        Constructor of SyntheticDataLoader.\n\n        :param batch_size: The number of samples in each mini-batch\n        :type batch_size: int\n        :param num_genes: The number of genes in the synthetic data (this is the number\n            of datapoints in our dataset)\n        :type num_genes: int\n        :param bound: The proportion of genes in each sample group that are put in the\n            bound grop (i.e. have a non-zero binding effect and expression response)\n        :type bound: List[int]\n        :param n_sample: The number of samples to draw from each bound group\n        :type n_sample: List[int]\n        :param val_size: The proportion of the dataset to include in the validation\n            split\n        :type val_size: float\n        :param test_size: The proportion of the dataset to include in the test split\n        :type test_size: float\n        :param random_state: The random seed to use for splitting the data (keep this\n            consistent to ensure reproduceability)\n        :type random_state: int\n        :param bound_mean: The mean of the bound distribution\n        :type bound_mean: float\n        :param max_mean_adjustment: The maximum mean adjustment to apply to the mean\n                                    of the bound (bound) perturbation effects\n        :type max_mean_adjustment: float\n        :param adjustment_function: A function that adjusts the mean of the bound\n                                    (bound) perturbation effects\n        :type adjustment_function: Callable[[torch.Tensor, float, float,\n                                   float, dict[int, list[int]]], torch.Tensor]\n        :raises TypeError: If batch_size is not an positive integer\n        :raises TypeError: If num_genes is not an positive integer\n        :raises TypeError: If bound is not a list of integers or floats\n        :raises TypeError: If n_sample is not a list of integers\n        :raises TypeError: If val_size is not a float between 0 and 1 (inclusive)\n        :raises TypeError: If test_size is not a float between 0 and 1 (inclusive)\n        :raises TypeError: If random_state is not an integer\n        :raises TypeError: If bound_mean is not a float\n        :raises ValueError: If val_size + test_size is greater than 1 (i.e. the splits\n            are too large)\n\n        \"\"\"\n        if not isinstance(batch_size, int) or batch_size &lt; 1:\n            raise TypeError(\"batch_size must be a positive integer\")\n        if not isinstance(num_genes, int) or num_genes &lt; 1:\n            raise TypeError(\"num_genes must be a positive integer\")\n        if not isinstance(bound, list) or not all(\n            isinstance(x, (int, float)) for x in bound\n        ):\n            raise TypeError(\"bound must be a list of integers or floats\")\n        if not isinstance(n_sample, list) or not all(\n            isinstance(x, int) for x in n_sample\n        ):\n            raise TypeError(\"n_sample must be a list of integers\")\n        if not isinstance(val_size, (int, float)) or val_size &lt;= 0 or val_size &gt;= 1:\n            raise TypeError(\"val_size must be a float between 0 and 1 (inclusive)\")\n        if not isinstance(test_size, (int, float)) or test_size &lt;= 0 or test_size &gt;= 1:\n            raise TypeError(\"test_size must be a float between 0 and 1 (inclusive)\")\n        if not isinstance(random_state, int):\n            raise TypeError(\"random_state must be an integer\")\n        if not isinstance(bound_mean, float):\n            raise TypeError(\"bound_mean must be a float\")\n        if test_size + val_size &gt; 1:\n            raise ValueError(\"val_size + test_size must be less than or equal to 1\")\n\n        super().__init__()\n        self.batch_size = batch_size\n        self.num_genes = num_genes\n        self.bound_mean = bound_mean\n        self.bound = bound or [0.1, 0.15, 0.2, 0.25, 0.3]\n        self.n_sample = n_sample or [1 for _ in range(len(self.bound))]\n        self.num_tfs = sum(self.n_sample)  # sum of all n_sample is the number of TFs\n        self.val_size = val_size\n        self.test_size = test_size\n        self.random_state = random_state\n\n        self.max_mean_adjustment = max_mean_adjustment\n        self.adjustment_function = adjustment_function\n        self.tf_relationships = tf_relationships\n\n        self.final_data_tensor: torch.Tensor = None\n        self.binding_effect_matrix: torch.Tensor | None = None\n        self.perturbation_effect_matrix: torch.Tensor | None = None\n        self.val_dataset: TensorDataset | None = None\n        self.test_dataset: TensorDataset | None = None\n\n    def prepare_data(self) -&gt; None:\n        \"\"\"Function to generate the raw synthetic data and save it in a tensor For\n        explanations of the functions used to generate the data, see the\n        generate_in_silico_data tutorial notebook in the docs No assertion checks are\n        performed as that is handled in the functions in generate_data.py.\"\"\"\n        # this will be a list of length 10 with a GenePopulation object in each element\n        gene_populations_list = []\n        for bound_proportion, n_draws in zip(self.bound, self.n_sample):\n            for _ in range(n_draws):\n                gene_populations_list.append(\n                    generate_gene_population(self.num_genes, bound_proportion)\n                )\n\n        # Generate binding data for each gene population\n        binding_effect_list = [\n            generate_binding_effects(gene_population)\n            for gene_population in gene_populations_list\n        ]\n\n        # Calculate p-values for binding data\n        binding_pvalue_list = [\n            generate_pvalues(binding_data) for binding_data in binding_effect_list\n        ]\n\n        binding_data_combined = [\n            torch.stack((gene_population.labels, binding_effect, binding_pval), dim=1)\n            for gene_population, binding_effect, binding_pval in zip(\n                gene_populations_list, binding_effect_list, binding_pvalue_list\n            )\n        ]\n\n        # Stack along a new dimension (dim=1) to create a tensor of shape\n        # [num_genes, num_TFs, 3]\n        binding_data_tensor = torch.stack(binding_data_combined, dim=1)\n\n        # if we are using a mean adjustment, we need to generate perturbation\n        # effects in a slightly different way than if we are not using\n        # a mean adjustment\n        if self.max_mean_adjustment &gt; 0:\n            perturbation_effects_list = generate_perturbation_effects(\n                binding_data_tensor,\n                bound_mean=self.bound_mean,\n                tf_index=0,  # unused\n                max_mean_adjustment=self.max_mean_adjustment,\n                adjustment_function=self.adjustment_function,\n                tf_relationships=self.tf_relationships,\n            )\n\n            perturbation_pvalue_list = torch.zeros_like(perturbation_effects_list)\n            for col_index in range(perturbation_effects_list.shape[1]):\n                perturbation_pvalue_list[:, col_index] = generate_pvalues(\n                    perturbation_effects_list[:, col_index]\n                )\n\n            # take absolute values\n            perturbation_effects_list = torch.abs(perturbation_effects_list)\n\n            perturbation_effects_tensor = perturbation_effects_list\n            perturbation_pvalues_tensor = perturbation_pvalue_list\n        else:\n            perturbation_effects_list = [\n                generate_perturbation_effects(\n                    binding_data_tensor[:, tf_index, :].unsqueeze(1),\n                    bound_mean=self.bound_mean,\n                    tf_index=0,  # unused\n                )\n                for tf_index in range(sum(self.n_sample))\n            ]\n            perturbation_pvalue_list = [\n                generate_pvalues(perturbation_effects)\n                for perturbation_effects in perturbation_effects_list\n            ]\n\n            # take absolute values\n            perturbation_effects_list = [\n                torch.abs(perturbation_effects)\n                for perturbation_effects in perturbation_effects_list\n            ]\n\n            # Convert lists to tensors\n            perturbation_effects_tensor = torch.stack(perturbation_effects_list, dim=1)\n            perturbation_pvalues_tensor = torch.stack(perturbation_pvalue_list, dim=1)\n\n        # Ensure perturbation data is reshaped to match [n_genes, n_tfs]\n        # This step might need adjustment based on the actual shapes of your tensors.\n        perturbation_effects_tensor = perturbation_effects_tensor.unsqueeze(\n            -1\n        )  # Adds an extra dimension for concatenation\n        perturbation_pvalues_tensor = perturbation_pvalues_tensor.unsqueeze(\n            -1\n        )  # Adds an extra dimension for concatenation\n\n        # Concatenate along the last dimension to form a [n_genes, n_tfs, 5] tensor\n        self.final_data_tensor = torch.cat(\n            (\n                binding_data_tensor,\n                perturbation_effects_tensor,\n                perturbation_pvalues_tensor,\n            ),\n            dim=2,\n        )\n\n    def setup(self, stage: str | None = None) -&gt; None:\n        \"\"\"\n        This function runs after prepare_data finishes and is used to split the data\n        into train, validation, and test sets It ensures that these datasets are of the\n        correct dimensionality and size to be used by the model.\n\n        :param stage: The stage of the data setup (either 'fit' for training, 'validate'\n            for validation, or 'test' for testing), unused for now as the model is not\n            complicated enough to necessitate this\n        :type stage: Optional[str]\n\n        \"\"\"\n        self.binding_effect_matrix = self.final_data_tensor[:, :, 1]\n        self.perturbation_effect_matrix = self.final_data_tensor[:, :, 3]\n\n        # split into train, val, and test\n        X_train, X_temp, Y_train, Y_temp = train_test_split(\n            self.binding_effect_matrix,\n            self.perturbation_effect_matrix,\n            test_size=(self.val_size + self.test_size),\n            random_state=self.random_state,\n        )\n\n        # normalize test_size so that it is a percentage of the remaining data\n        self.test_size = self.test_size / (self.val_size + self.test_size)\n        X_val, X_test, Y_val, Y_test = train_test_split(\n            X_temp, Y_temp, test_size=self.test_size, random_state=self.random_state\n        )\n\n        # Convert to tensors\n        X_train, Y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(\n            Y_train, dtype=torch.float32\n        )\n        X_val, Y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(\n            Y_val, dtype=torch.float32\n        )\n        X_test, Y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(\n            Y_test, dtype=torch.float32\n        )\n\n        # Set our datasets\n        self.train_dataset = TensorDataset(X_train, Y_train)\n        self.val_dataset = TensorDataset(X_val, Y_val)\n        self.test_dataset = TensorDataset(X_test, Y_test)\n\n    def train_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Function to return the training dataloader, we shuffle to avoid learning based\n        on the order of the data.\n\n        :return: The training dataloader\n        :rtype: DataLoader\n\n        \"\"\"\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            num_workers=15,\n            shuffle=True,\n            persistent_workers=True,\n        )\n\n    def val_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Function to return the validation dataloader.\n\n        :return: The validation dataloader\n        :rtype: DataLoader\n\n        \"\"\"\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            num_workers=15,\n            shuffle=False,\n            persistent_workers=True,\n        )\n\n    def test_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Function to return the testing dataloader.\n\n        :return: The testing dataloader\n        :rtype: DataLoader\n\n        \"\"\"\n        return DataLoader(\n            self.test_dataset,\n            batch_size=self.batch_size,\n            num_workers=15,\n            shuffle=False,\n            persistent_workers=True,\n        )\n</code></pre>"},{"location":"data_loaders/synthetic_data_loader/#yeastdnnexplorer.data_loaders.synthetic_data_loader.SyntheticDataLoader.__init__","title":"<code>__init__(batch_size=32, num_genes=1000, bound=[0.1, 0.2, 0.2, 0.4, 0.5], bound_mean=3.0, n_sample=[1, 2, 2, 4, 4], val_size=0.1, test_size=0.1, random_state=42, max_mean_adjustment=0.0, adjustment_function=default_perturbation_effect_adjustment_function, tf_relationships={})</code>","text":"<p>Constructor of SyntheticDataLoader.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The number of samples in each mini-batch</p> <code>32</code> <code>num_genes</code> <code>int</code> <p>The number of genes in the synthetic data (this is the number of datapoints in our dataset)</p> <code>1000</code> <code>bound</code> <code>list[float]</code> <p>The proportion of genes in each sample group that are put in the bound grop (i.e. have a non-zero binding effect and expression response)</p> <code>[0.1, 0.2, 0.2, 0.4, 0.5]</code> <code>n_sample</code> <code>list[int]</code> <p>The number of samples to draw from each bound group</p> <code>[1, 2, 2, 4, 4]</code> <code>val_size</code> <code>float</code> <p>The proportion of the dataset to include in the validation split</p> <code>0.1</code> <code>test_size</code> <code>float</code> <p>The proportion of the dataset to include in the test split</p> <code>0.1</code> <code>random_state</code> <code>int</code> <p>The random seed to use for splitting the data (keep this consistent to ensure reproduceability)</p> <code>42</code> <code>bound_mean</code> <code>float</code> <p>The mean of the bound distribution</p> <code>3.0</code> <code>max_mean_adjustment</code> <code>float</code> <p>The maximum mean adjustment to apply to the mean of the bound (bound) perturbation effects</p> <code>0.0</code> <code>adjustment_function</code> <code>Callable[[Tensor, float, float, float], Tensor]</code> <p>A function that adjusts the mean of the bound (bound) perturbation effects</p> <code>default_perturbation_effect_adjustment_function</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If batch_size is not an positive integer</p> <code>TypeError</code> <p>If num_genes is not an positive integer</p> <code>TypeError</code> <p>If bound is not a list of integers or floats</p> <code>TypeError</code> <p>If n_sample is not a list of integers</p> <code>TypeError</code> <p>If val_size is not a float between 0 and 1 (inclusive)</p> <code>TypeError</code> <p>If test_size is not a float between 0 and 1 (inclusive)</p> <code>TypeError</code> <p>If random_state is not an integer</p> <code>TypeError</code> <p>If bound_mean is not a float</p> <code>ValueError</code> <p>If val_size + test_size is greater than 1 (i.e. the splits are too large)</p> Source code in <code>yeastdnnexplorer/data_loaders/synthetic_data_loader.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 32,\n    num_genes: int = 1000,\n    bound: list[float] = [0.1, 0.2, 0.2, 0.4, 0.5],\n    bound_mean: float = 3.0,\n    n_sample: list[int] = [1, 2, 2, 4, 4],\n    val_size: float = 0.1,\n    test_size: float = 0.1,\n    random_state: int = 42,\n    max_mean_adjustment: float = 0.0,\n    adjustment_function: Callable[\n        [torch.Tensor, float, float, float], torch.Tensor\n    ] = default_perturbation_effect_adjustment_function,\n    tf_relationships: dict[int, list[int] | list[Relation]] = {},\n) -&gt; None:\n    \"\"\"\n    Constructor of SyntheticDataLoader.\n\n    :param batch_size: The number of samples in each mini-batch\n    :type batch_size: int\n    :param num_genes: The number of genes in the synthetic data (this is the number\n        of datapoints in our dataset)\n    :type num_genes: int\n    :param bound: The proportion of genes in each sample group that are put in the\n        bound grop (i.e. have a non-zero binding effect and expression response)\n    :type bound: List[int]\n    :param n_sample: The number of samples to draw from each bound group\n    :type n_sample: List[int]\n    :param val_size: The proportion of the dataset to include in the validation\n        split\n    :type val_size: float\n    :param test_size: The proportion of the dataset to include in the test split\n    :type test_size: float\n    :param random_state: The random seed to use for splitting the data (keep this\n        consistent to ensure reproduceability)\n    :type random_state: int\n    :param bound_mean: The mean of the bound distribution\n    :type bound_mean: float\n    :param max_mean_adjustment: The maximum mean adjustment to apply to the mean\n                                of the bound (bound) perturbation effects\n    :type max_mean_adjustment: float\n    :param adjustment_function: A function that adjusts the mean of the bound\n                                (bound) perturbation effects\n    :type adjustment_function: Callable[[torch.Tensor, float, float,\n                               float, dict[int, list[int]]], torch.Tensor]\n    :raises TypeError: If batch_size is not an positive integer\n    :raises TypeError: If num_genes is not an positive integer\n    :raises TypeError: If bound is not a list of integers or floats\n    :raises TypeError: If n_sample is not a list of integers\n    :raises TypeError: If val_size is not a float between 0 and 1 (inclusive)\n    :raises TypeError: If test_size is not a float between 0 and 1 (inclusive)\n    :raises TypeError: If random_state is not an integer\n    :raises TypeError: If bound_mean is not a float\n    :raises ValueError: If val_size + test_size is greater than 1 (i.e. the splits\n        are too large)\n\n    \"\"\"\n    if not isinstance(batch_size, int) or batch_size &lt; 1:\n        raise TypeError(\"batch_size must be a positive integer\")\n    if not isinstance(num_genes, int) or num_genes &lt; 1:\n        raise TypeError(\"num_genes must be a positive integer\")\n    if not isinstance(bound, list) or not all(\n        isinstance(x, (int, float)) for x in bound\n    ):\n        raise TypeError(\"bound must be a list of integers or floats\")\n    if not isinstance(n_sample, list) or not all(\n        isinstance(x, int) for x in n_sample\n    ):\n        raise TypeError(\"n_sample must be a list of integers\")\n    if not isinstance(val_size, (int, float)) or val_size &lt;= 0 or val_size &gt;= 1:\n        raise TypeError(\"val_size must be a float between 0 and 1 (inclusive)\")\n    if not isinstance(test_size, (int, float)) or test_size &lt;= 0 or test_size &gt;= 1:\n        raise TypeError(\"test_size must be a float between 0 and 1 (inclusive)\")\n    if not isinstance(random_state, int):\n        raise TypeError(\"random_state must be an integer\")\n    if not isinstance(bound_mean, float):\n        raise TypeError(\"bound_mean must be a float\")\n    if test_size + val_size &gt; 1:\n        raise ValueError(\"val_size + test_size must be less than or equal to 1\")\n\n    super().__init__()\n    self.batch_size = batch_size\n    self.num_genes = num_genes\n    self.bound_mean = bound_mean\n    self.bound = bound or [0.1, 0.15, 0.2, 0.25, 0.3]\n    self.n_sample = n_sample or [1 for _ in range(len(self.bound))]\n    self.num_tfs = sum(self.n_sample)  # sum of all n_sample is the number of TFs\n    self.val_size = val_size\n    self.test_size = test_size\n    self.random_state = random_state\n\n    self.max_mean_adjustment = max_mean_adjustment\n    self.adjustment_function = adjustment_function\n    self.tf_relationships = tf_relationships\n\n    self.final_data_tensor: torch.Tensor = None\n    self.binding_effect_matrix: torch.Tensor | None = None\n    self.perturbation_effect_matrix: torch.Tensor | None = None\n    self.val_dataset: TensorDataset | None = None\n    self.test_dataset: TensorDataset | None = None\n</code></pre>"},{"location":"data_loaders/synthetic_data_loader/#yeastdnnexplorer.data_loaders.synthetic_data_loader.SyntheticDataLoader.prepare_data","title":"<code>prepare_data()</code>","text":"<p>Function to generate the raw synthetic data and save it in a tensor For explanations of the functions used to generate the data, see the generate_in_silico_data tutorial notebook in the docs No assertion checks are performed as that is handled in the functions in generate_data.py.</p> Source code in <code>yeastdnnexplorer/data_loaders/synthetic_data_loader.py</code> <pre><code>def prepare_data(self) -&gt; None:\n    \"\"\"Function to generate the raw synthetic data and save it in a tensor For\n    explanations of the functions used to generate the data, see the\n    generate_in_silico_data tutorial notebook in the docs No assertion checks are\n    performed as that is handled in the functions in generate_data.py.\"\"\"\n    # this will be a list of length 10 with a GenePopulation object in each element\n    gene_populations_list = []\n    for bound_proportion, n_draws in zip(self.bound, self.n_sample):\n        for _ in range(n_draws):\n            gene_populations_list.append(\n                generate_gene_population(self.num_genes, bound_proportion)\n            )\n\n    # Generate binding data for each gene population\n    binding_effect_list = [\n        generate_binding_effects(gene_population)\n        for gene_population in gene_populations_list\n    ]\n\n    # Calculate p-values for binding data\n    binding_pvalue_list = [\n        generate_pvalues(binding_data) for binding_data in binding_effect_list\n    ]\n\n    binding_data_combined = [\n        torch.stack((gene_population.labels, binding_effect, binding_pval), dim=1)\n        for gene_population, binding_effect, binding_pval in zip(\n            gene_populations_list, binding_effect_list, binding_pvalue_list\n        )\n    ]\n\n    # Stack along a new dimension (dim=1) to create a tensor of shape\n    # [num_genes, num_TFs, 3]\n    binding_data_tensor = torch.stack(binding_data_combined, dim=1)\n\n    # if we are using a mean adjustment, we need to generate perturbation\n    # effects in a slightly different way than if we are not using\n    # a mean adjustment\n    if self.max_mean_adjustment &gt; 0:\n        perturbation_effects_list = generate_perturbation_effects(\n            binding_data_tensor,\n            bound_mean=self.bound_mean,\n            tf_index=0,  # unused\n            max_mean_adjustment=self.max_mean_adjustment,\n            adjustment_function=self.adjustment_function,\n            tf_relationships=self.tf_relationships,\n        )\n\n        perturbation_pvalue_list = torch.zeros_like(perturbation_effects_list)\n        for col_index in range(perturbation_effects_list.shape[1]):\n            perturbation_pvalue_list[:, col_index] = generate_pvalues(\n                perturbation_effects_list[:, col_index]\n            )\n\n        # take absolute values\n        perturbation_effects_list = torch.abs(perturbation_effects_list)\n\n        perturbation_effects_tensor = perturbation_effects_list\n        perturbation_pvalues_tensor = perturbation_pvalue_list\n    else:\n        perturbation_effects_list = [\n            generate_perturbation_effects(\n                binding_data_tensor[:, tf_index, :].unsqueeze(1),\n                bound_mean=self.bound_mean,\n                tf_index=0,  # unused\n            )\n            for tf_index in range(sum(self.n_sample))\n        ]\n        perturbation_pvalue_list = [\n            generate_pvalues(perturbation_effects)\n            for perturbation_effects in perturbation_effects_list\n        ]\n\n        # take absolute values\n        perturbation_effects_list = [\n            torch.abs(perturbation_effects)\n            for perturbation_effects in perturbation_effects_list\n        ]\n\n        # Convert lists to tensors\n        perturbation_effects_tensor = torch.stack(perturbation_effects_list, dim=1)\n        perturbation_pvalues_tensor = torch.stack(perturbation_pvalue_list, dim=1)\n\n    # Ensure perturbation data is reshaped to match [n_genes, n_tfs]\n    # This step might need adjustment based on the actual shapes of your tensors.\n    perturbation_effects_tensor = perturbation_effects_tensor.unsqueeze(\n        -1\n    )  # Adds an extra dimension for concatenation\n    perturbation_pvalues_tensor = perturbation_pvalues_tensor.unsqueeze(\n        -1\n    )  # Adds an extra dimension for concatenation\n\n    # Concatenate along the last dimension to form a [n_genes, n_tfs, 5] tensor\n    self.final_data_tensor = torch.cat(\n        (\n            binding_data_tensor,\n            perturbation_effects_tensor,\n            perturbation_pvalues_tensor,\n        ),\n        dim=2,\n    )\n</code></pre>"},{"location":"data_loaders/synthetic_data_loader/#yeastdnnexplorer.data_loaders.synthetic_data_loader.SyntheticDataLoader.setup","title":"<code>setup(stage=None)</code>","text":"<p>This function runs after prepare_data finishes and is used to split the data into train, validation, and test sets It ensures that these datasets are of the correct dimensionality and size to be used by the model.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str | None</code> <p>The stage of the data setup (either \u2018fit\u2019 for training, \u2018validate\u2019 for validation, or \u2018test\u2019 for testing), unused for now as the model is not complicated enough to necessitate this</p> <code>None</code> Source code in <code>yeastdnnexplorer/data_loaders/synthetic_data_loader.py</code> <pre><code>def setup(self, stage: str | None = None) -&gt; None:\n    \"\"\"\n    This function runs after prepare_data finishes and is used to split the data\n    into train, validation, and test sets It ensures that these datasets are of the\n    correct dimensionality and size to be used by the model.\n\n    :param stage: The stage of the data setup (either 'fit' for training, 'validate'\n        for validation, or 'test' for testing), unused for now as the model is not\n        complicated enough to necessitate this\n    :type stage: Optional[str]\n\n    \"\"\"\n    self.binding_effect_matrix = self.final_data_tensor[:, :, 1]\n    self.perturbation_effect_matrix = self.final_data_tensor[:, :, 3]\n\n    # split into train, val, and test\n    X_train, X_temp, Y_train, Y_temp = train_test_split(\n        self.binding_effect_matrix,\n        self.perturbation_effect_matrix,\n        test_size=(self.val_size + self.test_size),\n        random_state=self.random_state,\n    )\n\n    # normalize test_size so that it is a percentage of the remaining data\n    self.test_size = self.test_size / (self.val_size + self.test_size)\n    X_val, X_test, Y_val, Y_test = train_test_split(\n        X_temp, Y_temp, test_size=self.test_size, random_state=self.random_state\n    )\n\n    # Convert to tensors\n    X_train, Y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(\n        Y_train, dtype=torch.float32\n    )\n    X_val, Y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(\n        Y_val, dtype=torch.float32\n    )\n    X_test, Y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(\n        Y_test, dtype=torch.float32\n    )\n\n    # Set our datasets\n    self.train_dataset = TensorDataset(X_train, Y_train)\n    self.val_dataset = TensorDataset(X_val, Y_val)\n    self.test_dataset = TensorDataset(X_test, Y_test)\n</code></pre>"},{"location":"data_loaders/synthetic_data_loader/#yeastdnnexplorer.data_loaders.synthetic_data_loader.SyntheticDataLoader.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Function to return the testing dataloader.</p> <p>Returns:</p> Type Description <code>DataLoader</code> <p>The testing dataloader</p> Source code in <code>yeastdnnexplorer/data_loaders/synthetic_data_loader.py</code> <pre><code>def test_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Function to return the testing dataloader.\n\n    :return: The testing dataloader\n    :rtype: DataLoader\n\n    \"\"\"\n    return DataLoader(\n        self.test_dataset,\n        batch_size=self.batch_size,\n        num_workers=15,\n        shuffle=False,\n        persistent_workers=True,\n    )\n</code></pre>"},{"location":"data_loaders/synthetic_data_loader/#yeastdnnexplorer.data_loaders.synthetic_data_loader.SyntheticDataLoader.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Function to return the training dataloader, we shuffle to avoid learning based on the order of the data.</p> <p>Returns:</p> Type Description <code>DataLoader</code> <p>The training dataloader</p> Source code in <code>yeastdnnexplorer/data_loaders/synthetic_data_loader.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Function to return the training dataloader, we shuffle to avoid learning based\n    on the order of the data.\n\n    :return: The training dataloader\n    :rtype: DataLoader\n\n    \"\"\"\n    return DataLoader(\n        self.train_dataset,\n        batch_size=self.batch_size,\n        num_workers=15,\n        shuffle=True,\n        persistent_workers=True,\n    )\n</code></pre>"},{"location":"data_loaders/synthetic_data_loader/#yeastdnnexplorer.data_loaders.synthetic_data_loader.SyntheticDataLoader.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Function to return the validation dataloader.</p> <p>Returns:</p> Type Description <code>DataLoader</code> <p>The validation dataloader</p> Source code in <code>yeastdnnexplorer/data_loaders/synthetic_data_loader.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Function to return the validation dataloader.\n\n    :return: The validation dataloader\n    :rtype: DataLoader\n\n    \"\"\"\n    return DataLoader(\n        self.val_dataset,\n        batch_size=self.batch_size,\n        num_workers=15,\n        shuffle=False,\n        persistent_workers=True,\n    )\n</code></pre>"},{"location":"interface/AbstractAPI/","title":"Developer Classes","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for creating API clients that require token authentication.</p> <p>This class provides a template for connecting to a cache for caching API responses, validating parameters against a list of valid keys, and provides an interface for CRUD operations.</p> Source code in <code>yeastdnnexplorer/interface/AbstractAPI.py</code> <pre><code>class AbstractAPI(ABC):\n    \"\"\"\n    Abstract base class for creating API clients that require token authentication.\n\n    This class provides a template for connecting to a cache for caching API responses,\n    validating parameters against a list of valid keys, and provides an interface for\n    CRUD operations.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        url: str = \"\",\n        token: str = \"\",\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize the API client.\n\n        :param url: The API endpoint URL. Defaults to the `BASE_URL`\n            environment variable.\n        :param token: The authentication token. Defaults to the `TOKEN`\n            environment variable.\n        :param valid_param_keys: A list of valid parameter keys for the API.\n        :param params: A ParamsDict object containing parameters for the API request.\n        :param cache: a Cache object for caching API responses.\n        :param kwargs: Additional keyword arguments that may be passed on to the\n            ParamsDict and Cache constructors.\n\n        \"\"\"\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self._token = token or os.getenv(\"TOKEN\", \"\")\n        self.url = url or os.getenv(\"BASE_URL\", \"\")\n        self.params = ParamsDict(\n            params=kwargs.pop(\"params\", {}),\n            valid_keys=kwargs.pop(\"valid_keys\", []),\n        )\n        self.cache = Cache(\n            maxsize=kwargs.pop(\"maxsize\", 100), ttl=kwargs.pop(\"ttl\", 300)\n        )\n\n    @property\n    def header(self) -&gt; dict[str, str]:\n        \"\"\"The HTTP authorization header.\"\"\"\n        return {\n            \"Authorization\": f\"token {self.token}\",\n            \"Content-Type\": \"application/json\",\n        }\n\n    @property\n    def url(self) -&gt; str:\n        \"\"\"The URL for the API.\"\"\"\n        return self._url  # type: ignore\n\n    @url.setter\n    def url(self, value: str) -&gt; None:\n        if not value:\n            self._url = None\n        elif hasattr(self, \"token\") and self.token:\n            # validate the URL with the new token\n            self._is_valid_url(value)\n            self._url = value\n        else:\n            self.logger.warning(\"No token provided: URL un-validated\")\n            self._url = value\n\n    @property\n    def token(self) -&gt; str:\n        \"\"\"The authentication token for the API.\"\"\"\n        return self._token\n\n    @token.setter\n    def token(self, value: str) -&gt; None:\n        self._token = value\n        # validate the URL with the new token\n        if hasattr(self, \"url\") and self.url:\n            self.logger.info(\"Validating URL with new token\")\n            self._is_valid_url(self.url)\n\n    @property\n    def cache(self) -&gt; Cache:\n        \"\"\"The cache object for caching API responses.\"\"\"\n        return self._cache\n\n    @cache.setter\n    def cache(self, value: Cache) -&gt; None:\n        self._cache = value\n\n    @property\n    def params(self) -&gt; ParamsDict:\n        \"\"\"The ParamsDict object containing parameters for the API request.\"\"\"\n        return self._params\n\n    @params.setter\n    def params(self, value: ParamsDict) -&gt; None:\n        self._params = value\n\n    def push_params(self, params: dict[str, Any]) -&gt; None:\n        \"\"\"Adds or updates parameters in the ParamsDict.\"\"\"\n        try:\n            self.params.update(params)\n        except KeyError as e:\n            self.logger.error(f\"Error updating parameters: {e}\")\n\n    def pop_params(self, keys: list[str] | None = None) -&gt; None:\n        \"\"\"Removes parameters from the ParamsDict.\"\"\"\n        if keys is None:\n            self.params.clear()\n            return\n        if keys is not None and not isinstance(keys, list):\n            keys = [keys]\n        for key in keys:\n            del self.params[key]\n\n    @abstractmethod\n    def create(self, data: dict[str, Any], **kwargs) -&gt; Any:\n        \"\"\"Placeholder for the create method.\"\"\"\n        raise NotImplementedError(\n            f\"`create()` is not implemented for {self.__class__.__name__}\"\n        )\n\n    @abstractmethod\n    def read(self, **kwargs) -&gt; Any:\n        \"\"\"Placeholder for the read method.\"\"\"\n        raise NotImplementedError(\n            f\"`read()` is not implemented for {self.__class__.__name__}\"\n        )\n\n    @abstractmethod\n    def update(self, df: pd.DataFrame, **kwargs) -&gt; Any:\n        \"\"\"Placeholder for the update method.\"\"\"\n        raise NotImplementedError(\n            f\"`update()` is not implemented for {self.__class__.__name__}\"\n        )\n\n    @abstractmethod\n    def delete(self, id: str, **kwargs) -&gt; Any:\n        \"\"\"Placeholder for the delete method.\"\"\"\n        raise NotImplementedError(\n            f\"`delete()` is not implemented for {self.__class__.__name__}\"\n        )\n\n    @abstractmethod\n    def submit(self, post_dict: dict[str, Any], **kwargs) -&gt; Any:\n        \"\"\"Placeholder for the submit method.\"\"\"\n        raise NotImplementedError(\n            f\"`submit()` is not implemented for {self.__class__.__name__}\"\n        )\n\n    @abstractmethod\n    def retrieve(\n        self, group_task_id: str, timeout: int, polling_interval: int, **kwargs\n    ) -&gt; Coroutine[Any, Any, Any]:\n        \"\"\"Placeholder for the retrieve method.\"\"\"\n        raise NotImplementedError(\n            f\"`retrieve()` is not implemented for {self.__class__.__name__}\"\n        )\n\n    def _is_valid_url(self, url: str) -&gt; None:\n        \"\"\"\n        Confirms that the URL is valid and the header authorization is appropriate.\n\n        :param url: The URL to validate.\n        :type url: str\n        :raises ValueError: If the URL is invalid or the token is not set.\n\n        \"\"\"\n        try:\n            # note that with allow_redirect=True the result can be a 300 status code\n            # which is not an error, and then another request to the redirected URL\n            response = requests.head(url, headers=self.header, allow_redirects=True)\n            if response.status_code != 200:\n                raise ValueError(\"Invalid URL or token provided. Check both.\")\n        except requests.RequestException as e:\n            raise AttributeError(f\"Error validating URL: {e}\") from e\n        except AttributeError as e:\n            self.logger.error(f\"Error validating URL: {e}\")\n\n    def _cache_get(self, key: str, default: Any = None) -&gt; Any:\n        \"\"\"\n        Get a value from the cache if configured.\n\n        :param key: The key to retrieve from the cache.\n        :type key: str\n        :param default: The default value to return if the key is not found.\n        :type default: any, optional\n        :return: The value from the cache or the default value.\n        :rtype: any\n\n        \"\"\"\n        return self.cache.get(key, default=default)\n\n    def _cache_set(self, key: str, value: Any) -&gt; None:\n        \"\"\"\n        Set a value in the cache if configured.\n\n        :param key: The key to set in the cache.\n        :type key: str\n        :param value: The value to set in the cache.\n        :type value: any\n\n        \"\"\"\n        self.cache.set(key, value)\n\n    def _cache_list(self) -&gt; list[str]:\n        \"\"\"List keys in the cache if configured.\"\"\"\n        return self.cache.list()\n\n    def _cache_delete(self, key: str) -&gt; None:\n        \"\"\"\n        Delete a key from the cache if configured.\n\n        :param key: The key to delete from the cache.\n        :type key: str\n\n        \"\"\"\n        self.cache.delete(key)\n</code></pre>"},{"location":"interface/AbstractAPI/#yeastdnnexplorer.interface.AbstractAPI.AbstractAPI.cache","title":"<code>cache: Cache</code>  <code>property</code> <code>writable</code>","text":"<p>The cache object for caching API responses.</p>"},{"location":"interface/AbstractAPI/#yeastdnnexplorer.interface.AbstractAPI.AbstractAPI.header","title":"<code>header: dict[str, str]</code>  <code>property</code>","text":"<p>The HTTP authorization header.</p>"},{"location":"interface/AbstractAPI/#yeastdnnexplorer.interface.AbstractAPI.AbstractAPI.params","title":"<code>params: ParamsDict</code>  <code>property</code> <code>writable</code>","text":"<p>The ParamsDict object containing parameters for the API request.</p>"},{"location":"interface/AbstractAPI/#yeastdnnexplorer.interface.AbstractAPI.AbstractAPI.token","title":"<code>token: str</code>  <code>property</code> <code>writable</code>","text":"<p>The authentication token for the API.</p>"},{"location":"interface/AbstractAPI/#yeastdnnexplorer.interface.AbstractAPI.AbstractAPI.url","title":"<code>url: str</code>  <code>property</code> <code>writable</code>","text":"<p>The URL for the API.</p>"},{"location":"interface/AbstractAPI/#yeastdnnexplorer.interface.AbstractAPI.AbstractAPI.__init__","title":"<code>__init__(url='', token='', **kwargs)</code>","text":"<p>Initialize the API client.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The API endpoint URL. Defaults to the <code>BASE_URL</code> environment variable.</p> <code>''</code> <code>token</code> <code>str</code> <p>The authentication token. Defaults to the <code>TOKEN</code> environment variable.</p> <code>''</code> <code>valid_param_keys</code> <p>A list of valid parameter keys for the API.</p> required <code>params</code> <p>A ParamsDict object containing parameters for the API request.</p> required <code>cache</code> <p>a Cache object for caching API responses.</p> required <code>kwargs</code> <p>Additional keyword arguments that may be passed on to the ParamsDict and Cache constructors.</p> <code>{}</code> Source code in <code>yeastdnnexplorer/interface/AbstractAPI.py</code> <pre><code>def __init__(\n    self,\n    url: str = \"\",\n    token: str = \"\",\n    **kwargs,\n):\n    \"\"\"\n    Initialize the API client.\n\n    :param url: The API endpoint URL. Defaults to the `BASE_URL`\n        environment variable.\n    :param token: The authentication token. Defaults to the `TOKEN`\n        environment variable.\n    :param valid_param_keys: A list of valid parameter keys for the API.\n    :param params: A ParamsDict object containing parameters for the API request.\n    :param cache: a Cache object for caching API responses.\n    :param kwargs: Additional keyword arguments that may be passed on to the\n        ParamsDict and Cache constructors.\n\n    \"\"\"\n    self.logger = logging.getLogger(self.__class__.__name__)\n    self._token = token or os.getenv(\"TOKEN\", \"\")\n    self.url = url or os.getenv(\"BASE_URL\", \"\")\n    self.params = ParamsDict(\n        params=kwargs.pop(\"params\", {}),\n        valid_keys=kwargs.pop(\"valid_keys\", []),\n    )\n    self.cache = Cache(\n        maxsize=kwargs.pop(\"maxsize\", 100), ttl=kwargs.pop(\"ttl\", 300)\n    )\n</code></pre>"},{"location":"interface/AbstractAPI/#yeastdnnexplorer.interface.AbstractAPI.AbstractAPI.create","title":"<code>create(data, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Placeholder for the create method.</p> Source code in <code>yeastdnnexplorer/interface/AbstractAPI.py</code> <pre><code>@abstractmethod\ndef create(self, data: dict[str, Any], **kwargs) -&gt; Any:\n    \"\"\"Placeholder for the create method.\"\"\"\n    raise NotImplementedError(\n        f\"`create()` is not implemented for {self.__class__.__name__}\"\n    )\n</code></pre>"},{"location":"interface/AbstractAPI/#yeastdnnexplorer.interface.AbstractAPI.AbstractAPI.delete","title":"<code>delete(id, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Placeholder for the delete method.</p> Source code in <code>yeastdnnexplorer/interface/AbstractAPI.py</code> <pre><code>@abstractmethod\ndef delete(self, id: str, **kwargs) -&gt; Any:\n    \"\"\"Placeholder for the delete method.\"\"\"\n    raise NotImplementedError(\n        f\"`delete()` is not implemented for {self.__class__.__name__}\"\n    )\n</code></pre>"},{"location":"interface/AbstractAPI/#yeastdnnexplorer.interface.AbstractAPI.AbstractAPI.pop_params","title":"<code>pop_params(keys=None)</code>","text":"<p>Removes parameters from the ParamsDict.</p> Source code in <code>yeastdnnexplorer/interface/AbstractAPI.py</code> <pre><code>def pop_params(self, keys: list[str] | None = None) -&gt; None:\n    \"\"\"Removes parameters from the ParamsDict.\"\"\"\n    if keys is None:\n        self.params.clear()\n        return\n    if keys is not None and not isinstance(keys, list):\n        keys = [keys]\n    for key in keys:\n        del self.params[key]\n</code></pre>"},{"location":"interface/AbstractAPI/#yeastdnnexplorer.interface.AbstractAPI.AbstractAPI.push_params","title":"<code>push_params(params)</code>","text":"<p>Adds or updates parameters in the ParamsDict.</p> Source code in <code>yeastdnnexplorer/interface/AbstractAPI.py</code> <pre><code>def push_params(self, params: dict[str, Any]) -&gt; None:\n    \"\"\"Adds or updates parameters in the ParamsDict.\"\"\"\n    try:\n        self.params.update(params)\n    except KeyError as e:\n        self.logger.error(f\"Error updating parameters: {e}\")\n</code></pre>"},{"location":"interface/AbstractAPI/#yeastdnnexplorer.interface.AbstractAPI.AbstractAPI.read","title":"<code>read(**kwargs)</code>  <code>abstractmethod</code>","text":"<p>Placeholder for the read method.</p> Source code in <code>yeastdnnexplorer/interface/AbstractAPI.py</code> <pre><code>@abstractmethod\ndef read(self, **kwargs) -&gt; Any:\n    \"\"\"Placeholder for the read method.\"\"\"\n    raise NotImplementedError(\n        f\"`read()` is not implemented for {self.__class__.__name__}\"\n    )\n</code></pre>"},{"location":"interface/AbstractAPI/#yeastdnnexplorer.interface.AbstractAPI.AbstractAPI.retrieve","title":"<code>retrieve(group_task_id, timeout, polling_interval, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Placeholder for the retrieve method.</p> Source code in <code>yeastdnnexplorer/interface/AbstractAPI.py</code> <pre><code>@abstractmethod\ndef retrieve(\n    self, group_task_id: str, timeout: int, polling_interval: int, **kwargs\n) -&gt; Coroutine[Any, Any, Any]:\n    \"\"\"Placeholder for the retrieve method.\"\"\"\n    raise NotImplementedError(\n        f\"`retrieve()` is not implemented for {self.__class__.__name__}\"\n    )\n</code></pre>"},{"location":"interface/AbstractAPI/#yeastdnnexplorer.interface.AbstractAPI.AbstractAPI.submit","title":"<code>submit(post_dict, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Placeholder for the submit method.</p> Source code in <code>yeastdnnexplorer/interface/AbstractAPI.py</code> <pre><code>@abstractmethod\ndef submit(self, post_dict: dict[str, Any], **kwargs) -&gt; Any:\n    \"\"\"Placeholder for the submit method.\"\"\"\n    raise NotImplementedError(\n        f\"`submit()` is not implemented for {self.__class__.__name__}\"\n    )\n</code></pre>"},{"location":"interface/AbstractAPI/#yeastdnnexplorer.interface.AbstractAPI.AbstractAPI.update","title":"<code>update(df, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Placeholder for the update method.</p> Source code in <code>yeastdnnexplorer/interface/AbstractAPI.py</code> <pre><code>@abstractmethod\ndef update(self, df: pd.DataFrame, **kwargs) -&gt; Any:\n    \"\"\"Placeholder for the update method.\"\"\"\n    raise NotImplementedError(\n        f\"`update()` is not implemented for {self.__class__.__name__}\"\n    )\n</code></pre>"},{"location":"interface/AbstractRecordsAndFilesAPI/","title":"AbstractRecordsAndFilesAPI","text":"<p>             Bases: <code>AbstractAPI</code></p> <p>Abstract class to interact with both the records and the data stored in the <code>file</code> field.</p> <p>The return for this class must be records, against the <code>/export</code> endpoint when <code>retrieve_files</code> is False. When <code>retrieve_files</code> is True, the cache should be checked first. If the file doesn\u2019t exist there, it should be retrieved from the database against the <code>/record_table_and_files</code> endpoint. The file should be a tarball with the metadata.csv and the file associated with the record, where the file is named according to the <code>id</code> field in metadata.csv. Data files should be <code>.csv.gz</code>.</p> Source code in <code>yeastdnnexplorer/interface/AbstractRecordsAndFilesAPI.py</code> <pre><code>class AbstractRecordsAndFilesAPI(AbstractAPI):\n    \"\"\"\n    Abstract class to interact with both the records and the data stored in the `file`\n    field.\n\n    The return for this class must be records, against the `/export`\n    endpoint when `retrieve_files` is False. When `retrieve_files` is True, the cache\n    should be checked first. If the file doesn't exist there, it should be retrieved\n    from the database against the `/record_table_and_files` endpoint. The file should\n    be a tarball with the metadata.csv and the file associated with the record,\n    where the file is named according to the `id` field in metadata.csv. Data files\n    should be `.csv.gz`.\n\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the AbstractRecordsAndFilesAPI object. This will serve as an\n        interface to an endpoint that can serve both records and files, and cache the\n        file/retrieve from the cache if it exists.\n\n        :param kwargs: parameters to pass to AbstractAPI.\n\n        \"\"\"\n        self.export_url_suffix = kwargs.pop(\"export_url_suffix\", \"export\")\n        self.export_files_url_suffix = kwargs.pop(\n            \"export_files_url_suffix\", \"record_table_and_files\"\n        )\n        super().__init__(**kwargs)\n\n    @property\n    def export_url_suffix(self) -&gt; str:\n        \"\"\"The URL suffix for exporting records.\"\"\"\n        return self._export_url_suffix\n\n    @export_url_suffix.setter\n    def export_url_suffix(self, value: str) -&gt; None:\n        self._export_url_suffix = value\n\n    @property\n    def export_files_url_suffix(self) -&gt; str:\n        \"\"\"The URL suffix for exporting files.\"\"\"\n        return self._export_files_url_suffix\n\n    @export_files_url_suffix.setter\n    def export_files_url_suffix(self, value: str) -&gt; None:\n        self._export_files_url_suffix = value\n\n    def _detect_delimiter(self, file_path: str, sample_size: int = 1024) -&gt; str:\n        \"\"\"\n        Detect the delimiter of a CSV file.\n\n        :param file_path: The path to the CSV file.\n        :type file_path: str\n        :param sample_size: The number of bytes to read from the file to detect the\n            delimiter. Defaults to 1024.\n        :type sample_size: int\n        :return: The delimiter of the CSV file.\n        :rtype: str\n        :raises FileNotFoundError: If the file does not exist.\n        :raises gzip.BadGzipFile: If the file is not a valid gzip file.\n        :raises _csv.Error: If the CSV sniffer cannot determine the delimiter.\n\n        \"\"\"\n        try:\n            # by default, open() uses newline=False, which opens the file\n            # in universal newline mode and translates all new line characters\n            # to '\\n'\n            file = (\n                gzip.open(file_path, \"rt\")\n                if file_path.endswith(\".gz\")\n                else open(file_path)\n            )\n        except FileNotFoundError as exc:\n            raise FileNotFoundError(f\"File {file_path} not found.\") from exc\n\n        sample = file.read(sample_size)\n\n        # In order to avoid errors in the csv sniffer, attempt to find the\n        # last newline character in the string\n        last_newline_index = sample.rfind(\"\\n\")\n        # if a newline character is found, trim the sample to the last newline\n        if last_newline_index != -1:\n            # Trim to the last complete line\n            sample = sample[:last_newline_index]\n\n        sniffer = csv.Sniffer()\n        dialect = sniffer.sniff(sample)\n        delimiter = dialect.delimiter\n\n        file.close()\n\n        return delimiter\n\n    async def read(\n        self,\n        callback: Callable[\n            [pd.DataFrame, dict[str, Any] | None, Any], Any\n        ] = lambda metadata, data, cache, **kwargs: (\n            {\"metadata\": metadata, \"data\": data}\n        ),\n        retrieve_files: bool = False,\n        **kwargs,\n    ) -&gt; Any:\n        \"\"\"\n        Retrieve data from the endpoint according to the `retrieve_files` parameter. If\n        `retrieve_files` is False, the records will be returned as a dataframe. If\n        `retrieve_files` is True, the files associated with the records will be\n        retrieved either from the local cache or from the database. Note that a user can\n        select which effect_colname and pvalue_colname is used for a genomicfile (see\n        database documentation for more details). If one or both of those are present in\n        the params, and retrieve_file is true, then that column name is added to the\n        cache_key. Eg if record 1 is being retrieved from mcisaac data with\n        effect_colname \"log2_raio\", then the cache_key for that data will be\n        \"1_log2_ratio\". The default effect colname, which is set by the database, will\n        be stored with only the record id as the cache_key.\n\n        :param callback: The function to call with the metadata. Signature must\n            include `metadata`, `data`, and `cache`.\n        :type callback: Callable[[pd.DataFrame, dict[str, Any] | None, Any], Any]\n        :param retrieve_files: Boolean. Whether to retrieve the files associated with\n            the records. Defaults to False.\n        :type retrieve_files: bool\n\n        :return: The result of the callback function.\n        :rtype: Any\n\n        :raises ValueError: If the callback function does not have the correct\n            signature.\n        :raises aiohttp.ClientError: If there is an error in the GET request.\n        :raises pd.errors.ParserError: If there is an error reading the request\n\n        \"\"\"\n        if not callable(callback) or {\"metadata\", \"data\", \"cache\"} - set(\n            callback.__code__.co_varnames\n        ):\n            raise ValueError(\n                \"The callback must be a callable function with `metadata`, `data`, \",\n                \"and `cache` as parameters.\",\n            )\n\n        export_url = f\"{self.url.rstrip('/')}/{self.export_url_suffix}\"\n        self.logger.debug(\"read() export_url: %s\", export_url)\n\n        async with aiohttp.ClientSession() as session:\n            try:\n                async with session.get(\n                    export_url, headers=self.header, params=self.params\n                ) as response:\n                    response.raise_for_status()\n                    content = await response.content.read()\n                    with gzip.GzipFile(fileobj=BytesIO(content)) as f:\n                        records_df = pd.read_csv(f)\n\n                    if not retrieve_files:\n                        return callback(records_df, None, self.cache, **kwargs)\n                    else:\n                        data_list = await self._retrieve_files(session, records_df)\n                        return callback(\n                            records_df,\n                            data_list,\n                            self.cache,\n                            **kwargs,\n                        )\n\n            except aiohttp.ClientError as e:\n                self.logger.error(f\"Error in GET request: {e}\")\n                raise\n            except pd.errors.ParserError as e:\n                self.logger.error(f\"Error reading request content: {e}\")\n                raise\n\n    async def _retrieve_files(\n        self, session: aiohttp.ClientSession, records_df: pd.DataFrame\n    ) -&gt; dict[str, pd.DataFrame]:\n        \"\"\"\n        Retrieve files associated with the records either from the local cache or from\n        the database.\n\n        :param session: The aiohttp ClientSession.\n        :type session: aiohttp.ClientSession\n        :param records_df: The DataFrame containing the records.\n        :type records_df: pd.DataFrame\n        :return: A dictionary where the keys are record IDs and the values are\n            DataFrames of the associated files.\n        :rtype: dict[str, pd.DataFrame]\n\n        \"\"\"\n        data_list = {}\n        for record_id in records_df[\"id\"]:\n            data_list[str(record_id)] = await self._retrieve_file(session, record_id)\n        return data_list\n\n    async def _retrieve_file(\n        self, session: aiohttp.ClientSession, record_id: int\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Retrieve a file associated with a record either from the local cache or from the\n        database.\n\n        :param session: The aiohttp ClientSession.\n        :type session: aiohttp.ClientSession\n        :param record_id: The ID of the record.\n        :type record_id: int\n        :return: A DataFrame containing the file's data.\n        :rtype: pd.DataFrame\n        :raises FileNotFoundError: If the file is not found in the tar archive.\n        :raises ValueError: If the delimiter is not supported.\n\n        \"\"\"\n        export_files_url = f\"{self.url.rstrip('/')}/{self.export_files_url_suffix}\"\n        self.logger.debug(\"_retrieve_file() export_url: %s\", export_files_url)\n\n        # set key for local cache\n        cache_key = str(record_id)\n        if \"effect_colname\" in self.params:\n            cache_key += f\"_{self.params['effect_colname']}\"\n        if \"pvalue_colname\" in self.params:\n            cache_key += f\"_{self.params['pvalue_colname']}\"\n        cached_data = self._cache_get(cache_key)\n        if cached_data is not None:\n            self.logger.info(f\"cache_key {cache_key} retrieved from cache.\")\n            return pd.read_json(BytesIO(cached_data.encode()))\n        else:\n            self.logger.debug(f\"cache_key {cache_key} not found in cache.\")\n\n        try:\n            header = self.header.copy()\n            header[\"Content-Type\"] = \"application/gzip\"\n            retrieve_files_params = self.params.copy()\n            retrieve_files_params.update({\"id\": record_id})\n            async with session.get(\n                export_files_url,\n                headers=header,\n                params=retrieve_files_params,\n                timeout=120,\n            ) as response:\n                response.raise_for_status()\n                tar_data = await response.read()\n\n            # Create a temporary file for the tarball\n            tar_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".tar.gz\")\n            try:\n                tar_file.write(tar_data)\n                tar_file.flush()\n                tar_file.seek(0)\n\n                # Create a temporary directory for extraction\n                with tempfile.TemporaryDirectory() as extract_dir:\n                    # Open the tar file and log its contents\n                    with tarfile.open(fileobj=tar_file, mode=\"r:gz\") as tar:\n                        tar_members = tar.getmembers()\n                        self.logger.debug(\n                            f\"Tar file contains: \"\n                            f\"{[member.name for member in tar_members]}\",\n                        )\n\n                        # Find the specific file to extract\n                        csv_filename = f\"{record_id}.csv.gz\"\n                        member = next(\n                            (m for m in tar_members if m.name == csv_filename), None\n                        )\n                        if member is None:\n                            raise FileNotFoundError(\n                                f\"{csv_filename} not found in tar archive\"\n                            )\n\n                        # Extract only the specific member\n                        tar.extract(member, path=extract_dir)\n\n                    # Read the extracted CSV file\n                    csv_path = os.path.join(extract_dir, csv_filename)\n                    self.logger.debug(f\"Extracted file: {csv_path}\")\n\n                    delimiter = self._detect_delimiter(csv_path)\n\n                    # raise an error if the delimiter is not a \",\" or a \"\\t\"\n                    if delimiter not in [\",\", \"\\t\"]:\n                        raise ValueError(\n                            f\"Delimiter {delimiter} is not supported. \"\n                            \"Supported delimiters are ',' and '\\\\t'.\"\n                        )\n\n                    df = pd.read_csv(csv_path, delimiter=delimiter)\n\n                    # Store the data in the cache\n                    self.logger.debug(f\"Storing {cache_key} in cache.\")\n                    self._cache_set(cache_key, df.to_json())\n            finally:\n                os.unlink(tar_file.name)\n\n            return df\n        except Exception as e:\n            self.logger.error(f\"Error retrieving file for cache_key {cache_key}: {e}\")\n            raise\n</code></pre>"},{"location":"interface/AbstractRecordsAndFilesAPI/#yeastdnnexplorer.interface.AbstractRecordsAndFilesAPI.AbstractRecordsAndFilesAPI.export_files_url_suffix","title":"<code>export_files_url_suffix: str</code>  <code>property</code> <code>writable</code>","text":"<p>The URL suffix for exporting files.</p>"},{"location":"interface/AbstractRecordsAndFilesAPI/#yeastdnnexplorer.interface.AbstractRecordsAndFilesAPI.AbstractRecordsAndFilesAPI.export_url_suffix","title":"<code>export_url_suffix: str</code>  <code>property</code> <code>writable</code>","text":"<p>The URL suffix for exporting records.</p>"},{"location":"interface/AbstractRecordsAndFilesAPI/#yeastdnnexplorer.interface.AbstractRecordsAndFilesAPI.AbstractRecordsAndFilesAPI.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the AbstractRecordsAndFilesAPI object. This will serve as an interface to an endpoint that can serve both records and files, and cache the file/retrieve from the cache if it exists.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>parameters to pass to AbstractAPI.</p> <code>{}</code> Source code in <code>yeastdnnexplorer/interface/AbstractRecordsAndFilesAPI.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the AbstractRecordsAndFilesAPI object. This will serve as an\n    interface to an endpoint that can serve both records and files, and cache the\n    file/retrieve from the cache if it exists.\n\n    :param kwargs: parameters to pass to AbstractAPI.\n\n    \"\"\"\n    self.export_url_suffix = kwargs.pop(\"export_url_suffix\", \"export\")\n    self.export_files_url_suffix = kwargs.pop(\n        \"export_files_url_suffix\", \"record_table_and_files\"\n    )\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"interface/AbstractRecordsAndFilesAPI/#yeastdnnexplorer.interface.AbstractRecordsAndFilesAPI.AbstractRecordsAndFilesAPI.read","title":"<code>read(callback=lambda , , : {'metadata': metadata, 'data': data}, retrieve_files=False, **kwargs)</code>  <code>async</code>","text":"<p>Retrieve data from the endpoint according to the <code>retrieve_files</code> parameter. If <code>retrieve_files</code> is False, the records will be returned as a dataframe. If <code>retrieve_files</code> is True, the files associated with the records will be retrieved either from the local cache or from the database. Note that a user can select which effect_colname and pvalue_colname is used for a genomicfile (see database documentation for more details). If one or both of those are present in the params, and retrieve_file is true, then that column name is added to the cache_key. Eg if record 1 is being retrieved from mcisaac data with effect_colname \u201clog2_raio\u201d, then the cache_key for that data will be \u201c1_log2_ratio\u201d. The default effect colname, which is set by the database, will be stored with only the record id as the cache_key.</p> <p>Parameters:</p> Name Type Description Default <code>callback</code> <code>Callable[[DataFrame, dict[str, Any] | None, Any], Any]</code> <p>The function to call with the metadata. Signature must include <code>metadata</code>, <code>data</code>, and <code>cache</code>.</p> <code>lambda , , : {'metadata': metadata, 'data': data}</code> <code>retrieve_files</code> <code>bool</code> <p>Boolean. Whether to retrieve the files associated with the records. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The result of the callback function.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the callback function does not have the correct signature.</p> <code>aiohttp.ClientError</code> <p>If there is an error in the GET request.</p> <code>pd.errors.ParserError</code> <p>If there is an error reading the request</p> Source code in <code>yeastdnnexplorer/interface/AbstractRecordsAndFilesAPI.py</code> <pre><code>async def read(\n    self,\n    callback: Callable[\n        [pd.DataFrame, dict[str, Any] | None, Any], Any\n    ] = lambda metadata, data, cache, **kwargs: (\n        {\"metadata\": metadata, \"data\": data}\n    ),\n    retrieve_files: bool = False,\n    **kwargs,\n) -&gt; Any:\n    \"\"\"\n    Retrieve data from the endpoint according to the `retrieve_files` parameter. If\n    `retrieve_files` is False, the records will be returned as a dataframe. If\n    `retrieve_files` is True, the files associated with the records will be\n    retrieved either from the local cache or from the database. Note that a user can\n    select which effect_colname and pvalue_colname is used for a genomicfile (see\n    database documentation for more details). If one or both of those are present in\n    the params, and retrieve_file is true, then that column name is added to the\n    cache_key. Eg if record 1 is being retrieved from mcisaac data with\n    effect_colname \"log2_raio\", then the cache_key for that data will be\n    \"1_log2_ratio\". The default effect colname, which is set by the database, will\n    be stored with only the record id as the cache_key.\n\n    :param callback: The function to call with the metadata. Signature must\n        include `metadata`, `data`, and `cache`.\n    :type callback: Callable[[pd.DataFrame, dict[str, Any] | None, Any], Any]\n    :param retrieve_files: Boolean. Whether to retrieve the files associated with\n        the records. Defaults to False.\n    :type retrieve_files: bool\n\n    :return: The result of the callback function.\n    :rtype: Any\n\n    :raises ValueError: If the callback function does not have the correct\n        signature.\n    :raises aiohttp.ClientError: If there is an error in the GET request.\n    :raises pd.errors.ParserError: If there is an error reading the request\n\n    \"\"\"\n    if not callable(callback) or {\"metadata\", \"data\", \"cache\"} - set(\n        callback.__code__.co_varnames\n    ):\n        raise ValueError(\n            \"The callback must be a callable function with `metadata`, `data`, \",\n            \"and `cache` as parameters.\",\n        )\n\n    export_url = f\"{self.url.rstrip('/')}/{self.export_url_suffix}\"\n    self.logger.debug(\"read() export_url: %s\", export_url)\n\n    async with aiohttp.ClientSession() as session:\n        try:\n            async with session.get(\n                export_url, headers=self.header, params=self.params\n            ) as response:\n                response.raise_for_status()\n                content = await response.content.read()\n                with gzip.GzipFile(fileobj=BytesIO(content)) as f:\n                    records_df = pd.read_csv(f)\n\n                if not retrieve_files:\n                    return callback(records_df, None, self.cache, **kwargs)\n                else:\n                    data_list = await self._retrieve_files(session, records_df)\n                    return callback(\n                        records_df,\n                        data_list,\n                        self.cache,\n                        **kwargs,\n                    )\n\n        except aiohttp.ClientError as e:\n            self.logger.error(f\"Error in GET request: {e}\")\n            raise\n        except pd.errors.ParserError as e:\n            self.logger.error(f\"Error reading request content: {e}\")\n            raise\n</code></pre>"},{"location":"interface/AbstractRecordsOnlyAPI/","title":"AbstractRecordsOnlyAPI","text":"<p>             Bases: <code>AbstractAPI</code></p> <p>Abstract class for CRUD operations on records-only (no file storage) endpoints.</p> Source code in <code>yeastdnnexplorer/interface/AbstractRecordsOnlyAPI.py</code> <pre><code>class AbstractRecordsOnlyAPI(AbstractAPI):\n    \"\"\"Abstract class for CRUD operations on records-only (no file storage)\n    endpoints.\"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the RecordsOnlyAPI object.\n\n        :param kwargs: Additional parameters to pass to AbstractAPI.\n\n        \"\"\"\n        self.logger = logging.getLogger(__name__)\n        super().__init__(**kwargs)\n\n    async def read(\n        self,\n        callback: Callable[\n            [pd.DataFrame, dict[str, Any] | None, Any], Any\n        ] = lambda metadata, data, cache, **kwargs: {\n            \"metadata\": metadata,\n            \"data\": data,\n        },\n        export_url_suffix=\"export\",\n        **kwargs,\n    ) -&gt; Any:\n        \"\"\"\n        Retrieve data from the endpoint. The data will be returned as a dataframe. The\n        callback function must take metadata, data, and cache as parameters.\n\n        :param callback: The function to call with the data. Signature must\n            include `metadata`, `data`, and `cache` as parameters.\n        :param export_url_suffix: The URL suffix for the export endpoint. This will\n            return a response object with a csv file.\n        :param kwargs: Additional arguments to pass to the callback function.\n        :return: The result of the callback function.\n\n        \"\"\"\n        if not callable(callback) or {\"metadata\", \"data\", \"cache\"} - set(\n            callback.__code__.co_varnames\n        ):\n            raise ValueError(\n                \"The callback must be a callable function with `metadata`,\",\n                \"`data`, and `cache` as parameters.\",\n            )\n\n        export_url = f\"{self.url.rstrip('/')}/{export_url_suffix}\"\n        self.logger.debug(\"read() export_url: %s\", export_url)\n\n        async with aiohttp.ClientSession() as session:\n            try:\n                # note that the url and the export suffix are joined such that\n                # the url is stripped of any trailing slashes and the export suffix is\n                # added without a leading slash\n                async with session.get(\n                    export_url,\n                    headers=self.header,\n                    params=self.params,\n                ) as response:\n                    response.raise_for_status()\n                    content = await response.content.read()\n                    with gzip.GzipFile(fileobj=BytesIO(content)) as f:\n                        records_df = pd.read_csv(f)\n                    return callback(records_df, None, self.cache, **kwargs)\n            except aiohttp.ClientError as e:\n                self.logger.error(f\"Error in GET request: {e}\")\n                raise\n            except pd.errors.ParserError as e:\n                self.logger.error(f\"Error reading request content: {e}\")\n                raise\n</code></pre>"},{"location":"interface/AbstractRecordsOnlyAPI/#yeastdnnexplorer.interface.AbstractRecordsOnlyAPI.AbstractRecordsOnlyAPI.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the RecordsOnlyAPI object.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>Additional parameters to pass to AbstractAPI.</p> <code>{}</code> Source code in <code>yeastdnnexplorer/interface/AbstractRecordsOnlyAPI.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the RecordsOnlyAPI object.\n\n    :param kwargs: Additional parameters to pass to AbstractAPI.\n\n    \"\"\"\n    self.logger = logging.getLogger(__name__)\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"interface/AbstractRecordsOnlyAPI/#yeastdnnexplorer.interface.AbstractRecordsOnlyAPI.AbstractRecordsOnlyAPI.read","title":"<code>read(callback=lambda , , : {'metadata': metadata, 'data': data}, export_url_suffix='export', **kwargs)</code>  <code>async</code>","text":"<p>Retrieve data from the endpoint. The data will be returned as a dataframe. The callback function must take metadata, data, and cache as parameters.</p> <p>Parameters:</p> Name Type Description Default <code>callback</code> <code>Callable[[DataFrame, dict[str, Any] | None, Any], Any]</code> <p>The function to call with the data. Signature must include <code>metadata</code>, <code>data</code>, and <code>cache</code> as parameters.</p> <code>lambda , , : {'metadata': metadata, 'data': data}</code> <code>export_url_suffix</code> <p>The URL suffix for the export endpoint. This will return a response object with a csv file.</p> <code>'export'</code> <code>kwargs</code> <p>Additional arguments to pass to the callback function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The result of the callback function.</p> Source code in <code>yeastdnnexplorer/interface/AbstractRecordsOnlyAPI.py</code> <pre><code>async def read(\n    self,\n    callback: Callable[\n        [pd.DataFrame, dict[str, Any] | None, Any], Any\n    ] = lambda metadata, data, cache, **kwargs: {\n        \"metadata\": metadata,\n        \"data\": data,\n    },\n    export_url_suffix=\"export\",\n    **kwargs,\n) -&gt; Any:\n    \"\"\"\n    Retrieve data from the endpoint. The data will be returned as a dataframe. The\n    callback function must take metadata, data, and cache as parameters.\n\n    :param callback: The function to call with the data. Signature must\n        include `metadata`, `data`, and `cache` as parameters.\n    :param export_url_suffix: The URL suffix for the export endpoint. This will\n        return a response object with a csv file.\n    :param kwargs: Additional arguments to pass to the callback function.\n    :return: The result of the callback function.\n\n    \"\"\"\n    if not callable(callback) or {\"metadata\", \"data\", \"cache\"} - set(\n        callback.__code__.co_varnames\n    ):\n        raise ValueError(\n            \"The callback must be a callable function with `metadata`,\",\n            \"`data`, and `cache` as parameters.\",\n        )\n\n    export_url = f\"{self.url.rstrip('/')}/{export_url_suffix}\"\n    self.logger.debug(\"read() export_url: %s\", export_url)\n\n    async with aiohttp.ClientSession() as session:\n        try:\n            # note that the url and the export suffix are joined such that\n            # the url is stripped of any trailing slashes and the export suffix is\n            # added without a leading slash\n            async with session.get(\n                export_url,\n                headers=self.header,\n                params=self.params,\n            ) as response:\n                response.raise_for_status()\n                content = await response.content.read()\n                with gzip.GzipFile(fileobj=BytesIO(content)) as f:\n                    records_df = pd.read_csv(f)\n                return callback(records_df, None, self.cache, **kwargs)\n        except aiohttp.ClientError as e:\n            self.logger.error(f\"Error in GET request: {e}\")\n            raise\n        except pd.errors.ParserError as e:\n            self.logger.error(f\"Error reading request content: {e}\")\n            raise\n</code></pre>"},{"location":"interface/BindingAPI/","title":"Records and Files Classes","text":"<p>             Bases: <code>AbstractRecordsAndFilesAPI</code></p> <p>Class to interact with the BindingAPI endpoint.</p> Source code in <code>yeastdnnexplorer/interface/BindingAPI.py</code> <pre><code>class BindingAPI(AbstractRecordsAndFilesAPI):\n    \"\"\"Class to interact with the BindingAPI endpoint.\"\"\"\n\n    def __init__(self, **kwargs) -&gt; None:\n        \"\"\"\n        Initialize the BindingAPI object.\n\n        :param kwargs: parameters to pass through AbstractRecordsAndFilesAPI to\n            AbstractAPI.\n\n        \"\"\"\n        valid_param_keys = kwargs.pop(\n            \"valid_param_keys\",\n            [\n                \"id\",\n                \"regulator\",\n                \"regulator_locus_tag\",\n                \"regulator_symbol\",\n                \"batch\",\n                \"replicate\",\n                \"source\",\n                \"source_orig_id\",\n                \"strain\",\n                \"condition\",\n                \"lab\",\n                \"assay\",\n                \"workflow\",\n                \"data_usable\",\n            ],\n        )\n\n        url = kwargs.pop(\"url\", os.getenv(\"BINDING_URL\", None))\n\n        super().__init__(url=url, valid_keys=valid_param_keys, **kwargs)\n\n    def create(self, data: dict[str, Any], **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The BindingAPI does not support create.\")\n\n    def update(self, df: pd.DataFrame, **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The BindingAPI does not support update.\")\n\n    def delete(self, id: str, **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The BindingAPI does not support delete.\")\n\n    def submit(self, post_dict: dict[str, Any], **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The BindingAPI does not support submit.\")\n\n    def retrieve(\n        self, group_task_id: str, timeout: int, polling_interval: int, **kwargs\n    ) -&gt; Any:\n        raise NotImplementedError(\"The BindingAPI does not support retrieve.\")\n</code></pre>"},{"location":"interface/BindingAPI/#yeastdnnexplorer.interface.BindingAPI.BindingAPI.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the BindingAPI object.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>parameters to pass through AbstractRecordsAndFilesAPI to AbstractAPI.</p> <code>{}</code> Source code in <code>yeastdnnexplorer/interface/BindingAPI.py</code> <pre><code>def __init__(self, **kwargs) -&gt; None:\n    \"\"\"\n    Initialize the BindingAPI object.\n\n    :param kwargs: parameters to pass through AbstractRecordsAndFilesAPI to\n        AbstractAPI.\n\n    \"\"\"\n    valid_param_keys = kwargs.pop(\n        \"valid_param_keys\",\n        [\n            \"id\",\n            \"regulator\",\n            \"regulator_locus_tag\",\n            \"regulator_symbol\",\n            \"batch\",\n            \"replicate\",\n            \"source\",\n            \"source_orig_id\",\n            \"strain\",\n            \"condition\",\n            \"lab\",\n            \"assay\",\n            \"workflow\",\n            \"data_usable\",\n        ],\n    )\n\n    url = kwargs.pop(\"url\", os.getenv(\"BINDING_URL\", None))\n\n    super().__init__(url=url, valid_keys=valid_param_keys, **kwargs)\n</code></pre>"},{"location":"interface/BindingManualQCAPI/","title":"Records Only Classes","text":"<p>             Bases: <code>AbstractRecordsOnlyAPI</code></p> <p>A class to interact with the BindingManualQCAPI endpoint.</p> Source code in <code>yeastdnnexplorer/interface/BindingManualQCAPI.py</code> <pre><code>class BindingManualQCAPI(AbstractRecordsOnlyAPI):\n    \"\"\"A class to interact with the BindingManualQCAPI endpoint.\"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the BindingManualQCAPI object.\n\n        :param kwargs: parameters to pass to AbstractAPI via AbstractRecordsOnlyAPI.\n\n        \"\"\"\n        valid_param_keys = kwargs.pop(\n            \"valid_param_keys\",\n            [\n                \"id\",\n                \"binding\",\n                \"best_datatype\",\n                \"data_usable\",\n                \"passing_replicate\",\n                \"rank_recall\",\n                \"regulator\",\n                \"regulator_locus_tag\",\n                \"regulator_symbol\",\n                \"batch\",\n                \"source\",\n            ],\n        )\n\n        url = kwargs.pop(\"url\", os.getenv(\"BINDINGMANUALQC_URL\", None))\n        if not url:\n            raise AttributeError(\n                \"url must be provided or the environmental variable \",\n                \"`BINDINGMANUALQC_URL` must be set\",\n            )\n\n        self.bulk_update_url_suffix = kwargs.pop(\n            \"bulk_update_url_suffix\", \"bulk-update\"\n        )\n\n        super().__init__(url=url, valid_param_keys=valid_param_keys, **kwargs)\n\n    @property\n    def bulk_update_url_suffix(self) -&gt; str:\n        \"\"\"The URL suffix for updating multiple records in the same request.\"\"\"\n        return self._bulk_update_url_suffix\n\n    @bulk_update_url_suffix.setter\n    def bulk_update_url_suffix(self, value: str) -&gt; None:\n        self._bulk_update_url_suffix = value\n\n    def update(self, df: pd.DataFrame, **kwargs: Any) -&gt; requests.Response:\n        \"\"\"\n        Update the records in the database.\n\n        :param df: The DataFrame containing the records to update.\n        :type df: pd.DataFrame\n        :param kwargs: Additional fields to include in the payload.\n        :type kwargs: Any\n        :return: The response from the POST request.\n        :rtype: requests.Response\n        :raises requests.RequestException: If the request fails.\n\n        \"\"\"\n        bulk_update_url = (\n            f\"{self.url.rstrip('/')}/{self.bulk_update_url_suffix.rstrip('/')}/\"\n        )\n\n        self.logger.debug(\"bulk_update_url: %s\", bulk_update_url)\n\n        # Include additional fields in the payload if provided\n        payload = {\"data\": df.to_dict(orient=\"records\")}\n        payload.update(kwargs)\n\n        try:\n            response = requests.post(\n                bulk_update_url,\n                headers=self.header,\n                json=payload,\n            )\n            response.raise_for_status()\n            return response\n        except requests.RequestException as e:\n            self.logger.error(f\"Error in POST request: {e}\")\n            raise\n\n    def create(self, data: dict[str, Any], **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The BindingManualQCAPI does not support create.\")\n\n    def delete(self, id: str, **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The BindingManualQCAPI does not support delete.\")\n\n    def submit(self, post_dict: dict[str, Any], **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The BindingManualQCAPI does not support submit.\")\n\n    def retrieve(\n        self, group_task_id: str, timeout: int, polling_interval: int, **kwargs\n    ) -&gt; Any:\n        raise NotImplementedError(\"The BindingManualQCAPI does not support retrieve.\")\n</code></pre>"},{"location":"interface/BindingManualQCAPI/#yeastdnnexplorer.interface.BindingManualQCAPI.BindingManualQCAPI.bulk_update_url_suffix","title":"<code>bulk_update_url_suffix: str</code>  <code>property</code> <code>writable</code>","text":"<p>The URL suffix for updating multiple records in the same request.</p>"},{"location":"interface/BindingManualQCAPI/#yeastdnnexplorer.interface.BindingManualQCAPI.BindingManualQCAPI.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the BindingManualQCAPI object.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>parameters to pass to AbstractAPI via AbstractRecordsOnlyAPI.</p> <code>{}</code> Source code in <code>yeastdnnexplorer/interface/BindingManualQCAPI.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the BindingManualQCAPI object.\n\n    :param kwargs: parameters to pass to AbstractAPI via AbstractRecordsOnlyAPI.\n\n    \"\"\"\n    valid_param_keys = kwargs.pop(\n        \"valid_param_keys\",\n        [\n            \"id\",\n            \"binding\",\n            \"best_datatype\",\n            \"data_usable\",\n            \"passing_replicate\",\n            \"rank_recall\",\n            \"regulator\",\n            \"regulator_locus_tag\",\n            \"regulator_symbol\",\n            \"batch\",\n            \"source\",\n        ],\n    )\n\n    url = kwargs.pop(\"url\", os.getenv(\"BINDINGMANUALQC_URL\", None))\n    if not url:\n        raise AttributeError(\n            \"url must be provided or the environmental variable \",\n            \"`BINDINGMANUALQC_URL` must be set\",\n        )\n\n    self.bulk_update_url_suffix = kwargs.pop(\n        \"bulk_update_url_suffix\", \"bulk-update\"\n    )\n\n    super().__init__(url=url, valid_param_keys=valid_param_keys, **kwargs)\n</code></pre>"},{"location":"interface/BindingManualQCAPI/#yeastdnnexplorer.interface.BindingManualQCAPI.BindingManualQCAPI.update","title":"<code>update(df, **kwargs)</code>","text":"<p>Update the records in the database.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame containing the records to update.</p> required <code>kwargs</code> <code>Any</code> <p>Additional fields to include in the payload.</p> <code>{}</code> <p>Returns:</p> Type Description <code>requests.Response</code> <p>The response from the POST request.</p> <p>Raises:</p> Type Description <code>requests.RequestException</code> <p>If the request fails.</p> Source code in <code>yeastdnnexplorer/interface/BindingManualQCAPI.py</code> <pre><code>def update(self, df: pd.DataFrame, **kwargs: Any) -&gt; requests.Response:\n    \"\"\"\n    Update the records in the database.\n\n    :param df: The DataFrame containing the records to update.\n    :type df: pd.DataFrame\n    :param kwargs: Additional fields to include in the payload.\n    :type kwargs: Any\n    :return: The response from the POST request.\n    :rtype: requests.Response\n    :raises requests.RequestException: If the request fails.\n\n    \"\"\"\n    bulk_update_url = (\n        f\"{self.url.rstrip('/')}/{self.bulk_update_url_suffix.rstrip('/')}/\"\n    )\n\n    self.logger.debug(\"bulk_update_url: %s\", bulk_update_url)\n\n    # Include additional fields in the payload if provided\n    payload = {\"data\": df.to_dict(orient=\"records\")}\n    payload.update(kwargs)\n\n    try:\n        response = requests.post(\n            bulk_update_url,\n            headers=self.header,\n            json=payload,\n        )\n        response.raise_for_status()\n        return response\n    except requests.RequestException as e:\n        self.logger.error(f\"Error in POST request: {e}\")\n        raise\n</code></pre>"},{"location":"interface/Cache/","title":"Cache","text":"<p>A caching class that uses cachetools for TTL caching with an LRU eviction policy.</p> Source code in <code>yeastdnnexplorer/interface/Cache.py</code> <pre><code>class Cache:\n    \"\"\"A caching class that uses cachetools for TTL caching with an LRU eviction\n    policy.\"\"\"\n\n    def __init__(self, maxsize: int = 100, ttl: int = 300):\n        self.ttl_cache = TTLCache(maxsize=maxsize, ttl=ttl)\n        self.logger = logging.getLogger(__name__)\n\n    def get(self, key: str, default: Any = None) -&gt; Any:\n        \"\"\"Get a value from the cache.\"\"\"\n        return self.ttl_cache.get(key, default)\n\n    def set(self, key: str, value: Any) -&gt; None:\n        \"\"\"Set a value in the cache.\"\"\"\n        self.ttl_cache[key] = value\n\n    def list(self) -&gt; list[str]:\n        \"\"\"List all keys in the cache.\"\"\"\n        return list(self.ttl_cache.keys())\n\n    def delete(self, key: str) -&gt; None:\n        \"\"\"Delete a key from the cache.\"\"\"\n        self.ttl_cache.pop(key, None)\n</code></pre>"},{"location":"interface/Cache/#yeastdnnexplorer.interface.Cache.Cache.delete","title":"<code>delete(key)</code>","text":"<p>Delete a key from the cache.</p> Source code in <code>yeastdnnexplorer/interface/Cache.py</code> <pre><code>def delete(self, key: str) -&gt; None:\n    \"\"\"Delete a key from the cache.\"\"\"\n    self.ttl_cache.pop(key, None)\n</code></pre>"},{"location":"interface/Cache/#yeastdnnexplorer.interface.Cache.Cache.get","title":"<code>get(key, default=None)</code>","text":"<p>Get a value from the cache.</p> Source code in <code>yeastdnnexplorer/interface/Cache.py</code> <pre><code>def get(self, key: str, default: Any = None) -&gt; Any:\n    \"\"\"Get a value from the cache.\"\"\"\n    return self.ttl_cache.get(key, default)\n</code></pre>"},{"location":"interface/Cache/#yeastdnnexplorer.interface.Cache.Cache.list","title":"<code>list()</code>","text":"<p>List all keys in the cache.</p> Source code in <code>yeastdnnexplorer/interface/Cache.py</code> <pre><code>def list(self) -&gt; list[str]:\n    \"\"\"List all keys in the cache.\"\"\"\n    return list(self.ttl_cache.keys())\n</code></pre>"},{"location":"interface/Cache/#yeastdnnexplorer.interface.Cache.Cache.set","title":"<code>set(key, value)</code>","text":"<p>Set a value in the cache.</p> Source code in <code>yeastdnnexplorer/interface/Cache.py</code> <pre><code>def set(self, key: str, value: Any) -&gt; None:\n    \"\"\"Set a value in the cache.\"\"\"\n    self.ttl_cache[key] = value\n</code></pre>"},{"location":"interface/CallingCardsBackgroundAPI/","title":"CallingCardsBackgroundAPI","text":"<p>             Bases: <code>AbstractRecordsAndFilesAPI</code></p> <p>Class to interact with the CallingCardsBackgroundAPI endpoint.</p> Source code in <code>yeastdnnexplorer/interface/CallingCardsBackgroundAPI.py</code> <pre><code>class CallingCardsBackgroundAPI(AbstractRecordsAndFilesAPI):\n    \"\"\"Class to interact with the CallingCardsBackgroundAPI endpoint.\"\"\"\n\n    def __init__(self, **kwargs) -&gt; None:\n        \"\"\"\n        Initialize the CallingCardsBackgroundAPI object.\n\n        :param kwargs: parameters to pass through AbstractRecordsAndFilesAPI to\n            AbstractAPI.\n\n        \"\"\"\n        valid_param_keys = kwargs.pop(\n            \"valid_param_keys\",\n            [\"id\", \"name\"],\n        )\n\n        url = kwargs.pop(\"url\", os.getenv(\"CALLINGCARDSBACKGROUND_URL\", None))\n\n        super().__init__(url=url, valid_keys=valid_param_keys, **kwargs)\n\n    def create(self, data: dict[str, Any], **kwargs) -&gt; Any:\n        raise NotImplementedError(\n            \"The CallingCardsBackgroundAPI does not support create.\"\n        )\n\n    def update(self, df: pd.DataFrame, **kwargs) -&gt; Any:\n        raise NotImplementedError(\n            \"The CallingCardsBackgroundAPI does not support update.\"\n        )\n\n    def delete(self, id: str, **kwargs) -&gt; Any:\n        raise NotImplementedError(\n            \"The CallingCardsBackgroundAPI does not support delete.\"\n        )\n\n    def submit(self, post_dict: dict[str, Any], **kwargs) -&gt; Any:\n        raise NotImplementedError(\n            \"The CallingCardsBackgroundAPI does not support submit.\"\n        )\n\n    def retrieve(\n        self, group_task_id: str, timeout: int, polling_interval: int, **kwargs\n    ) -&gt; Any:\n        raise NotImplementedError(\n            \"The CallingCardsBackgroundAPI does not support retrieve.\"\n        )\n</code></pre>"},{"location":"interface/CallingCardsBackgroundAPI/#yeastdnnexplorer.interface.CallingCardsBackgroundAPI.CallingCardsBackgroundAPI.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the CallingCardsBackgroundAPI object.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>parameters to pass through AbstractRecordsAndFilesAPI to AbstractAPI.</p> <code>{}</code> Source code in <code>yeastdnnexplorer/interface/CallingCardsBackgroundAPI.py</code> <pre><code>def __init__(self, **kwargs) -&gt; None:\n    \"\"\"\n    Initialize the CallingCardsBackgroundAPI object.\n\n    :param kwargs: parameters to pass through AbstractRecordsAndFilesAPI to\n        AbstractAPI.\n\n    \"\"\"\n    valid_param_keys = kwargs.pop(\n        \"valid_param_keys\",\n        [\"id\", \"name\"],\n    )\n\n    url = kwargs.pop(\"url\", os.getenv(\"CALLINGCARDSBACKGROUND_URL\", None))\n\n    super().__init__(url=url, valid_keys=valid_param_keys, **kwargs)\n</code></pre>"},{"location":"interface/DataSourceAPI/","title":"DataSourceAPI","text":"<p>             Bases: <code>AbstractRecordsOnlyAPI</code></p> <p>A class to interact with the DataSourceAPI endpoint.</p> Source code in <code>yeastdnnexplorer/interface/DataSourceAPI.py</code> <pre><code>class DataSourceAPI(AbstractRecordsOnlyAPI):\n    \"\"\"A class to interact with the DataSourceAPI endpoint.\"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the DataSourceAPI object.\n\n        :param kwargs: parameters to pass to AbstractAPI via AbstractRecordsOnlyAPI.\n\n        \"\"\"\n        valid_param_keys = kwargs.pop(\n            \"valid_param_keys\",\n            [\"id\", \"fileformat_id\", \"fileformat\", \"lab\", \"assay\", \"workflow\"],\n        )\n\n        url = kwargs.pop(\"url\", os.getenv(\"DATASOURCE_URL\", None))\n        if not url:\n            raise AttributeError(\n                \"url must be provided or the environmental variable \",\n                \"`DATASOURCE_URL` must be set\",\n            )\n\n        super().__init__(url=url, valid_keys=valid_param_keys, **kwargs)\n\n    def create(self, data: dict[str, Any], **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The DataSourceAPI does not support create.\")\n\n    def update(self, df: pd.DataFrame, **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The DataSourceAPI does not support update.\")\n\n    def delete(self, id: str, **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The DataSourceAPI does not support delete.\")\n\n    def submit(self, post_dict: dict[str, Any], **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The DataSourceAPI does not support submit.\")\n\n    def retrieve(\n        self, group_task_id: str, timeout: int, polling_interval: int, **kwargs\n    ) -&gt; Any:\n        raise NotImplementedError(\"The DataSourceAPI does not support retrieve.\")\n</code></pre>"},{"location":"interface/DataSourceAPI/#yeastdnnexplorer.interface.DataSourceAPI.DataSourceAPI.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the DataSourceAPI object.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>parameters to pass to AbstractAPI via AbstractRecordsOnlyAPI.</p> <code>{}</code> Source code in <code>yeastdnnexplorer/interface/DataSourceAPI.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the DataSourceAPI object.\n\n    :param kwargs: parameters to pass to AbstractAPI via AbstractRecordsOnlyAPI.\n\n    \"\"\"\n    valid_param_keys = kwargs.pop(\n        \"valid_param_keys\",\n        [\"id\", \"fileformat_id\", \"fileformat\", \"lab\", \"assay\", \"workflow\"],\n    )\n\n    url = kwargs.pop(\"url\", os.getenv(\"DATASOURCE_URL\", None))\n    if not url:\n        raise AttributeError(\n            \"url must be provided or the environmental variable \",\n            \"`DATASOURCE_URL` must be set\",\n        )\n\n    super().__init__(url=url, valid_keys=valid_param_keys, **kwargs)\n</code></pre>"},{"location":"interface/ExpressionAPI/","title":"ExpressionAPI","text":"<p>             Bases: <code>AbstractRecordsAndFilesAPI</code></p> <p>Class to interact with the ExpressionAPI endpoint.</p> Source code in <code>yeastdnnexplorer/interface/ExpressionAPI.py</code> <pre><code>class ExpressionAPI(AbstractRecordsAndFilesAPI):\n    \"\"\"Class to interact with the ExpressionAPI endpoint.\"\"\"\n\n    def __init__(self, **kwargs) -&gt; None:\n        \"\"\"\n        Initialize the ExpressionAPI object.\n\n        :param kwargs: parameters to pass through AbstractRecordsAndFilesAPI to\n            AbstractAPI.\n\n        \"\"\"\n        valid_param_keys = kwargs.pop(\n            \"valid_param_keys\",\n            [\n                \"id\",\n                \"regulator\",\n                \"regulator_locus_tag\",\n                \"regulator_symbol\",\n                \"batch\",\n                \"control\",\n                \"mechanism\",\n                \"restriction\",\n                \"time\",\n                \"source\",\n                \"source_name\",\n                \"source_time\",\n                \"lab\",\n                \"assay\",\n                \"workflow\",\n                \"effect_colname\",\n                \"pvalue_colname\",\n            ],\n        )\n\n        url = kwargs.pop(\"url\", os.getenv(\"EXPRESSION_URL\", None))\n\n        super().__init__(url=url, valid_keys=valid_param_keys, **kwargs)\n\n    def create(self, data: dict[str, Any], **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The ExpressionAPI does not support create.\")\n\n    def update(self, df: pd.DataFrame, **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The ExpressionAPI does not support update.\")\n\n    def delete(self, id: str, **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The ExpressionAPI does not support delete.\")\n\n    def submit(self, post_dict: dict[str, Any], **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The ExpressionAPI does not support submit.\")\n\n    def retrieve(\n        self, group_task_id: str, timeout: int, polling_interval: int, **kwargs\n    ) -&gt; Any:\n        raise NotImplementedError(\"The ExpressionAPI does not support retrieve.\")\n</code></pre>"},{"location":"interface/ExpressionAPI/#yeastdnnexplorer.interface.ExpressionAPI.ExpressionAPI.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the ExpressionAPI object.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>parameters to pass through AbstractRecordsAndFilesAPI to AbstractAPI.</p> <code>{}</code> Source code in <code>yeastdnnexplorer/interface/ExpressionAPI.py</code> <pre><code>def __init__(self, **kwargs) -&gt; None:\n    \"\"\"\n    Initialize the ExpressionAPI object.\n\n    :param kwargs: parameters to pass through AbstractRecordsAndFilesAPI to\n        AbstractAPI.\n\n    \"\"\"\n    valid_param_keys = kwargs.pop(\n        \"valid_param_keys\",\n        [\n            \"id\",\n            \"regulator\",\n            \"regulator_locus_tag\",\n            \"regulator_symbol\",\n            \"batch\",\n            \"control\",\n            \"mechanism\",\n            \"restriction\",\n            \"time\",\n            \"source\",\n            \"source_name\",\n            \"source_time\",\n            \"lab\",\n            \"assay\",\n            \"workflow\",\n            \"effect_colname\",\n            \"pvalue_colname\",\n        ],\n    )\n\n    url = kwargs.pop(\"url\", os.getenv(\"EXPRESSION_URL\", None))\n\n    super().__init__(url=url, valid_keys=valid_param_keys, **kwargs)\n</code></pre>"},{"location":"interface/ExpressionManualQCAPI/","title":"ExpressionManualQCAPI","text":"<p>             Bases: <code>AbstractRecordsOnlyAPI</code></p> <p>A class to interact with the ExpressionManualQCAPI endpoint.</p> Source code in <code>yeastdnnexplorer/interface/ExpressionManualQCAPI.py</code> <pre><code>class ExpressionManualQCAPI(AbstractRecordsOnlyAPI):\n    \"\"\"A class to interact with the ExpressionManualQCAPI endpoint.\"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the ExpressionManualQCAPI object.\n\n        :param kwargs: parameters to pass to AbstractAPI via AbstractRecordsOnlyAPI.\n\n        \"\"\"\n        valid_param_keys = kwargs.pop(\n            \"valid_param_keys\",\n            [\n                \"id\",\n                \"expression\",\n                \"strain_verified\",\n                \"regulator_locus_tag\",\n                \"regulator_symbol\",\n                \"batch\",\n                \"replicate\",\n                \"control\",\n                \"mechanism\",\n                \"restriction\",\n                \"time\",\n                \"source\",\n                \"lab\",\n                \"assay\",\n                \"workflow\",\n            ],\n        )\n\n        url = kwargs.pop(\"url\", os.getenv(\"EXPRESSIONMANUALQC_URL\", None))\n        if not url:\n            raise AttributeError(\n                \"url must be provided or the environmental variable \",\n                \"`EXPRESSIONMANUALQC_URL` must be set\",\n            )\n\n        super().__init__(url=url, valid_keys=valid_param_keys, **kwargs)\n\n    def create(self, data: dict[str, Any], **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The ExpressionManualQCAPI does not support create.\")\n\n    def update(self, df: pd.DataFrame, **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The ExpressionManualQCAPI does not support update.\")\n\n    def delete(self, id: str, **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The ExpressionManualQCAPI does not support delete.\")\n\n    def submit(self, post_dict: dict[str, Any], **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The ExpressionManualQCAPI does not support submit.\")\n\n    def retrieve(\n        self, group_task_id: str, timeout: int, polling_interval: int, **kwargs\n    ) -&gt; Any:\n        raise NotImplementedError(\n            \"The ExpressionManualQCAPI does not support retrieve.\"\n        )\n</code></pre>"},{"location":"interface/ExpressionManualQCAPI/#yeastdnnexplorer.interface.ExpressionManualQCAPI.ExpressionManualQCAPI.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the ExpressionManualQCAPI object.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>parameters to pass to AbstractAPI via AbstractRecordsOnlyAPI.</p> <code>{}</code> Source code in <code>yeastdnnexplorer/interface/ExpressionManualQCAPI.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the ExpressionManualQCAPI object.\n\n    :param kwargs: parameters to pass to AbstractAPI via AbstractRecordsOnlyAPI.\n\n    \"\"\"\n    valid_param_keys = kwargs.pop(\n        \"valid_param_keys\",\n        [\n            \"id\",\n            \"expression\",\n            \"strain_verified\",\n            \"regulator_locus_tag\",\n            \"regulator_symbol\",\n            \"batch\",\n            \"replicate\",\n            \"control\",\n            \"mechanism\",\n            \"restriction\",\n            \"time\",\n            \"source\",\n            \"lab\",\n            \"assay\",\n            \"workflow\",\n        ],\n    )\n\n    url = kwargs.pop(\"url\", os.getenv(\"EXPRESSIONMANUALQC_URL\", None))\n    if not url:\n        raise AttributeError(\n            \"url must be provided or the environmental variable \",\n            \"`EXPRESSIONMANUALQC_URL` must be set\",\n        )\n\n    super().__init__(url=url, valid_keys=valid_param_keys, **kwargs)\n</code></pre>"},{"location":"interface/FileFormatAPI/","title":"FileFormatAPI","text":"<p>             Bases: <code>AbstractRecordsOnlyAPI</code></p> <p>A class to interact with the FileFormatAPI endpoint.</p> Source code in <code>yeastdnnexplorer/interface/FileFormatAPI.py</code> <pre><code>class FileFormatAPI(AbstractRecordsOnlyAPI):\n    \"\"\"A class to interact with the FileFormatAPI endpoint.\"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the FileFormatAPI object.\n\n        :param kwargs: parameters to pass to AbstractAPI via AbstractRecordsOnlyAPI.\n\n        \"\"\"\n        valid_param_keys = kwargs.pop(\n            \"valid_param_keys\",\n            [\n                \"fileformat\",\n                \"fields\",\n                \"separator\",\n                \"feature_identifier_col\",\n                \"effect_col\",\n                \"default_effect_threshold\",\n                \"pval_col\",\n                \"default_pvalue_threshold\",\n            ],\n        )\n\n        url = kwargs.pop(\"url\", os.getenv(\"FILEFORMAT_URL\", None))\n        if not url:\n            raise AttributeError(\n                \"url must be provided or the environmental variable \",\n                \"`FILEFORMAT_URL` must be set\",\n            )\n\n        super().__init__(url=url, valid_keys=valid_param_keys, **kwargs)\n\n    def create(self, data: dict[str, Any], **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The FileFormatAPI does not support create.\")\n\n    def update(self, df: pd.DataFrame, **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The FileFormatAPI does not support update.\")\n\n    def delete(self, id: str, **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The FileFormatAPI does not support delete.\")\n\n    def submit(self, post_dict: dict[str, Any], **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The FileFormatAPI does not support submit.\")\n\n    def retrieve(\n        self, group_task_id: str, timeout: int, polling_interval: int, **kwargs\n    ) -&gt; Any:\n        raise NotImplementedError(\"The FileFormatAPI does not support retrieve.\")\n</code></pre>"},{"location":"interface/FileFormatAPI/#yeastdnnexplorer.interface.FileFormatAPI.FileFormatAPI.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the FileFormatAPI object.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>parameters to pass to AbstractAPI via AbstractRecordsOnlyAPI.</p> <code>{}</code> Source code in <code>yeastdnnexplorer/interface/FileFormatAPI.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the FileFormatAPI object.\n\n    :param kwargs: parameters to pass to AbstractAPI via AbstractRecordsOnlyAPI.\n\n    \"\"\"\n    valid_param_keys = kwargs.pop(\n        \"valid_param_keys\",\n        [\n            \"fileformat\",\n            \"fields\",\n            \"separator\",\n            \"feature_identifier_col\",\n            \"effect_col\",\n            \"default_effect_threshold\",\n            \"pval_col\",\n            \"default_pvalue_threshold\",\n        ],\n    )\n\n    url = kwargs.pop(\"url\", os.getenv(\"FILEFORMAT_URL\", None))\n    if not url:\n        raise AttributeError(\n            \"url must be provided or the environmental variable \",\n            \"`FILEFORMAT_URL` must be set\",\n        )\n\n    super().__init__(url=url, valid_keys=valid_param_keys, **kwargs)\n</code></pre>"},{"location":"interface/GenomicFeatureAPI/","title":"GenomicFeatureAPI","text":"<p>             Bases: <code>AbstractRecordsOnlyAPI</code></p> <p>A class to interact with the GenomicFeatureAPI endpoint.</p> Source code in <code>yeastdnnexplorer/interface/GenomicFeatureAPI.py</code> <pre><code>class GenomicFeatureAPI(AbstractRecordsOnlyAPI):\n    \"\"\"A class to interact with the GenomicFeatureAPI endpoint.\"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the GenomicFeatureAPI object.\n\n        :param kwargs: parameters to pass to AbstractAPI via AbstractRecordsOnlyAPI.\n\n        \"\"\"\n        valid_param_keys = kwargs.pop(\n            \"valid_param_keys\",\n            [\n                \"id\",\n                \"chr\",\n                \"start\",\n                \"end\",\n                \"strand\",\n                \"type\",\n                \"locus_tag\",\n                \"symbol\",\n                \"source\",\n                \"alias\",\n                \"note\",\n            ],\n        )\n\n        url = kwargs.pop(\"url\", os.getenv(\"GENOMICFEATURE_URL\", None))\n        if not url:\n            raise AttributeError(\n                \"url must be provided or the environmental variable \",\n                \"`GENOMICFEATURE_URL` must be set\",\n            )\n\n        super().__init__(url=url, valid_keys=valid_param_keys, **kwargs)\n\n    def create(self, data: dict[str, Any], **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The GenomicFeatureAPI does not support create.\")\n\n    def update(self, df: pd.DataFrame, **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The GenomicFeatureAPI does not support update.\")\n\n    def delete(self, id: str, **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The GenomicFeatureAPI does not support delete.\")\n\n    def submit(self, post_dict: dict[str, Any], **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The GenomicFeatureAPI does not support submit.\")\n\n    def retrieve(\n        self, group_task_id: str, timeout: int, polling_interval: int, **kwargs\n    ) -&gt; Any:\n        raise NotImplementedError(\"The GenomicFeatureAPI does not support retrieve.\")\n</code></pre>"},{"location":"interface/GenomicFeatureAPI/#yeastdnnexplorer.interface.GenomicFeatureAPI.GenomicFeatureAPI.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the GenomicFeatureAPI object.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>parameters to pass to AbstractAPI via AbstractRecordsOnlyAPI.</p> <code>{}</code> Source code in <code>yeastdnnexplorer/interface/GenomicFeatureAPI.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the GenomicFeatureAPI object.\n\n    :param kwargs: parameters to pass to AbstractAPI via AbstractRecordsOnlyAPI.\n\n    \"\"\"\n    valid_param_keys = kwargs.pop(\n        \"valid_param_keys\",\n        [\n            \"id\",\n            \"chr\",\n            \"start\",\n            \"end\",\n            \"strand\",\n            \"type\",\n            \"locus_tag\",\n            \"symbol\",\n            \"source\",\n            \"alias\",\n            \"note\",\n        ],\n    )\n\n    url = kwargs.pop(\"url\", os.getenv(\"GENOMICFEATURE_URL\", None))\n    if not url:\n        raise AttributeError(\n            \"url must be provided or the environmental variable \",\n            \"`GENOMICFEATURE_URL` must be set\",\n        )\n\n    super().__init__(url=url, valid_keys=valid_param_keys, **kwargs)\n</code></pre>"},{"location":"interface/ParamsDict/","title":"ParamsDict","text":"<p>             Bases: <code>dict</code></p> <p>A dictionary subclass that ensures all keys are strings and supports multiple key- value assignments at once, with validation against a list of valid keys.</p> <p>This class is designed to be used for passing parameters to HTTP requests and extends the base dictionary class, ensuring that insertion order is preserved.</p> Source code in <code>yeastdnnexplorer/interface/ParamsDict.py</code> <pre><code>class ParamsDict(dict):\n    \"\"\"\n    A dictionary subclass that ensures all keys are strings and supports multiple key-\n    value assignments at once, with validation against a list of valid keys.\n\n    This class is designed to be used for passing parameters to HTTP requests and\n    extends the base dictionary class, ensuring that insertion order is preserved.\n\n    \"\"\"\n\n    def __init__(self, params: dict[str, Any] = {}, valid_keys: list[str] = []) -&gt; None:\n        \"\"\"\n        Initialize the ParamsDict with optional initial parameters and valid keys.\n\n        :param params: A dictionary of initial parameters. All keys must be strings.\n        :type params: dict, optional\n        :param valid_keys: A list of valid keys for validation.\n        :type valid_keys: list of str, optional\n        :raises ValueError: If `params` is not a dictionary or if any of the keys\n            are not strings.\n\n        \"\"\"\n        params = params or {}\n        valid_keys = valid_keys or []\n        if not isinstance(params, dict):\n            raise ValueError(\"params must be a dictionary\")\n        if len(params) &gt; 0 and not all(isinstance(k, str) for k in params.keys()):\n            raise ValueError(\"params must be a dictionary with string keys\")\n        super().__init__(params)\n        self._valid_keys = valid_keys\n\n    def __setitem__(self, key: str | list[str], value: Any | list[Any]) -&gt; None:\n        \"\"\"\n        Set a parameter value or multiple parameter values.\n\n        :param key: The parameter key or a list of parameter keys.\n        :type key: str or list of str\n        :param value: The parameter value or a list of parameter values.\n        :type value: any or list of any\n        :raises ValueError: If the length of `key` and `value` lists do not match.\n        :raises KeyError: If `key` is not a string or a list of strings.\n\n        \"\"\"\n        if isinstance(key, str):\n            self._validate_key(key)\n            super().__setitem__(key, value)\n        elif isinstance(key, list) and isinstance(value, list):\n            if len(key) != len(value):\n                raise ValueError(\"Length of keys and values must match\")\n            for k, v in zip(key, value):\n                if not isinstance(k, str):\n                    raise KeyError(\"All keys must be strings\")\n                self._validate_key(k)\n                super().__setitem__(k, v)\n        else:\n            raise KeyError(\"Key must be a string or list of strings\")\n\n    def __getitem__(self, key: str | list[str]) -&gt; Union[Any, \"ParamsDict\"]:\n        \"\"\"\n        Get a parameter value or a new ParamsDict with specified keys.\n\n        :param key: The parameter key or a list of parameter keys.\n        :type key: str or list of str\n        :return: The parameter value or a new ParamsDict with the specified keys.\n        :rtype: any or ParamsDict\n        :raises KeyError: If `key` is not a string or a list of strings.\n\n        \"\"\"\n        if isinstance(key, str):\n            return super().__getitem__(key)\n        elif isinstance(key, list):\n            return ParamsDict({k: dict.__getitem__(self, k) for k in key if k in self})\n        else:\n            raise KeyError(\"Key must be a string or list of strings\")\n\n    def __delitem__(self, key: str) -&gt; None:\n        \"\"\"\n        Delete a parameter by key.\n\n        :param key: The parameter key.\n        :type key: str\n        :raises KeyError: If `key` is not a string.\n\n        \"\"\"\n        if isinstance(key, str):\n            super().__delitem__(key)\n        else:\n            raise KeyError(\"Key must be a string\")\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Return a string representation of the ParamsDict.\n\n        :return: A string representation of the ParamsDict.\n        :rtype: str\n\n        \"\"\"\n        return f\"ParamsDict({super().__repr__()})\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"\n        Return a human-readable string representation of the ParamsDict.\n\n        :return: A human-readable string representation of the ParamsDict.\n        :rtype: str\n\n        \"\"\"\n        return \", \".join(f\"{k}: {v}\" for k, v in self.items())\n\n    def update(self, *args, **kwargs) -&gt; None:\n        \"\"\"Update the ParamsDict with the key/value pairs from other, overwriting\n        existing keys.\"\"\"\n        if args:\n            other = args[0]\n            if isinstance(other, dict):\n                [self._validate_key(k) for k in other.keys()]\n                for key, value in other.items():\n                    self.__setitem__(key, value)\n            else:\n                [self._validate_key(k) for k, _ in other]\n                for key, value in other:\n                    self.__setitem__(key, value)\n        [self._validate_key(k) for k in kwargs.keys()]\n        for key, value in kwargs.items():\n            self.__setitem__(key, value)\n\n    def as_dict(self) -&gt; dict:\n        \"\"\"\n        Convert the ParamsDict to a standard dictionary.\n\n        :return: A standard dictionary with the same items as the ParamsDict.\n        :rtype: dict\n\n        \"\"\"\n        return dict(self)\n\n    def _validate_key(self, key: str) -&gt; bool:\n        \"\"\"Validate that the key is in the list of valid keys.\"\"\"\n        if self._valid_keys and key not in self._valid_keys:\n            raise KeyError(f\"Invalid parameter key provided: {key}\")\n        return True\n\n    @property\n    def valid_keys(self) -&gt; list[str]:\n        \"\"\"Get the list of valid keys.\"\"\"\n        return self._valid_keys\n\n    @valid_keys.setter\n    def valid_keys(self, keys: list[str]) -&gt; None:\n        \"\"\"Set the list of valid keys.\"\"\"\n        if not all(isinstance(k, str) for k in keys):\n            raise ValueError(\"valid_keys must be a list of strings\")\n        self._valid_keys = keys\n</code></pre>"},{"location":"interface/ParamsDict/#yeastdnnexplorer.interface.ParamsDict.ParamsDict.valid_keys","title":"<code>valid_keys: list[str]</code>  <code>property</code> <code>writable</code>","text":"<p>Get the list of valid keys.</p>"},{"location":"interface/ParamsDict/#yeastdnnexplorer.interface.ParamsDict.ParamsDict.__delitem__","title":"<code>__delitem__(key)</code>","text":"<p>Delete a parameter by key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The parameter key.</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>If <code>key</code> is not a string.</p> Source code in <code>yeastdnnexplorer/interface/ParamsDict.py</code> <pre><code>def __delitem__(self, key: str) -&gt; None:\n    \"\"\"\n    Delete a parameter by key.\n\n    :param key: The parameter key.\n    :type key: str\n    :raises KeyError: If `key` is not a string.\n\n    \"\"\"\n    if isinstance(key, str):\n        super().__delitem__(key)\n    else:\n        raise KeyError(\"Key must be a string\")\n</code></pre>"},{"location":"interface/ParamsDict/#yeastdnnexplorer.interface.ParamsDict.ParamsDict.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Get a parameter value or a new ParamsDict with specified keys.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str | list[str]</code> <p>The parameter key or a list of parameter keys.</p> required <p>Returns:</p> Type Description <code>any | ParamsDict</code> <p>The parameter value or a new ParamsDict with the specified keys.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If <code>key</code> is not a string or a list of strings.</p> Source code in <code>yeastdnnexplorer/interface/ParamsDict.py</code> <pre><code>def __getitem__(self, key: str | list[str]) -&gt; Union[Any, \"ParamsDict\"]:\n    \"\"\"\n    Get a parameter value or a new ParamsDict with specified keys.\n\n    :param key: The parameter key or a list of parameter keys.\n    :type key: str or list of str\n    :return: The parameter value or a new ParamsDict with the specified keys.\n    :rtype: any or ParamsDict\n    :raises KeyError: If `key` is not a string or a list of strings.\n\n    \"\"\"\n    if isinstance(key, str):\n        return super().__getitem__(key)\n    elif isinstance(key, list):\n        return ParamsDict({k: dict.__getitem__(self, k) for k in key if k in self})\n    else:\n        raise KeyError(\"Key must be a string or list of strings\")\n</code></pre>"},{"location":"interface/ParamsDict/#yeastdnnexplorer.interface.ParamsDict.ParamsDict.__init__","title":"<code>__init__(params={}, valid_keys=[])</code>","text":"<p>Initialize the ParamsDict with optional initial parameters and valid keys.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict[str, Any]</code> <p>A dictionary of initial parameters. All keys must be strings.</p> <code>{}</code> <code>valid_keys</code> <code>list[str]</code> <p>A list of valid keys for validation.</p> <code>[]</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>params</code> is not a dictionary or if any of the keys are not strings.</p> Source code in <code>yeastdnnexplorer/interface/ParamsDict.py</code> <pre><code>def __init__(self, params: dict[str, Any] = {}, valid_keys: list[str] = []) -&gt; None:\n    \"\"\"\n    Initialize the ParamsDict with optional initial parameters and valid keys.\n\n    :param params: A dictionary of initial parameters. All keys must be strings.\n    :type params: dict, optional\n    :param valid_keys: A list of valid keys for validation.\n    :type valid_keys: list of str, optional\n    :raises ValueError: If `params` is not a dictionary or if any of the keys\n        are not strings.\n\n    \"\"\"\n    params = params or {}\n    valid_keys = valid_keys or []\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be a dictionary\")\n    if len(params) &gt; 0 and not all(isinstance(k, str) for k in params.keys()):\n        raise ValueError(\"params must be a dictionary with string keys\")\n    super().__init__(params)\n    self._valid_keys = valid_keys\n</code></pre>"},{"location":"interface/ParamsDict/#yeastdnnexplorer.interface.ParamsDict.ParamsDict.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a string representation of the ParamsDict.</p> <p>Returns:</p> Type Description <code>str</code> <p>A string representation of the ParamsDict.</p> Source code in <code>yeastdnnexplorer/interface/ParamsDict.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Return a string representation of the ParamsDict.\n\n    :return: A string representation of the ParamsDict.\n    :rtype: str\n\n    \"\"\"\n    return f\"ParamsDict({super().__repr__()})\"\n</code></pre>"},{"location":"interface/ParamsDict/#yeastdnnexplorer.interface.ParamsDict.ParamsDict.__setitem__","title":"<code>__setitem__(key, value)</code>","text":"<p>Set a parameter value or multiple parameter values.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str | list[str]</code> <p>The parameter key or a list of parameter keys.</p> required <code>value</code> <code>Any | list[Any]</code> <p>The parameter value or a list of parameter values.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the length of <code>key</code> and <code>value</code> lists do not match.</p> <code>KeyError</code> <p>If <code>key</code> is not a string or a list of strings.</p> Source code in <code>yeastdnnexplorer/interface/ParamsDict.py</code> <pre><code>def __setitem__(self, key: str | list[str], value: Any | list[Any]) -&gt; None:\n    \"\"\"\n    Set a parameter value or multiple parameter values.\n\n    :param key: The parameter key or a list of parameter keys.\n    :type key: str or list of str\n    :param value: The parameter value or a list of parameter values.\n    :type value: any or list of any\n    :raises ValueError: If the length of `key` and `value` lists do not match.\n    :raises KeyError: If `key` is not a string or a list of strings.\n\n    \"\"\"\n    if isinstance(key, str):\n        self._validate_key(key)\n        super().__setitem__(key, value)\n    elif isinstance(key, list) and isinstance(value, list):\n        if len(key) != len(value):\n            raise ValueError(\"Length of keys and values must match\")\n        for k, v in zip(key, value):\n            if not isinstance(k, str):\n                raise KeyError(\"All keys must be strings\")\n            self._validate_key(k)\n            super().__setitem__(k, v)\n    else:\n        raise KeyError(\"Key must be a string or list of strings\")\n</code></pre>"},{"location":"interface/ParamsDict/#yeastdnnexplorer.interface.ParamsDict.ParamsDict.__str__","title":"<code>__str__()</code>","text":"<p>Return a human-readable string representation of the ParamsDict.</p> <p>Returns:</p> Type Description <code>str</code> <p>A human-readable string representation of the ParamsDict.</p> Source code in <code>yeastdnnexplorer/interface/ParamsDict.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"\n    Return a human-readable string representation of the ParamsDict.\n\n    :return: A human-readable string representation of the ParamsDict.\n    :rtype: str\n\n    \"\"\"\n    return \", \".join(f\"{k}: {v}\" for k, v in self.items())\n</code></pre>"},{"location":"interface/ParamsDict/#yeastdnnexplorer.interface.ParamsDict.ParamsDict.as_dict","title":"<code>as_dict()</code>","text":"<p>Convert the ParamsDict to a standard dictionary.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A standard dictionary with the same items as the ParamsDict.</p> Source code in <code>yeastdnnexplorer/interface/ParamsDict.py</code> <pre><code>def as_dict(self) -&gt; dict:\n    \"\"\"\n    Convert the ParamsDict to a standard dictionary.\n\n    :return: A standard dictionary with the same items as the ParamsDict.\n    :rtype: dict\n\n    \"\"\"\n    return dict(self)\n</code></pre>"},{"location":"interface/ParamsDict/#yeastdnnexplorer.interface.ParamsDict.ParamsDict.update","title":"<code>update(*args, **kwargs)</code>","text":"<p>Update the ParamsDict with the key/value pairs from other, overwriting existing keys.</p> Source code in <code>yeastdnnexplorer/interface/ParamsDict.py</code> <pre><code>def update(self, *args, **kwargs) -&gt; None:\n    \"\"\"Update the ParamsDict with the key/value pairs from other, overwriting\n    existing keys.\"\"\"\n    if args:\n        other = args[0]\n        if isinstance(other, dict):\n            [self._validate_key(k) for k in other.keys()]\n            for key, value in other.items():\n                self.__setitem__(key, value)\n        else:\n            [self._validate_key(k) for k, _ in other]\n            for key, value in other:\n                self.__setitem__(key, value)\n    [self._validate_key(k) for k in kwargs.keys()]\n    for key, value in kwargs.items():\n        self.__setitem__(key, value)\n</code></pre>"},{"location":"interface/PromoterSetAPI/","title":"PromoterSetAPI","text":"<p>             Bases: <code>AbstractRecordsAndFilesAPI</code></p> <p>Class to interact with the PromoterSetAPI endpoint.</p> Source code in <code>yeastdnnexplorer/interface/PromoterSetAPI.py</code> <pre><code>class PromoterSetAPI(AbstractRecordsAndFilesAPI):\n    \"\"\"Class to interact with the PromoterSetAPI endpoint.\"\"\"\n\n    def __init__(self, **kwargs) -&gt; None:\n        \"\"\"\n        Initialize the PromoterSetAPI object.\n\n        :param kwargs: parameters to pass through AbstractRecordsAndFilesAPI to\n            AbstractAPI.\n\n        \"\"\"\n        valid_param_keys = kwargs.pop(\n            \"valid_param_keys\",\n            [\"id\", \"name\"],\n        )\n\n        url = kwargs.pop(\"url\", os.getenv(\"PROMOTERSET_URL\", None))\n\n        super().__init__(url=url, valid_keys=valid_param_keys, **kwargs)\n\n    def create(self, data: dict[str, Any], **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The PromoterSetAPI does not support create.\")\n\n    def update(self, df: pd.DataFrame, **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The PromoterSetAPI does not support update.\")\n\n    def delete(self, id: str, **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The PromoterSetAPI does not support delete.\")\n\n    def submit(self, post_dict: dict[str, Any], **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The PromoterSetAPI does not support submit.\")\n\n    def retrieve(\n        self, group_task_id: str, timeout: int, polling_interval: int, **kwargs\n    ) -&gt; Any:\n        raise NotImplementedError(\"The PromoterSetAPI does not support retrieve.\")\n</code></pre>"},{"location":"interface/PromoterSetAPI/#yeastdnnexplorer.interface.PromoterSetAPI.PromoterSetAPI.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the PromoterSetAPI object.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>parameters to pass through AbstractRecordsAndFilesAPI to AbstractAPI.</p> <code>{}</code> Source code in <code>yeastdnnexplorer/interface/PromoterSetAPI.py</code> <pre><code>def __init__(self, **kwargs) -&gt; None:\n    \"\"\"\n    Initialize the PromoterSetAPI object.\n\n    :param kwargs: parameters to pass through AbstractRecordsAndFilesAPI to\n        AbstractAPI.\n\n    \"\"\"\n    valid_param_keys = kwargs.pop(\n        \"valid_param_keys\",\n        [\"id\", \"name\"],\n    )\n\n    url = kwargs.pop(\"url\", os.getenv(\"PROMOTERSET_URL\", None))\n\n    super().__init__(url=url, valid_keys=valid_param_keys, **kwargs)\n</code></pre>"},{"location":"interface/PromoterSetSigAPI/","title":"PromoterSetSigAPI","text":"<p>             Bases: <code>AbstractRecordsAndFilesAPI</code></p> <p>Class to interact with the PromoterSetSigAPI endpoint.</p> Source code in <code>yeastdnnexplorer/interface/PromoterSetSigAPI.py</code> <pre><code>class PromoterSetSigAPI(AbstractRecordsAndFilesAPI):\n    \"\"\"Class to interact with the PromoterSetSigAPI endpoint.\"\"\"\n\n    def __init__(self, **kwargs) -&gt; None:\n        \"\"\"\n        Initialize the PromoterSetSigAPI object.\n\n        :param kwargs: parameters to pass through AbstractRecordsAndFilesAPI to\n            AbstractAPI.\n\n        \"\"\"\n        valid_param_keys = kwargs.pop(\n            \"valid_param_keys\",\n            [\n                \"id\",\n                \"binding\",\n                \"promoter\",\n                \"promoter_name\",\n                \"background\",\n                \"background_name\",\n                \"regulator_locus_tag\",\n                \"regulator_symbol\",\n                \"batch\",\n                \"replicate\",\n                \"source\",\n                \"source_name\",\n                \"lab\",\n                \"assay\",\n                \"workflow\",\n                \"data_usable\",\n                \"aggregated\",\n                \"condition\",\n            ],\n        )\n\n        url = kwargs.pop(\"url\", os.getenv(\"PROMOTERSETSIG_URL\", None))\n\n        super().__init__(url=url, valid_keys=valid_param_keys, **kwargs)\n\n    def create(self, data: dict[str, Any], **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The PromoterSetSigAPI does not support create.\")\n\n    def update(self, df: pd.DataFrame, **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The PromoterSetSigAPI does not support update.\")\n\n    def delete(self, id: str, **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The PromoterSetSigAPI does not support delete.\")\n\n    def submit(self, post_dict: dict[str, Any], **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The PromoterSetSigAPI does not support submit.\")\n\n    def retrieve(\n        self, group_task_id: str, timeout: int, polling_interval: int, **kwargs\n    ) -&gt; Any:\n        raise NotImplementedError(\"The PromoterSetSigAPI does not support retrieve.\")\n</code></pre>"},{"location":"interface/PromoterSetSigAPI/#yeastdnnexplorer.interface.PromoterSetSigAPI.PromoterSetSigAPI.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the PromoterSetSigAPI object.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>parameters to pass through AbstractRecordsAndFilesAPI to AbstractAPI.</p> <code>{}</code> Source code in <code>yeastdnnexplorer/interface/PromoterSetSigAPI.py</code> <pre><code>def __init__(self, **kwargs) -&gt; None:\n    \"\"\"\n    Initialize the PromoterSetSigAPI object.\n\n    :param kwargs: parameters to pass through AbstractRecordsAndFilesAPI to\n        AbstractAPI.\n\n    \"\"\"\n    valid_param_keys = kwargs.pop(\n        \"valid_param_keys\",\n        [\n            \"id\",\n            \"binding\",\n            \"promoter\",\n            \"promoter_name\",\n            \"background\",\n            \"background_name\",\n            \"regulator_locus_tag\",\n            \"regulator_symbol\",\n            \"batch\",\n            \"replicate\",\n            \"source\",\n            \"source_name\",\n            \"lab\",\n            \"assay\",\n            \"workflow\",\n            \"data_usable\",\n            \"aggregated\",\n            \"condition\",\n        ],\n    )\n\n    url = kwargs.pop(\"url\", os.getenv(\"PROMOTERSETSIG_URL\", None))\n\n    super().__init__(url=url, valid_keys=valid_param_keys, **kwargs)\n</code></pre>"},{"location":"interface/RegulatorAPI/","title":"RegulatorAPI","text":"<p>             Bases: <code>AbstractRecordsOnlyAPI</code></p> <p>A class to interact with the RegulatorAPI endpoint.</p> Source code in <code>yeastdnnexplorer/interface/RegulatorAPI.py</code> <pre><code>class RegulatorAPI(AbstractRecordsOnlyAPI):\n    \"\"\"A class to interact with the RegulatorAPI endpoint.\"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize the RegulatorAPI object.\n\n        :param kwargs: parameters to pass to AbstractAPI via AbstractRecordsOnlyAPI.\n\n        \"\"\"\n        valid_param_keys = kwargs.pop(\n            \"valid_param_keys\",\n            [\n                \"id\",\n                \"regulator_locus_tag\",\n                \"regulator_symbol\",\n                \"under_development\",\n            ],\n        )\n\n        url = kwargs.pop(\"url\", os.getenv(\"REGULATOR_URL\", None))\n        if not url:\n            raise AttributeError(\n                \"url must be provided or the environmental variable \",\n                \"`REGULATOR_URL` must be set\",\n            )\n\n        super().__init__(url=url, valid_keys=valid_param_keys, **kwargs)\n\n    def create(self, data: dict[str, Any], **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The RegulatorAPI does not support create.\")\n\n    def update(self, df: pd.DataFrame, **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The RegulatorAPI does not support update.\")\n\n    def delete(self, id: str, **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The RegulatorAPI does not support delete.\")\n\n    def submit(self, post_dict: dict[str, Any], **kwargs) -&gt; Any:\n        raise NotImplementedError(\"The RegulatorAPI does not support submit.\")\n\n    def retrieve(\n        self, group_task_id: str, timeout: int, polling_interval: int, **kwargs\n    ) -&gt; Any:\n        raise NotImplementedError(\"The RegulatorAPI does not support retrieve.\")\n</code></pre>"},{"location":"interface/RegulatorAPI/#yeastdnnexplorer.interface.RegulatorAPI.RegulatorAPI.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialize the RegulatorAPI object.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>parameters to pass to AbstractAPI via AbstractRecordsOnlyAPI.</p> <code>{}</code> Source code in <code>yeastdnnexplorer/interface/RegulatorAPI.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialize the RegulatorAPI object.\n\n    :param kwargs: parameters to pass to AbstractAPI via AbstractRecordsOnlyAPI.\n\n    \"\"\"\n    valid_param_keys = kwargs.pop(\n        \"valid_param_keys\",\n        [\n            \"id\",\n            \"regulator_locus_tag\",\n            \"regulator_symbol\",\n            \"under_development\",\n        ],\n    )\n\n    url = kwargs.pop(\"url\", os.getenv(\"REGULATOR_URL\", None))\n    if not url:\n        raise AttributeError(\n            \"url must be provided or the environmental variable \",\n            \"`REGULATOR_URL` must be set\",\n        )\n\n    super().__init__(url=url, valid_keys=valid_param_keys, **kwargs)\n</code></pre>"},{"location":"ml_models/customizable_model/","title":"Models","text":"<p>             Bases: <code>LightningModule</code></p> <p>A class for a customizable model that takes in binding effects for each transcription factor and predicts gene expression values This class contains all of the logic for setup, training, validation, and testing of the model, as well as defining how data is passed through the model It is a subclass of pytorch_lightning.LightningModule, which is similar to a regular PyTorch nn.module but with added functionality for training and validation.</p> <p>This model takes in many more parameters that SimpleModel, allowing us to experiement with many hyperparameter and architecture choices in order to decide what is best for our task &amp; data</p> Source code in <code>yeastdnnexplorer/ml_models/customizable_model.py</code> <pre><code>class CustomizableModel(pl.LightningModule):\n    \"\"\"\n    A class for a customizable model that takes in binding effects for each\n    transcription factor and predicts gene expression values This class contains all of\n    the logic for setup, training, validation, and testing of the model, as well as\n    defining how data is passed through the model It is a subclass of\n    pytorch_lightning.LightningModule, which is similar to a regular PyTorch nn.module\n    but with added functionality for training and validation.\n\n    This model takes in many more parameters that SimpleModel, allowing us to\n    experiement with many hyperparameter and architecture choices in order to decide\n    what is best for our task &amp; data\n\n    \"\"\"\n\n    def __init__(\n        self,\n        input_dim: int,\n        output_dim: int,\n        lr: float = 0.001,\n        hidden_layer_num: int = 1,\n        hidden_layer_sizes: list = [128],\n        activation: str = \"ReLU\",  # can be \"ReLU\", \"Sigmoid\", \"Tanh\", \"LeakyReLU\"\n        optimizer: str = \"Adam\",  # can be \"Adam\", \"SGD\", \"RMSprop\"\n        L2_regularization_term: float = 0.0,\n        dropout_rate: float = 0.0,\n    ) -&gt; None:\n        \"\"\"\n        Constructor of CustomizableModel.\n\n        :param input_dim: The number of input features to our model, these are the\n            binding effects for each transcription factor for a specific gene\n        :type input_dim: int\n        :param output_dim: The number of output features of our model, this is the\n            predicted gene expression value for each TF\n        :type output_dim: int\n        :param lr: The learning rate for the optimizer\n        :type lr: float\n        :raises TypeError: If input_dim is not an integer\n        :raises TypeError: If output_dim is not an integer\n        :raises TypeError: If lr is not a positive float\n        :raises ValueError: If input_dim or output_dim are not positive\n        :param hidden_layer_num: The number of hidden layers in the model\n        :type hidden_layer_num: int\n        :param hidden_layer_sizes: The size of each hidden layer in the model\n        :type hidden_layer_sizes: list\n\n        \"\"\"\n        if not isinstance(input_dim, int):\n            raise TypeError(\"input_dim must be an integer\")\n        if not isinstance(output_dim, int):\n            raise TypeError(\"output_dim must be an integer\")\n        if not isinstance(lr, float) or lr &lt;= 0:\n            raise TypeError(\"lr must be a positive float\")\n        if input_dim &lt; 1 or output_dim &lt; 1:\n            raise ValueError(\"input_dim and output_dim must be positive integers\")\n        if not isinstance(hidden_layer_num, int):\n            raise TypeError(\"hidden_layer_num must be an integer\")\n        if not isinstance(hidden_layer_sizes, list) or not all(\n            isinstance(i, int) for i in hidden_layer_sizes\n        ):\n            raise TypeError(\"hidden_layer_sizes must be a list of integers\")\n        if len(hidden_layer_sizes) != hidden_layer_num:\n            raise ValueError(\n                \"hidden_layer_sizes must have length equal to hidden_layer_num\"\n            )\n        if not isinstance(activation, str) or activation not in [\n            \"ReLU\",\n            \"Sigmoid\",\n            \"Tanh\",\n            \"LeakyReLU\",\n        ]:\n            raise ValueError(\n                \"activation must be one of 'ReLU', 'Sigmoid', 'Tanh', 'LeakyReLU'\"\n            )\n        if not isinstance(optimizer, str) or optimizer not in [\n            \"Adam\",\n            \"SGD\",\n            \"RMSprop\",\n        ]:\n            raise ValueError(\"optimizer must be one of 'Adam', 'SGD', 'RMSprop'\")\n        if not isinstance(L2_regularization_term, float) or L2_regularization_term &lt; 0:\n            raise TypeError(\"L2_regularization_term must be a non-negative float\")\n        if not isinstance(dropout_rate, float) or dropout_rate &lt; 0 or dropout_rate &gt; 1:\n            raise TypeError(\"dropout_rate must be a float between 0 and 1\")\n\n        super().__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.lr = lr\n        self.hidden_layer_num = hidden_layer_num\n        self.hidden_layer_sizes = hidden_layer_sizes\n        self.optimizer = optimizer\n        self.L2_regularization_term = L2_regularization_term\n        self.save_hyperparameters()\n\n        match activation:\n            case \"ReLU\":\n                self.activation = nn.ReLU()\n            case \"Sigmoid\":\n                self.activation = nn.Sigmoid()\n            case \"Tanh\":\n                self.activation = nn.Tanh()\n            case \"LeakyReLU\":\n                self.activation = nn.LeakyReLU()\n            case _:\n                raise ValueError(\n                    \"activation must be one of 'ReLU', 'Sigmoid', 'Tanh', 'LeakyReLU'\"\n                )\n\n        self.input_layer = nn.Linear(input_dim, hidden_layer_sizes[0])\n        self.hidden_layers = nn.ModuleList([])\n        for i in range(hidden_layer_num - 1):\n            self.hidden_layers.append(\n                nn.Linear(hidden_layer_sizes[i], hidden_layer_sizes[i + 1])\n            )\n        self.output_layer = nn.Linear(hidden_layer_sizes[-1], output_dim)\n\n        self.dropout = nn.Dropout(p=dropout_rate)\n\n        self.mae = MeanAbsoluteError()\n        self.SMSE = SMSE()\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the model (i.e. how predictions are made for a given input)\n\n        :param x: The input data to the model (minus the target y values)\n        :type x: torch.Tensor\n        :return: The predicted y values for the input x, this is a tensor of shape\n            (batch_size, output_dim)\n        :rtype: torch.Tensor\n\n        \"\"\"\n        x = self.dropout(self.activation(self.input_layer(x)))\n        for hidden_layer in self.hidden_layers:\n            x = self.dropout(self.activation(hidden_layer(x)))\n        x = self.output_layer(x)\n        return x\n\n    def training_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Training step for the model, this is called for each batch of data during\n        training.\n\n        :param batch: The batch of data to train on\n        :type batch: Any\n        :param batch_idx: The index of the batch\n        :type batch_idx: int\n        :return: The loss for the training batch\n        :rtype: torch.Tensor\n\n        \"\"\"\n        x, y = batch\n        y_pred = self(x)\n        mse_loss = nn.functional.mse_loss(y_pred, y)\n        self.log(\"train_mse\", mse_loss)\n        self.log(\"train_mae\", self.mae(y_pred, y))\n        self.log(\"train_smse\", self.SMSE(y_pred, y))\n        return mse_loss\n\n    def validation_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Validation step for the model, this is called for each batch of data during\n        validation.\n\n        :param batch: The batch of data to validate on\n        :type batch: Any\n        :param batch_idx: The index of the batch\n        :type batch_idx: int\n        :return: The loss for the validation batch\n        :rtype: torch.Tensor\n\n        \"\"\"\n        x, y = batch\n        y_pred = self(x)\n        mse_loss = nn.functional.mse_loss(y_pred, y)\n        self.log(\"val_mse\", mse_loss)\n        self.log(\"val_mae\", self.mae(y_pred, y))\n        self.log(\"val_smse\", self.SMSE(y_pred, y))\n        return mse_loss\n\n    def test_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Test step for the model, this is called for each batch of data during testing\n        Testing is only performed after training and validation when we have chosen a\n        final model We want to test our final model on unseen data (which is why we use\n        validation sets to \"test\" during training)\n\n        :param batch: The batch of data to test on (this will have size (batch_size,\n            input_dim)\n        :type batch: Any\n        :param batch_idx: The index of the batch\n        :type batch_idx: int\n        :return: The loss for the test batch\n        :rtype: torch.Tensor\n\n        \"\"\"\n        x, y = batch\n        y_pred = self(x)\n        mse_loss = nn.functional.mse_loss(y_pred, y)\n        self.log(\"test_mse\", mse_loss)\n        self.log(\"test_mae\", self.mae(y_pred, y))\n        self.log(\"test_smse\", self.SMSE(y_pred, y))\n        return mse_loss\n\n    def configure_optimizers(self) -&gt; Optimizer:\n        \"\"\"\n        Configure the optimizer for the model.\n\n        :return: The optimizer for the model\n        :rtype: Optimizer\n\n        \"\"\"\n        match self.optimizer:\n            case \"Adam\":\n                return torch.optim.Adam(\n                    self.parameters(),\n                    lr=self.lr,\n                    weight_decay=self.L2_regularization_term,\n                )\n            case \"SGD\":\n                return torch.optim.SGD(\n                    self.parameters(),\n                    lr=self.lr,\n                    weight_decay=self.L2_regularization_term,\n                )\n            case \"RMSprop\":\n                return torch.optim.RMSprop(\n                    self.parameters(),\n                    lr=self.lr,\n                    weight_decay=self.L2_regularization_term,\n                )\n            case _:\n                raise ValueError(\"optimizer must be one of 'Adam', 'SGD', 'RMSprop'\")\n</code></pre>"},{"location":"ml_models/customizable_model/#yeastdnnexplorer.ml_models.customizable_model.CustomizableModel.__init__","title":"<code>__init__(input_dim, output_dim, lr=0.001, hidden_layer_num=1, hidden_layer_sizes=[128], activation='ReLU', optimizer='Adam', L2_regularization_term=0.0, dropout_rate=0.0)</code>","text":"<p>Constructor of CustomizableModel.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>The number of input features to our model, these are the binding effects for each transcription factor for a specific gene</p> required <code>output_dim</code> <code>int</code> <p>The number of output features of our model, this is the predicted gene expression value for each TF</p> required <code>lr</code> <code>float</code> <p>The learning rate for the optimizer</p> <code>0.001</code> <code>hidden_layer_num</code> <code>int</code> <p>The number of hidden layers in the model</p> <code>1</code> <code>hidden_layer_sizes</code> <code>list</code> <p>The size of each hidden layer in the model</p> <code>[128]</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If input_dim is not an integer</p> <code>TypeError</code> <p>If output_dim is not an integer</p> <code>TypeError</code> <p>If lr is not a positive float</p> <code>ValueError</code> <p>If input_dim or output_dim are not positive</p> Source code in <code>yeastdnnexplorer/ml_models/customizable_model.py</code> <pre><code>def __init__(\n    self,\n    input_dim: int,\n    output_dim: int,\n    lr: float = 0.001,\n    hidden_layer_num: int = 1,\n    hidden_layer_sizes: list = [128],\n    activation: str = \"ReLU\",  # can be \"ReLU\", \"Sigmoid\", \"Tanh\", \"LeakyReLU\"\n    optimizer: str = \"Adam\",  # can be \"Adam\", \"SGD\", \"RMSprop\"\n    L2_regularization_term: float = 0.0,\n    dropout_rate: float = 0.0,\n) -&gt; None:\n    \"\"\"\n    Constructor of CustomizableModel.\n\n    :param input_dim: The number of input features to our model, these are the\n        binding effects for each transcription factor for a specific gene\n    :type input_dim: int\n    :param output_dim: The number of output features of our model, this is the\n        predicted gene expression value for each TF\n    :type output_dim: int\n    :param lr: The learning rate for the optimizer\n    :type lr: float\n    :raises TypeError: If input_dim is not an integer\n    :raises TypeError: If output_dim is not an integer\n    :raises TypeError: If lr is not a positive float\n    :raises ValueError: If input_dim or output_dim are not positive\n    :param hidden_layer_num: The number of hidden layers in the model\n    :type hidden_layer_num: int\n    :param hidden_layer_sizes: The size of each hidden layer in the model\n    :type hidden_layer_sizes: list\n\n    \"\"\"\n    if not isinstance(input_dim, int):\n        raise TypeError(\"input_dim must be an integer\")\n    if not isinstance(output_dim, int):\n        raise TypeError(\"output_dim must be an integer\")\n    if not isinstance(lr, float) or lr &lt;= 0:\n        raise TypeError(\"lr must be a positive float\")\n    if input_dim &lt; 1 or output_dim &lt; 1:\n        raise ValueError(\"input_dim and output_dim must be positive integers\")\n    if not isinstance(hidden_layer_num, int):\n        raise TypeError(\"hidden_layer_num must be an integer\")\n    if not isinstance(hidden_layer_sizes, list) or not all(\n        isinstance(i, int) for i in hidden_layer_sizes\n    ):\n        raise TypeError(\"hidden_layer_sizes must be a list of integers\")\n    if len(hidden_layer_sizes) != hidden_layer_num:\n        raise ValueError(\n            \"hidden_layer_sizes must have length equal to hidden_layer_num\"\n        )\n    if not isinstance(activation, str) or activation not in [\n        \"ReLU\",\n        \"Sigmoid\",\n        \"Tanh\",\n        \"LeakyReLU\",\n    ]:\n        raise ValueError(\n            \"activation must be one of 'ReLU', 'Sigmoid', 'Tanh', 'LeakyReLU'\"\n        )\n    if not isinstance(optimizer, str) or optimizer not in [\n        \"Adam\",\n        \"SGD\",\n        \"RMSprop\",\n    ]:\n        raise ValueError(\"optimizer must be one of 'Adam', 'SGD', 'RMSprop'\")\n    if not isinstance(L2_regularization_term, float) or L2_regularization_term &lt; 0:\n        raise TypeError(\"L2_regularization_term must be a non-negative float\")\n    if not isinstance(dropout_rate, float) or dropout_rate &lt; 0 or dropout_rate &gt; 1:\n        raise TypeError(\"dropout_rate must be a float between 0 and 1\")\n\n    super().__init__()\n    self.input_dim = input_dim\n    self.output_dim = output_dim\n    self.lr = lr\n    self.hidden_layer_num = hidden_layer_num\n    self.hidden_layer_sizes = hidden_layer_sizes\n    self.optimizer = optimizer\n    self.L2_regularization_term = L2_regularization_term\n    self.save_hyperparameters()\n\n    match activation:\n        case \"ReLU\":\n            self.activation = nn.ReLU()\n        case \"Sigmoid\":\n            self.activation = nn.Sigmoid()\n        case \"Tanh\":\n            self.activation = nn.Tanh()\n        case \"LeakyReLU\":\n            self.activation = nn.LeakyReLU()\n        case _:\n            raise ValueError(\n                \"activation must be one of 'ReLU', 'Sigmoid', 'Tanh', 'LeakyReLU'\"\n            )\n\n    self.input_layer = nn.Linear(input_dim, hidden_layer_sizes[0])\n    self.hidden_layers = nn.ModuleList([])\n    for i in range(hidden_layer_num - 1):\n        self.hidden_layers.append(\n            nn.Linear(hidden_layer_sizes[i], hidden_layer_sizes[i + 1])\n        )\n    self.output_layer = nn.Linear(hidden_layer_sizes[-1], output_dim)\n\n    self.dropout = nn.Dropout(p=dropout_rate)\n\n    self.mae = MeanAbsoluteError()\n    self.SMSE = SMSE()\n</code></pre>"},{"location":"ml_models/customizable_model/#yeastdnnexplorer.ml_models.customizable_model.CustomizableModel.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure the optimizer for the model.</p> <p>Returns:</p> Type Description <code>Optimizer</code> <p>The optimizer for the model</p> Source code in <code>yeastdnnexplorer/ml_models/customizable_model.py</code> <pre><code>def configure_optimizers(self) -&gt; Optimizer:\n    \"\"\"\n    Configure the optimizer for the model.\n\n    :return: The optimizer for the model\n    :rtype: Optimizer\n\n    \"\"\"\n    match self.optimizer:\n        case \"Adam\":\n            return torch.optim.Adam(\n                self.parameters(),\n                lr=self.lr,\n                weight_decay=self.L2_regularization_term,\n            )\n        case \"SGD\":\n            return torch.optim.SGD(\n                self.parameters(),\n                lr=self.lr,\n                weight_decay=self.L2_regularization_term,\n            )\n        case \"RMSprop\":\n            return torch.optim.RMSprop(\n                self.parameters(),\n                lr=self.lr,\n                weight_decay=self.L2_regularization_term,\n            )\n        case _:\n            raise ValueError(\"optimizer must be one of 'Adam', 'SGD', 'RMSprop'\")\n</code></pre>"},{"location":"ml_models/customizable_model/#yeastdnnexplorer.ml_models.customizable_model.CustomizableModel.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the model (i.e. how predictions are made for a given input)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input data to the model (minus the target y values)</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The predicted y values for the input x, this is a tensor of shape (batch_size, output_dim)</p> Source code in <code>yeastdnnexplorer/ml_models/customizable_model.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the model (i.e. how predictions are made for a given input)\n\n    :param x: The input data to the model (minus the target y values)\n    :type x: torch.Tensor\n    :return: The predicted y values for the input x, this is a tensor of shape\n        (batch_size, output_dim)\n    :rtype: torch.Tensor\n\n    \"\"\"\n    x = self.dropout(self.activation(self.input_layer(x)))\n    for hidden_layer in self.hidden_layers:\n        x = self.dropout(self.activation(hidden_layer(x)))\n    x = self.output_layer(x)\n    return x\n</code></pre>"},{"location":"ml_models/customizable_model/#yeastdnnexplorer.ml_models.customizable_model.CustomizableModel.test_step","title":"<code>test_step(batch, batch_idx)</code>","text":"<p>Test step for the model, this is called for each batch of data during testing Testing is only performed after training and validation when we have chosen a final model We want to test our final model on unseen data (which is why we use validation sets to \u201ctest\u201d during training)</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The batch of data to test on (this will have size (batch_size, input_dim)</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The loss for the test batch</p> Source code in <code>yeastdnnexplorer/ml_models/customizable_model.py</code> <pre><code>def test_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Test step for the model, this is called for each batch of data during testing\n    Testing is only performed after training and validation when we have chosen a\n    final model We want to test our final model on unseen data (which is why we use\n    validation sets to \"test\" during training)\n\n    :param batch: The batch of data to test on (this will have size (batch_size,\n        input_dim)\n    :type batch: Any\n    :param batch_idx: The index of the batch\n    :type batch_idx: int\n    :return: The loss for the test batch\n    :rtype: torch.Tensor\n\n    \"\"\"\n    x, y = batch\n    y_pred = self(x)\n    mse_loss = nn.functional.mse_loss(y_pred, y)\n    self.log(\"test_mse\", mse_loss)\n    self.log(\"test_mae\", self.mae(y_pred, y))\n    self.log(\"test_smse\", self.SMSE(y_pred, y))\n    return mse_loss\n</code></pre>"},{"location":"ml_models/customizable_model/#yeastdnnexplorer.ml_models.customizable_model.CustomizableModel.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Training step for the model, this is called for each batch of data during training.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The batch of data to train on</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The loss for the training batch</p> Source code in <code>yeastdnnexplorer/ml_models/customizable_model.py</code> <pre><code>def training_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Training step for the model, this is called for each batch of data during\n    training.\n\n    :param batch: The batch of data to train on\n    :type batch: Any\n    :param batch_idx: The index of the batch\n    :type batch_idx: int\n    :return: The loss for the training batch\n    :rtype: torch.Tensor\n\n    \"\"\"\n    x, y = batch\n    y_pred = self(x)\n    mse_loss = nn.functional.mse_loss(y_pred, y)\n    self.log(\"train_mse\", mse_loss)\n    self.log(\"train_mae\", self.mae(y_pred, y))\n    self.log(\"train_smse\", self.SMSE(y_pred, y))\n    return mse_loss\n</code></pre>"},{"location":"ml_models/customizable_model/#yeastdnnexplorer.ml_models.customizable_model.CustomizableModel.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Validation step for the model, this is called for each batch of data during validation.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The batch of data to validate on</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The loss for the validation batch</p> Source code in <code>yeastdnnexplorer/ml_models/customizable_model.py</code> <pre><code>def validation_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Validation step for the model, this is called for each batch of data during\n    validation.\n\n    :param batch: The batch of data to validate on\n    :type batch: Any\n    :param batch_idx: The index of the batch\n    :type batch_idx: int\n    :return: The loss for the validation batch\n    :rtype: torch.Tensor\n\n    \"\"\"\n    x, y = batch\n    y_pred = self(x)\n    mse_loss = nn.functional.mse_loss(y_pred, y)\n    self.log(\"val_mse\", mse_loss)\n    self.log(\"val_mae\", self.mae(y_pred, y))\n    self.log(\"val_smse\", self.SMSE(y_pred, y))\n    return mse_loss\n</code></pre>"},{"location":"ml_models/metrics_compute_nrmse/","title":"Metrics compute nrmse","text":"<p>Compute the Normalized Root Mean Squared Error. This can be used to better compare models trained on different datasets with differnet scales, although it is not perfectly scale invariant.</p> <p>Parameters:</p> Name Type Description Default <code>y_pred</code> <code>torch.Tensor</code> <p>The predicted y values</p> required <code>y_true</code> <code>torch.Tensor</code> <p>The true y values</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The normalized root mean squared error</p> Source code in <code>yeastdnnexplorer/ml_models/metrics.py</code> <pre><code>def compute_nrmse(self, y_pred, y_true):\n    \"\"\"\n    Compute the Normalized Root Mean Squared Error. This can be used to better compare\n    models trained on different datasets with differnet scales, although it is not\n    perfectly scale invariant.\n\n    :param y_pred: The predicted y values\n    :type y_pred: torch.Tensor\n    :param y_true: The true y values\n    :type y_true: torch.Tensor\n    :return: The normalized root mean squared error\n    :rtype: torch.Tensor\n\n    \"\"\"\n    rmse = torch.sqrt(F.mse_loss(y_pred, y_true))\n\n    # normalize with the range of true y values\n    y_range = y_true.max() - y_true.min()\n    nrmse = rmse / y_range\n    return nrmse\n</code></pre>"},{"location":"ml_models/metrics_smse/","title":"Metrics smse","text":"<p>             Bases: <code>Metric</code></p> <p>A class for computing the standardized mean squared error (SMSE) metric.</p> <p>This metric is defined as the mean squared error divided by the variance of the true values (the target data). Because we are dividing by the variance of the true values, this metric is scale-independent and does not depend on the mean of the true values. It allows us to effectively compare models drawn from different datasets with differring scales or means (as long as their variances are at least relatively similar)</p> Source code in <code>yeastdnnexplorer/ml_models/metrics.py</code> <pre><code>class SMSE(Metric):\n    \"\"\"\n    A class for computing the standardized mean squared error (SMSE) metric.\n\n    This metric is defined as the mean squared error divided by the variance of the true\n    values (the target data). Because we are dividing by the variance of the true\n    values, this metric is scale-independent and does not depend on the mean of the true\n    values. It allows us to effectively compare models drawn from different datasets\n    with differring scales or means (as long as their variances are at least relatively\n    similar)\n\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the SMSE metric.\"\"\"\n        super().__init__()\n        self.add_state(\"mse\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n        self.add_state(\"variance\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n        self.add_state(\"num_samples\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n\n    def update(self, y_pred: torch.Tensor, y_true: torch.Tensor):\n        \"\"\"\n        Update the metric with new predictions and true values.\n\n        :param y_pred: The predicted y values\n        :type y_pred: torch.Tensor\n        :param y_true: The true y values\n        :type y_true: torch.Tensor\n\n        \"\"\"\n        self.mse += F.mse_loss(y_pred, y_true, reduction=\"sum\")\n        self.variance += torch.var(y_true, unbiased=False) * y_true.size(\n            0\n        )  # Total variance (TODO should we have unbiased=False here?)\n        self.num_samples += y_true.numel()\n\n    def compute(self):\n        \"\"\"\n        Compute the SMSE metric.\n\n        :return: The SMSE metric\n        :rtype: torch.Tensor\n\n        \"\"\"\n        mean_mse = self.mse / self.num_samples\n        mean_variance = self.variance / self.num_samples\n        return mean_mse / mean_variance\n</code></pre>"},{"location":"ml_models/metrics_smse/#yeastdnnexplorer.ml_models.metrics.SMSE.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the SMSE metric.</p> Source code in <code>yeastdnnexplorer/ml_models/metrics.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the SMSE metric.\"\"\"\n    super().__init__()\n    self.add_state(\"mse\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n    self.add_state(\"variance\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n    self.add_state(\"num_samples\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n</code></pre>"},{"location":"ml_models/metrics_smse/#yeastdnnexplorer.ml_models.metrics.SMSE.compute","title":"<code>compute()</code>","text":"<p>Compute the SMSE metric.</p> <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The SMSE metric</p> Source code in <code>yeastdnnexplorer/ml_models/metrics.py</code> <pre><code>def compute(self):\n    \"\"\"\n    Compute the SMSE metric.\n\n    :return: The SMSE metric\n    :rtype: torch.Tensor\n\n    \"\"\"\n    mean_mse = self.mse / self.num_samples\n    mean_variance = self.variance / self.num_samples\n    return mean_mse / mean_variance\n</code></pre>"},{"location":"ml_models/metrics_smse/#yeastdnnexplorer.ml_models.metrics.SMSE.update","title":"<code>update(y_pred, y_true)</code>","text":"<p>Update the metric with new predictions and true values.</p> <p>Parameters:</p> Name Type Description Default <code>y_pred</code> <code>Tensor</code> <p>The predicted y values</p> required <code>y_true</code> <code>Tensor</code> <p>The true y values</p> required Source code in <code>yeastdnnexplorer/ml_models/metrics.py</code> <pre><code>def update(self, y_pred: torch.Tensor, y_true: torch.Tensor):\n    \"\"\"\n    Update the metric with new predictions and true values.\n\n    :param y_pred: The predicted y values\n    :type y_pred: torch.Tensor\n    :param y_true: The true y values\n    :type y_true: torch.Tensor\n\n    \"\"\"\n    self.mse += F.mse_loss(y_pred, y_true, reduction=\"sum\")\n    self.variance += torch.var(y_true, unbiased=False) * y_true.size(\n        0\n    )  # Total variance (TODO should we have unbiased=False here?)\n    self.num_samples += y_true.numel()\n</code></pre>"},{"location":"ml_models/simple_model/","title":"Simple model","text":""},{"location":"ml_models/simple_model/#yeastdnnexplorer.ml_models.simple_model.SimpleModel","title":"<code>SimpleModel</code>","text":"<p>             Bases: <code>LightningModule</code></p> <p>A class for a simple linear model that takes in binding effects for each transcription factor and predicts gene expression values This class contains all of the logic for setup, training, validation, and testing of the model, as well as defining how data is passed through the model It is a subclass of pytorch_lightning.LightningModule, which is similar to a regular PyTorch nn.module but with added functionality for training and validation.</p> Source code in <code>yeastdnnexplorer/ml_models/simple_model.py</code> <pre><code>class SimpleModel(pl.LightningModule):\n    \"\"\"A class for a simple linear model that takes in binding effects for each\n    transcription factor and predicts gene expression values This class contains all of\n    the logic for setup, training, validation, and testing of the model, as well as\n    defining how data is passed through the model It is a subclass of\n    pytorch_lightning.LightningModule, which is similar to a regular PyTorch nn.module\n    but with added functionality for training and validation.\"\"\"\n\n    def __init__(self, input_dim: int, output_dim: int, lr: float = 0.001) -&gt; None:\n        \"\"\"\n        Constructor of SimpleModel.\n\n        :param input_dim: The number of input features to our model, these are the\n            binding effects for each transcription factor for a specific gene\n        :type input_dim: int\n        :param output_dim: The number of output features of our model, this is the\n            predicted gene expression value for each TF\n        :type output_dim: int\n        :param lr: The learning rate for the optimizer\n        :type lr: float\n        :raises TypeError: If input_dim is not an integer\n        :raises TypeError: If output_dim is not an integer\n        :raises TypeError: If lr is not a positive float\n        :raises ValueError: If input_dim or output_dim are not positive\n\n        \"\"\"\n        if not isinstance(input_dim, int):\n            raise TypeError(\"input_dim must be an integer\")\n        if not isinstance(output_dim, int):\n            raise TypeError(\"output_dim must be an integer\")\n        if not isinstance(lr, float) or lr &lt;= 0:\n            raise TypeError(\"lr must be a positive float\")\n        if input_dim &lt; 1 or output_dim &lt; 1:\n            raise ValueError(\"input_dim and output_dim must be positive integers\")\n\n        super().__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.lr = lr\n        self.save_hyperparameters()\n\n        self.mae = MeanAbsoluteError()\n        self.SMSE = SMSE()\n\n        # define layers for the model here\n        self.linear1 = nn.Linear(input_dim, output_dim)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the model (i.e. how predictions are made for a given input)\n\n        :param x: The input data to the model (minus the target y values)\n        :type x: torch.Tensor\n        :return: The predicted y values for the input x, this is a tensor of shape\n            (batch_size, output_dim)\n        :rtype: torch.Tensor\n\n        \"\"\"\n        return self.linear1(x)\n\n    def training_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Training step for the model, this is called for each batch of data during\n        training.\n\n        :param batch: The batch of data to train on\n        :type batch: Any\n        :param batch_idx: The index of the batch\n        :type batch_idx: int\n        :return: The loss for the training batch\n        :rtype: torch.Tensor\n\n        \"\"\"\n        x, y = batch\n        y_pred = self(x)\n        loss = nn.functional.mse_loss(y_pred, y)\n        self.log(\"train_mse\", loss)\n        self.log(\"train_mae\", self.mae(y_pred, y))\n        self.log(\"train_smse\", self.SMSE(y_pred, y))\n        return loss\n\n    def validation_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Validation step for the model, this is called for each batch of data during\n        validation.\n\n        :param batch: The batch of data to validate on\n        :type batch: Any\n        :param batch_idx: The index of the batch\n        :type batch_idx: int\n        :return: The loss for the validation batch\n        :rtype: torch.Tensor\n\n        \"\"\"\n        x, y = batch\n        y_pred = self(x)\n        loss = nn.functional.mse_loss(y_pred, y)\n\n        self.log(\"val_mse\", loss)\n        self.log(\"val_mae\", self.mae(y_pred, y))\n        self.log(\"val_smse\", self.SMSE(y_pred, y))\n        return loss\n\n    def test_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Test step for the model, this is called for each batch of data during testing\n        Testing is only performed after training and validation when we have chosen a\n        final model We want to test our final model on unseen data (which is why we use\n        validation sets to \"test\" during training)\n\n        :param batch: The batch of data to test on (this will have size (batch_size,\n            input_dim)\n        :type batch: Any\n        :param batch_idx: The index of the batch\n        :type batch_idx: int\n        :return: The loss for the test batch\n        :rtype: torch.Tensor\n\n        \"\"\"\n        x, y = batch\n        y_pred = self(x)\n        loss = nn.functional.mse_loss(y_pred, y)\n        self.log(\"test_mse\", loss)\n        self.log(\"test_mae\", self.mae(y_pred, y))\n        self.log(\"test_smse\", self.SMSE(y_pred, y))\n        return loss\n\n    def configure_optimizers(self) -&gt; Optimizer:\n        \"\"\"\n        Configure the optimizer for the model.\n\n        :return: The optimizer for the model\n        :rtype: Optimizer\n\n        \"\"\"\n        return torch.optim.Adam(self.parameters(), lr=self.lr)\n</code></pre>"},{"location":"ml_models/simple_model/#yeastdnnexplorer.ml_models.simple_model.SimpleModel.__init__","title":"<code>__init__(input_dim, output_dim, lr=0.001)</code>","text":"<p>Constructor of SimpleModel.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>The number of input features to our model, these are the binding effects for each transcription factor for a specific gene</p> required <code>output_dim</code> <code>int</code> <p>The number of output features of our model, this is the predicted gene expression value for each TF</p> required <code>lr</code> <code>float</code> <p>The learning rate for the optimizer</p> <code>0.001</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If input_dim is not an integer</p> <code>TypeError</code> <p>If output_dim is not an integer</p> <code>TypeError</code> <p>If lr is not a positive float</p> <code>ValueError</code> <p>If input_dim or output_dim are not positive</p> Source code in <code>yeastdnnexplorer/ml_models/simple_model.py</code> <pre><code>def __init__(self, input_dim: int, output_dim: int, lr: float = 0.001) -&gt; None:\n    \"\"\"\n    Constructor of SimpleModel.\n\n    :param input_dim: The number of input features to our model, these are the\n        binding effects for each transcription factor for a specific gene\n    :type input_dim: int\n    :param output_dim: The number of output features of our model, this is the\n        predicted gene expression value for each TF\n    :type output_dim: int\n    :param lr: The learning rate for the optimizer\n    :type lr: float\n    :raises TypeError: If input_dim is not an integer\n    :raises TypeError: If output_dim is not an integer\n    :raises TypeError: If lr is not a positive float\n    :raises ValueError: If input_dim or output_dim are not positive\n\n    \"\"\"\n    if not isinstance(input_dim, int):\n        raise TypeError(\"input_dim must be an integer\")\n    if not isinstance(output_dim, int):\n        raise TypeError(\"output_dim must be an integer\")\n    if not isinstance(lr, float) or lr &lt;= 0:\n        raise TypeError(\"lr must be a positive float\")\n    if input_dim &lt; 1 or output_dim &lt; 1:\n        raise ValueError(\"input_dim and output_dim must be positive integers\")\n\n    super().__init__()\n    self.input_dim = input_dim\n    self.output_dim = output_dim\n    self.lr = lr\n    self.save_hyperparameters()\n\n    self.mae = MeanAbsoluteError()\n    self.SMSE = SMSE()\n\n    # define layers for the model here\n    self.linear1 = nn.Linear(input_dim, output_dim)\n</code></pre>"},{"location":"ml_models/simple_model/#yeastdnnexplorer.ml_models.simple_model.SimpleModel.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure the optimizer for the model.</p> <p>Returns:</p> Type Description <code>Optimizer</code> <p>The optimizer for the model</p> Source code in <code>yeastdnnexplorer/ml_models/simple_model.py</code> <pre><code>def configure_optimizers(self) -&gt; Optimizer:\n    \"\"\"\n    Configure the optimizer for the model.\n\n    :return: The optimizer for the model\n    :rtype: Optimizer\n\n    \"\"\"\n    return torch.optim.Adam(self.parameters(), lr=self.lr)\n</code></pre>"},{"location":"ml_models/simple_model/#yeastdnnexplorer.ml_models.simple_model.SimpleModel.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the model (i.e. how predictions are made for a given input)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input data to the model (minus the target y values)</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The predicted y values for the input x, this is a tensor of shape (batch_size, output_dim)</p> Source code in <code>yeastdnnexplorer/ml_models/simple_model.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the model (i.e. how predictions are made for a given input)\n\n    :param x: The input data to the model (minus the target y values)\n    :type x: torch.Tensor\n    :return: The predicted y values for the input x, this is a tensor of shape\n        (batch_size, output_dim)\n    :rtype: torch.Tensor\n\n    \"\"\"\n    return self.linear1(x)\n</code></pre>"},{"location":"ml_models/simple_model/#yeastdnnexplorer.ml_models.simple_model.SimpleModel.test_step","title":"<code>test_step(batch, batch_idx)</code>","text":"<p>Test step for the model, this is called for each batch of data during testing Testing is only performed after training and validation when we have chosen a final model We want to test our final model on unseen data (which is why we use validation sets to \u201ctest\u201d during training)</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The batch of data to test on (this will have size (batch_size, input_dim)</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The loss for the test batch</p> Source code in <code>yeastdnnexplorer/ml_models/simple_model.py</code> <pre><code>def test_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Test step for the model, this is called for each batch of data during testing\n    Testing is only performed after training and validation when we have chosen a\n    final model We want to test our final model on unseen data (which is why we use\n    validation sets to \"test\" during training)\n\n    :param batch: The batch of data to test on (this will have size (batch_size,\n        input_dim)\n    :type batch: Any\n    :param batch_idx: The index of the batch\n    :type batch_idx: int\n    :return: The loss for the test batch\n    :rtype: torch.Tensor\n\n    \"\"\"\n    x, y = batch\n    y_pred = self(x)\n    loss = nn.functional.mse_loss(y_pred, y)\n    self.log(\"test_mse\", loss)\n    self.log(\"test_mae\", self.mae(y_pred, y))\n    self.log(\"test_smse\", self.SMSE(y_pred, y))\n    return loss\n</code></pre>"},{"location":"ml_models/simple_model/#yeastdnnexplorer.ml_models.simple_model.SimpleModel.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Training step for the model, this is called for each batch of data during training.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The batch of data to train on</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The loss for the training batch</p> Source code in <code>yeastdnnexplorer/ml_models/simple_model.py</code> <pre><code>def training_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Training step for the model, this is called for each batch of data during\n    training.\n\n    :param batch: The batch of data to train on\n    :type batch: Any\n    :param batch_idx: The index of the batch\n    :type batch_idx: int\n    :return: The loss for the training batch\n    :rtype: torch.Tensor\n\n    \"\"\"\n    x, y = batch\n    y_pred = self(x)\n    loss = nn.functional.mse_loss(y_pred, y)\n    self.log(\"train_mse\", loss)\n    self.log(\"train_mae\", self.mae(y_pred, y))\n    self.log(\"train_smse\", self.SMSE(y_pred, y))\n    return loss\n</code></pre>"},{"location":"ml_models/simple_model/#yeastdnnexplorer.ml_models.simple_model.SimpleModel.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Validation step for the model, this is called for each batch of data during validation.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The batch of data to validate on</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The loss for the validation batch</p> Source code in <code>yeastdnnexplorer/ml_models/simple_model.py</code> <pre><code>def validation_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Validation step for the model, this is called for each batch of data during\n    validation.\n\n    :param batch: The batch of data to validate on\n    :type batch: Any\n    :param batch_idx: The index of the batch\n    :type batch_idx: int\n    :return: The loss for the validation batch\n    :rtype: torch.Tensor\n\n    \"\"\"\n    x, y = batch\n    y_pred = self(x)\n    loss = nn.functional.mse_loss(y_pred, y)\n\n    self.log(\"val_mse\", loss)\n    self.log(\"val_mae\", self.mae(y_pred, y))\n    self.log(\"val_smse\", self.SMSE(y_pred, y))\n    return loss\n</code></pre>"},{"location":"probability_models/GenePopulation/","title":"GenePopulation","text":"<p>A simple class to hold a tensor boolean 1D vector where 0 is meant to identify genes which are unaffected by a given TF and 1 is meant to identify genes which are affected by a given TF.</p> Source code in <code>yeastdnnexplorer/probability_models/generate_data.py</code> <pre><code>class GenePopulation:\n    \"\"\"A simple class to hold a tensor boolean 1D vector where 0 is meant to identify\n    genes which are unaffected by a given TF and 1 is meant to identify genes which are\n    affected by a given TF.\"\"\"\n\n    def __init__(self, labels: torch.Tensor) -&gt; None:\n        \"\"\"\n        Constructor of GenePopulation.\n\n        :param labels: This can be any 1D tensor of boolean values. But it is meant to\n            be the output of `generate_gene_population()`\n        :type labels: torch.Tensor\n        :raises TypeError: If labels is not a tensor\n        :raises ValueError: If labels is not a 1D tensor\n        :raises TypeError: If labels is not a boolean tensor\n\n        \"\"\"\n        if not isinstance(labels, torch.Tensor):\n            raise TypeError(\"labels must be a tensor\")\n        if not labels.ndim == 1:\n            raise ValueError(\"labels must be a 1D tensor\")\n        if not labels.dtype == torch.bool:\n            raise TypeError(\"labels must be a boolean tensor\")\n        self.labels = labels\n\n    def __repr__(self):\n        return f\"&lt;GenePopulation size={len(self.labels)}&gt;\"\n</code></pre>"},{"location":"probability_models/GenePopulation/#yeastdnnexplorer.probability_models.generate_data.GenePopulation.__init__","title":"<code>__init__(labels)</code>","text":"<p>Constructor of GenePopulation.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>Tensor</code> <p>This can be any 1D tensor of boolean values. But it is meant to be the output of <code>generate_gene_population()</code></p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If labels is not a tensor</p> <code>ValueError</code> <p>If labels is not a 1D tensor</p> <code>TypeError</code> <p>If labels is not a boolean tensor</p> Source code in <code>yeastdnnexplorer/probability_models/generate_data.py</code> <pre><code>def __init__(self, labels: torch.Tensor) -&gt; None:\n    \"\"\"\n    Constructor of GenePopulation.\n\n    :param labels: This can be any 1D tensor of boolean values. But it is meant to\n        be the output of `generate_gene_population()`\n    :type labels: torch.Tensor\n    :raises TypeError: If labels is not a tensor\n    :raises ValueError: If labels is not a 1D tensor\n    :raises TypeError: If labels is not a boolean tensor\n\n    \"\"\"\n    if not isinstance(labels, torch.Tensor):\n        raise TypeError(\"labels must be a tensor\")\n    if not labels.ndim == 1:\n        raise ValueError(\"labels must be a 1D tensor\")\n    if not labels.dtype == torch.bool:\n        raise TypeError(\"labels must be a boolean tensor\")\n    self.labels = labels\n</code></pre>"},{"location":"probability_models/default_perturbation_effect_adjustment_function/","title":"Probability Models","text":"<p>Default function to adjust the mean of the perturbation effect based on the enrichment score.</p> <p>All functions that are passed to generate_perturbation_effects() in the argument adjustment_function must have the same signature as this function.</p> <p>Parameters:</p> Name Type Description Default <code>binding_enrichment_data</code> <code>Tensor</code> <p>A tensor of enrichment scores for each gene with dimensions [n_genes, n_tfs, 3] where the entries in the third dimension are a matrix with columns [label, enrichment, pvalue].</p> required <code>bound_mean</code> <code>float</code> <p>The mean for bound genes.</p> required <code>unbound_mean</code> <code>float</code> <p>The mean for unbound genes.</p> required <code>max_adjustment</code> <code>float</code> <p>The maximum adjustment to the base mean based on enrichment.</p> required <code>tf_relationships</code> <code>dict[int, list[int]], optional</code> <p>Unused in this function. It is only here to match the signature of the other adjustment functions.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Adjusted mean as a tensor.</p> Source code in <code>yeastdnnexplorer/probability_models/generate_data.py</code> <pre><code>def default_perturbation_effect_adjustment_function(\n    binding_enrichment_data: torch.Tensor,\n    bound_mean: float,\n    unbound_mean: float,\n    max_adjustment: float,\n    **kwargs,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Default function to adjust the mean of the perturbation effect based on the\n    enrichment score.\n\n    All functions that are passed to generate_perturbation_effects() in the argument\n    adjustment_function must have the same signature as this function.\n\n    :param binding_enrichment_data: A tensor of enrichment scores for each gene with\n        dimensions [n_genes, n_tfs, 3] where the entries in the third dimension are a\n        matrix with columns [label, enrichment, pvalue].\n    :type binding_enrichment_data: torch.Tensor\n    :param bound_mean: The mean for bound genes.\n    :type bound_mean: float\n    :param unbound_mean: The mean for unbound genes.\n    :type unbound_mean: float\n    :param max_adjustment: The maximum adjustment to the base mean based on enrichment.\n    :type max_adjustment: float\n    :param tf_relationships: Unused in this function. It is only here to match the\n        signature of the other adjustment functions.\n    :type tf_relationships: dict[int, list[int]], optional\n    :return: Adjusted mean as a tensor.\n    :rtype: torch.Tensor\n\n    \"\"\"\n    # Extract bound/unbound labels and enrichment scores\n    bound_labels = binding_enrichment_data[:, :, 0]\n    enrichment_scores = binding_enrichment_data[:, :, 1]\n\n    adjusted_mean_matrix = torch.where(\n        bound_labels == 1, enrichment_scores, torch.zeros_like(enrichment_scores)\n    )\n\n    for gene_idx in range(bound_labels.shape[0]):\n        for tf_index in range(bound_labels.shape[1]):\n            if bound_labels[gene_idx, tf_index] == 1:\n                # divide its enrichment score by the maximum magnitude possible to\n                # create an adjustment multipler that scales with increasing enrichment\n                adjustment_multiplier = enrichment_scores[gene_idx, tf_index] / abs(\n                    enrichment_scores.max()\n                )\n\n                # randomly adjust the gene by some portion of the max adjustment\n                adjusted_mean_matrix[gene_idx, tf_index] = bound_mean + (\n                    adjustment_multiplier * max_adjustment\n                )\n            else:\n                # related tfs are not all bound, so set the enrichment\n                # score to unbound mean\n                adjusted_mean_matrix[gene_idx, tf_index] = unbound_mean\n\n    return adjusted_mean_matrix\n</code></pre>"},{"location":"probability_models/generate_binding_effects/","title":"Generate binding effects","text":"<p>Generate enrichment effects for genes using vectorized operations, based on their bound designation, with separate experiment hops ranges for unbound and bound genes.</p> <p>Note that the default values are a scaled down version of actual data. See also https://github.com/cmatKhan/callingCardsTools/blob/main/callingcardstools/PeakCalling/yeast/enrichment.py</p> <p>Parameters:</p> Name Type Description Default <code>gene_population</code> <code>GenePopulation</code> <p>A GenePopulation object. See <code>generate_gene_population()</code></p> required <code>background_hops_range</code> <code>tuple[int, int]</code> <p>The range of hops for background genes. Defaults to (1, 100)</p> <code>(1, 100)</code> <code>unbound_experiment_hops_range</code> <code>tuple[int, int]</code> <p>The range of hops for unbound genes. Defaults to (0, 1)</p> <code>(0, 1)</code> <code>bound_experiment_hops_range</code> <code>tuple[int, int]</code> <p>The range of hops for bound genes. Defaults to (1, 6)</p> <code>(1, 6)</code> <code>total_background_hops</code> <code>int</code> <p>The total number of background hops. Defaults to 1000</p> <code>1000</code> <code>total_experiment_hops</code> <code>int</code> <p>The total number of experiment hops. Defaults to 76</p> <code>76</code> <code>pseudocount</code> <code>float</code> <p>A pseudocount to avoid division by zero. Defaults to 1e-10</p> <code>1e-10</code> <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>A tensor of enrichment values for each gene.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If gene_population is not a GenePopulation object</p> <code>TypeError</code> <p>If total_background_hops is not an integer</p> <code>TypeError</code> <p>If total_experiment_hops is not an integer</p> <code>TypeError</code> <p>If pseudocount is not a float</p> <code>TypeError</code> <p>If background_hops_range is not a tuple</p> <code>TypeError</code> <p>If unbound_experiment_hops_range is not a tuple</p> <code>TypeError</code> <p>If bound_experiment_hops_range is not a tuple</p> <code>ValueError</code> <p>If background_hops_range is not a tuple of length 2</p> <code>ValueError</code> <p>If unbound_experiment_hops_range is not a tuple of length 2</p> <code>ValueError</code> <p>If bound_experiment_hops_range is not a tuple of length 2</p> Source code in <code>yeastdnnexplorer/probability_models/generate_data.py</code> <pre><code>def generate_binding_effects(\n    gene_population: GenePopulation,\n    background_hops_range: tuple[int, int] = (1, 100),\n    unbound_experiment_hops_range: tuple[int, int] = (0, 1),\n    bound_experiment_hops_range: tuple[int, int] = (1, 6),\n    total_background_hops: int = 1000,\n    total_experiment_hops: int = 76,\n    pseudocount: float = 1e-10,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Generate enrichment effects for genes using vectorized operations, based on their\n    bound designation, with separate experiment hops ranges for unbound and bound genes.\n\n    Note that the default values are a scaled down version of actual data. See also\n    https://github.com/cmatKhan/callingCardsTools/blob/main/callingcardstools/PeakCalling/yeast/enrichment.py\n\n    :param gene_population: A GenePopulation object. See `generate_gene_population()`\n    :type gene_population: GenePopulation\n    :param background_hops_range: The range of hops for background genes. Defaults to\n        (1, 100)\n    :type background_hops_range: tuple[int, int], optional\n    :param unbound_experiment_hops_range: The range of hops for unbound genes. Defaults\n        to (0, 1)\n    :type unbound_experiment_hops_range: tuple[int, int], optional\n    :param bound_experiment_hops_range: The range of hops for bound genes. Defaults to\n        (1, 6)\n    :type bound_experiment_hops_range: tuple[int, int], optional\n    :param total_background_hops: The total number of background hops. Defaults to 1000\n    :type total_background_hops: int, optional\n    :param total_experiment_hops: The total number of experiment hops. Defaults to 76\n    :type total_experiment_hops: int, optional\n    :param pseudocount: A pseudocount to avoid division by zero. Defaults to 1e-10\n    :type pseudocount: float, optional\n    :return: A tensor of enrichment values for each gene.\n    :rtype: torch.Tensor\n    :raises TypeError: If gene_population is not a GenePopulation object\n    :raises TypeError: If total_background_hops is not an integer\n    :raises TypeError: If total_experiment_hops is not an integer\n    :raises TypeError: If pseudocount is not a float\n    :raises TypeError: If background_hops_range is not a tuple\n    :raises TypeError: If unbound_experiment_hops_range is not a tuple\n    :raises TypeError: If bound_experiment_hops_range is not a tuple\n    :raises ValueError: If background_hops_range is not a tuple of length 2\n    :raises ValueError: If unbound_experiment_hops_range is not a tuple of length 2\n    :raises ValueError: If bound_experiment_hops_range is not a tuple of length 2\n\n    \"\"\"\n    # NOTE: torch intervals are half open on the right, so we add 1 to the\n    # high end of the range to make it inclusive\n\n    # check input\n    if not isinstance(gene_population, GenePopulation):\n        raise TypeError(\"gene_population must be a GenePopulation object\")\n    if not isinstance(total_background_hops, int):\n        raise TypeError(\"total_background_hops must be an integer\")\n    if not isinstance(total_experiment_hops, int):\n        raise TypeError(\"total_experiment_hops must be an integer\")\n    if not isinstance(pseudocount, float):\n        raise TypeError(\"pseudocount must be a float\")\n    for arg, tup in {\n        \"background_hops_range\": background_hops_range,\n        \"unbound_experiment_hops_range\": unbound_experiment_hops_range,\n        \"bound_experiment_hops_range\": bound_experiment_hops_range,\n    }.items():\n        if not isinstance(tup, tuple):\n            raise TypeError(f\"{arg} must be a tuple\")\n        if not len(tup) == 2:\n            raise ValueError(f\"{arg} must be a tuple of length 2\")\n        if not all(isinstance(i, int) for i in tup):\n            raise TypeError(f\"{arg} must be a tuple of integers\")\n\n    # Generate background hops for all genes\n    background_hops = torch.randint(\n        low=background_hops_range[0],\n        high=background_hops_range[1] + 1,\n        size=(gene_population.labels.shape[0],),\n    )\n\n    # Generate experiment hops unbound genes\n    unbound_experiment_hops = torch.randint(\n        low=unbound_experiment_hops_range[0],\n        high=unbound_experiment_hops_range[1] + 1,\n        size=(gene_population.labels.shape[0],),\n    )\n    # Generate experiment hops bound genes\n    bound_experiment_hops = torch.randint(\n        low=bound_experiment_hops_range[0],\n        high=bound_experiment_hops_range[1] + 1,\n        size=(gene_population.labels.shape[0],),\n    )\n\n    # Use bound designation to select appropriate experiment hops\n    experiment_hops = torch.where(\n        gene_population.labels == 1, bound_experiment_hops, unbound_experiment_hops\n    )\n\n    # Calculate enrichment for all genes\n    return (experiment_hops.float() / (total_experiment_hops + pseudocount)) / (\n        (background_hops.float() / (total_background_hops + pseudocount)) + pseudocount\n    )\n</code></pre>"},{"location":"probability_models/generate_gene_population/","title":"Generate Gene Population","text":"<p>Generate two sets of genes, one of which will be considered genes which show a bound, and the other which does not. The return is a one dimensional boolean tensor where a value of \u20180\u2019 means that the gene at that index is part of the unbound group and a \u20181\u2019 means the gene at that index is part of the bound group. The length of the tensor is the number of genes in this simulated organism.</p> <p>Parameters:</p> Name Type Description Default <code>total</code> <code>int</code> <p>The total number of genes. defaults to 1000</p> <code>1000</code> <code>bound_group</code> <code>float</code> <p>The proportion of genes in the bound group. defaults to 0.3</p> <code>0.3</code> <p>Returns:</p> Type Description <code>GenePopulation</code> <p>A one dimensional tensor of boolean values where the set of indices with a value of \u20181\u2019 are the bound group and the set of indices with a value of \u20180\u2019 are the unbound group.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>if total is not an integer</p> <code>ValueError</code> <p>If bound_group is not between 0 and 1</p> Source code in <code>yeastdnnexplorer/probability_models/generate_data.py</code> <pre><code>def generate_gene_population(\n    total: int = 1000, bound_group: float = 0.3\n) -&gt; GenePopulation:\n    \"\"\"\n    Generate two sets of genes, one of which will be considered genes which show a\n    bound, and the other which does not. The return is a one dimensional boolean tensor\n    where a value of '0' means that the gene at that index is part of the unbound group\n    and a '1' means the gene at that index is part of the bound group. The length of the\n    tensor is the number of genes in this simulated organism.\n\n    :param total: The total number of genes. defaults to 1000\n    :type total: int, optional\n    :param bound_group: The proportion of genes in the bound group. defaults to 0.3\n    :type bound_group: float, optional\n    :return: A one dimensional tensor of boolean values where the set of indices with a\n        value of '1' are the bound group and the set of indices with a value of '0' are\n        the unbound group.\n    :rtype: GenePopulation\n    :raises TypeError: if total is not an integer\n    :raises ValueError: If bound_group is not between 0 and 1\n\n    \"\"\"\n    if not isinstance(total, int):\n        raise TypeError(\"total must be an integer\")\n    if not 0 &lt;= bound_group &lt;= 1:\n        raise ValueError(\"bound_group must be between 0 and 1\")\n\n    bound_group_size = int(total * bound_group)\n    logger.info(\"Generating %s genes with bound\", bound_group_size)\n\n    labels = torch.cat(\n        (\n            torch.ones(bound_group_size, dtype=torch.bool),\n            torch.zeros(total - bound_group_size, dtype=torch.bool),\n        )\n    )[torch.randperm(total)]\n\n    return GenePopulation(labels)\n</code></pre>"},{"location":"probability_models/generate_perturbation_effects/","title":"Generate perturbation effects","text":"<p>Generate perturbation effects for genes.</p> <p>If <code>max_mean_adjustment</code> is greater than 0, then the mean of the effects are adjusted based on the binding_data and the function passed in <code>adjustment_function</code>. See <code>default_perturbation_effect_adjustment_function()</code> for the default option. If <code>max_mean_adjustment</code> is 0, then the mean is not adjusted. Additional keyword arguments may be passed in that will be passed along to the adjustment function.</p> <p>Parameters:</p> Name Type Description Default <code>binding_data</code> <code>Tensor</code> <p>A tensor of binding data with dimensions [n_genes, n_tfs, 3] where the entries in the third dimension are a matrix with columns [label, enrichment, pvalue].</p> required <code>tf_index</code> <code>int | None</code> <p>The index of the TF in the binding_data tensor. Not used if we are adjusting the means (ie only used if max_mean_adjustment == 0). Defaults to None</p> <code>None</code> <code>unbound_mean</code> <code>float</code> <p>The mean for unbound genes. Defaults to 0.0</p> <code>0.0</code> <code>unbound_std</code> <code>float</code> <p>The standard deviation for unbound genes. Defaults to 1.0</p> <code>1.0</code> <code>bound_mean</code> <code>float</code> <p>The mean for bound genes. Defaults to 3.0</p> <code>3.0</code> <code>bound_std</code> <code>float</code> <p>The standard deviation for bound genes. Defaults to 1.0</p> <code>1.0</code> <code>max_mean_adjustment</code> <code>float</code> <p>The maximum adjustment to the base mean based on enrichment. Defaults to 0.0</p> <code>0.0</code> <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>A tensor of perturbation effects for each gene.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If binding_data is not a 3D tensor with the third dimension having a length of 3</p> <code>ValueError</code> <p>If unbound_mean, unbound_std, bound_mean, bound_std, or max_mean_adjustment are not floats</p> Source code in <code>yeastdnnexplorer/probability_models/generate_data.py</code> <pre><code>def generate_perturbation_effects(\n    binding_data: torch.Tensor,\n    tf_index: int | None = None,\n    unbound_mean: float = 0.0,\n    unbound_std: float = 1.0,\n    bound_mean: float = 3.0,\n    bound_std: float = 1.0,\n    max_mean_adjustment: float = 0.0,\n    adjustment_function: Callable[\n        [torch.Tensor, float, float, float], torch.Tensor\n    ] = default_perturbation_effect_adjustment_function,\n    **kwargs,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Generate perturbation effects for genes.\n\n    If `max_mean_adjustment` is greater than 0, then the mean of the\n    effects are adjusted based on the binding_data and the function passed\n    in `adjustment_function`. See `default_perturbation_effect_adjustment_function()`\n    for the default option. If `max_mean_adjustment` is 0, then the mean\n    is not adjusted. Additional keyword arguments may be passed in that will be\n    passed along to the adjustment function.\n\n    :param binding_data: A tensor of binding data with dimensions [n_genes, n_tfs, 3]\n        where the entries in the third dimension are a matrix with columns\n        [label, enrichment, pvalue].\n    :type binding_data: torch.Tensor\n    :param tf_index: The index of the TF in the binding_data tensor. Not used if we\n        are adjusting the means (ie only used if max_mean_adjustment == 0).\n        Defaults to None\n    :type tf_index: int\n    :param unbound_mean: The mean for unbound genes. Defaults to 0.0\n    :type unbound_mean: float, optional\n    :param unbound_std: The standard deviation for unbound genes. Defaults to 1.0\n    :type unbound_std: float, optional\n    :param bound_mean: The mean for bound genes. Defaults to 3.0\n    :type bound_mean: float, optional\n    :param bound_std: The standard deviation for bound genes. Defaults to 1.0\n    :type bound_std: float, optional\n    :param max_mean_adjustment: The maximum adjustment to the base mean based\n        on enrichment. Defaults to 0.0\n    :type max_mean_adjustment: float, optional\n\n    :return: A tensor of perturbation effects for each gene.\n    :rtype: torch.Tensor\n\n    :raises ValueError: If binding_data is not a 3D tensor with the third\n        dimension having a length of 3\n    :raises ValueError: If unbound_mean, unbound_std, bound_mean, bound_std,\n        or max_mean_adjustment are not floats\n\n    \"\"\"\n    # check that a valid combination of inputs has been passed in\n    if max_mean_adjustment == 0.0 and tf_index is None:\n        raise ValueError(\"If max_mean_adjustment is 0, then tf_index must be specified\")\n\n    if binding_data.ndim != 3 or binding_data.shape[2] != 3:\n        raise ValueError(\n            \"enrichment_tensor must have dimensions [num_genes, num_TFs, \"\n            \"[label, enrichment, pvalue]]\"\n        )\n    # check the rest of the inputs\n    if not all(\n        isinstance(i, float)\n        for i in (unbound_mean, unbound_std, bound_mean, bound_std, max_mean_adjustment)\n    ):\n        raise ValueError(\n            \"unbound_mean, unbound_std, bound_mean, bound_std, \"\n            \"and max_mean_adjustment must be floats\"\n        )\n    # check the Callable signature\n    if not all(\n        i in inspect.signature(adjustment_function).parameters\n        for i in (\n            \"binding_enrichment_data\",\n            \"bound_mean\",\n            \"unbound_mean\",\n            \"max_adjustment\",\n        )\n    ):\n        raise ValueError(\n            \"adjustment_function must have the signature \"\n            \"(binding_enrichment_data, bound_mean, unbound_mean, max_adjustment)\"\n        )\n\n    # Initialize an effects tensor for all genes\n    effects = torch.empty(\n        binding_data.size(0), dtype=torch.float32, device=binding_data.device\n    )\n\n    # Randomly assign signs for each gene\n    # fmt: off\n    signs = torch.randint(0, 2, (effects.size(0),),\n                          dtype=torch.float32,\n                          device=binding_data.device) * 2 - 1\n    # fmt: on\n\n    # Apply adjustments to the base mean for the bound genes, if necessary\n    if max_mean_adjustment &gt; 0 and adjustment_function is not None:\n        # Assuming adjustment_function returns a vector of means for each gene.\n        # bound genes that meet the criteria for adjustment will be affected by\n        # the status of the TFs. What TFs affect a given gene must be specified by\n        # the adjustment_function()\n        adjusted_means = adjustment_function(\n            binding_data,\n            bound_mean,\n            unbound_mean,\n            max_mean_adjustment,\n            **kwargs,\n        )\n\n        # add adjustments, ensuring they respect the original sign\n        if adjusted_means.ndim == 1:\n            effects = signs * torch.abs(\n                torch.normal(mean=adjusted_means, std=bound_std)\n            )\n        else:\n            effects = torch.zeros_like(adjusted_means)\n            for col_idx in range(effects.size(1)):\n                effects[:, col_idx] = signs * torch.abs(\n                    torch.normal(mean=adjusted_means[:, col_idx], std=bound_std)\n                )\n    else:\n        bound_mask = binding_data[:, tf_index, 0] == 1\n\n        # Generate effects based on the unbound and bound means, applying the sign\n        effects[~bound_mask] = signs[~bound_mask] * torch.abs(\n            torch.normal(\n                mean=unbound_mean, std=unbound_std, size=(torch.sum(~bound_mask),)\n            )\n        )\n        effects[bound_mask] = signs[bound_mask] * torch.abs(\n            torch.normal(mean=bound_mean, std=bound_std, size=(torch.sum(bound_mask),))\n        )\n\n    return effects\n</code></pre>"},{"location":"probability_models/generate_pvalues/","title":"Generate pvalues","text":"<p>Generate p-values for genes where larger effects are less likely to be false positives.</p> <p>Parameters:</p> Name Type Description Default <code>effects</code> <code>Tensor</code> <p>A tensor of effects</p> required <code>large_effect_percentile</code> <code>float</code> <p>The percentile of effects that are considered large effects. Defaults to 0.9</p> <code>0.9</code> <code>large_effect_upper_pval</code> <code>float</code> <p>The upper bound of the p-values for large effects. Defaults to 0.2</p> <code>0.2</code> <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>A tensor of p-values</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If effects is not a tensor or the values themselves are not numeric</p> <code>ValueError</code> <p>If large_effect_percentile is not between 0 and 1</p> <code>ValueError</code> <p>If large_effect_upper_pval is not between 0 and 1</p> Source code in <code>yeastdnnexplorer/probability_models/generate_data.py</code> <pre><code>def generate_pvalues(\n    effects: torch.Tensor,\n    large_effect_percentile: float = 0.9,\n    large_effect_upper_pval: float = 0.2,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Generate p-values for genes where larger effects are less likely to be false\n    positives.\n\n    :param effects: A tensor of effects\n    :type effects: torch.Tensor\n    :param large_effect_percentile: The percentile of effects that are considered large\n        effects. Defaults to 0.9\n    :type large_effect_percentile: float, optional\n    :param large_effect_upper_pval: The upper bound of the p-values for large effects.\n        Defaults to 0.2\n    :return: A tensor of p-values\n    :rtype: torch.Tensor\n    :raises ValueError: If effects is not a tensor or the values themselves are not\n        numeric\n    :raises ValueError: If large_effect_percentile is not between 0 and 1\n    :raises ValueError: If large_effect_upper_pval is not between 0 and 1\n\n    \"\"\"\n    # check inputs\n    if not isinstance(effects, torch.Tensor):\n        raise ValueError(\"effects must be a tensor\")\n    if not torch.is_floating_point(effects):\n        raise ValueError(\"effects must be numeric\")\n    if not 0 &lt;= large_effect_percentile &lt;= 1:\n        raise ValueError(\"large_effect_percentile must be between 0 and 1\")\n    if not 0 &lt;= large_effect_upper_pval &lt;= 1:\n        raise ValueError(\"large_effect_upper_pval must be between 0 and 1\")\n\n    # Generate p-values\n    pvalues = torch.rand(effects.shape[0])\n\n    # Draw p-values from a uniform distribution where larger abs(effects) are\n    # less likely to be false positives\n    large_effect_threshold = torch.quantile(torch.abs(effects), large_effect_percentile)\n    large_effect_mask = torch.abs(effects) &gt;= large_effect_threshold\n    pvalues[large_effect_mask] = (\n        torch.rand(torch.sum(large_effect_mask)) * large_effect_upper_pval\n    )\n\n    return pvalues\n</code></pre>"},{"location":"probability_models/perturbation_effect_adjustment_function_with_tf_relationships/","title":"Perturbation effect adjustment function with tf relationships","text":"<p>Adjust the mean of the perturbation effect based on the enrichment score and the provided relationships between TFs. For each gene, the mean of the TF-gene pair\u2019s perturbation effect will be adjusted if the TF is bound to the gene and all related TFs are also bound to the gene. The adjustment will be a random value not exceeding the maximum adjustment.</p> <p>Parameters:</p> Name Type Description Default <code>binding_enrichment_data</code> <code>Tensor</code> <p>A tensor of enrichment scores for each gene with dimensions [n_genes, n_tfs, 3] where the entries in the third dimension are a matrix with columns [label, enrichment, pvalue].</p> required <code>bound_mean</code> <code>float</code> <p>The mean for bound genes.</p> required <code>unbound_mean</code> <code>float</code> <p>The mean for unbound genes.</p> required <code>max_adjustment</code> <code>float</code> <p>The maximum adjustment to the base mean based on enrichment.</p> required <code>tf_relationships</code> <code>dict[int, list[int]]</code> <p>A dictionary where the keys are the indices of the TFs and the values are lists of indices of other TFs that are related to the key TF.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Adjusted mean as a tensor.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If tf_relationships is not a dictionary between ints and lists of ints</p> <code>ValueError</code> <p>If the tf_relationships dict does not have the same number of TFs as the binding_data tensor passed into the function</p> <code>ValueError</code> <p>If the tf_relationships dict has any TFs in the values that are not also in the keys or any key or value TFs that are out of bounds for the binding_data tensor</p> Source code in <code>yeastdnnexplorer/probability_models/generate_data.py</code> <pre><code>def perturbation_effect_adjustment_function_with_tf_relationships(\n    binding_enrichment_data: torch.Tensor,\n    bound_mean: float,\n    unbound_mean: float,\n    max_adjustment: float,\n    tf_relationships: dict[int, list[int]],\n) -&gt; torch.Tensor:\n    \"\"\"\n    Adjust the mean of the perturbation effect based on the enrichment score and the\n    provided relationships between TFs. For each gene, the mean of the TF-gene pair's\n    perturbation effect will be adjusted if the TF is bound to the gene and all related\n    TFs are also bound to the gene. The adjustment will be a random value not exceeding\n    the maximum adjustment.\n\n    :param binding_enrichment_data: A tensor of enrichment scores for each gene with\n        dimensions [n_genes, n_tfs, 3] where the entries in the third dimension are a\n        matrix with columns [label, enrichment, pvalue].\n    :type binding_enrichment_data: torch.Tensor\n    :param bound_mean: The mean for bound genes.\n    :type bound_mean: float\n    :param unbound_mean: The mean for unbound genes.\n    :type unbound_mean: float\n    :param max_adjustment: The maximum adjustment to the base mean based on enrichment.\n    :type max_adjustment: float\n    :param tf_relationships: A dictionary where the keys are the indices of the TFs and\n        the values are lists of indices of other TFs that are related to the key TF.\n    :type tf_relationships: dict[int, list[int]]\n    :return: Adjusted mean as a tensor.\n    :rtype: torch.Tensor\n    :raises ValueError: If tf_relationships is not a dictionary between ints and lists\n        of ints\n    :raises ValueError: If the tf_relationships dict does not have the same number of\n        TFs as the binding_data tensor passed into the function\n    :raises ValueError: If the tf_relationships dict has any TFs in the values that are\n        not also in the keys or any key or value TFs that are out of bounds for the\n        binding_data tensor\n\n    \"\"\"\n    if (\n        not isinstance(tf_relationships, dict)\n        or not all(isinstance(v, list) for v in tf_relationships.values())\n        or not all(isinstance(k, int) for k in tf_relationships.keys())\n        or not all(isinstance(i, int) for v in tf_relationships.values() for i in v)\n    ):\n        raise ValueError(\n            \"tf_relationships must be a dictionary between ints and lists of ints\"\n        )\n    if not all(\n        k in range(binding_enrichment_data.shape[1]) for k in tf_relationships.keys()\n    ) or not all(\n        i in range(binding_enrichment_data.shape[1])\n        for v in tf_relationships.values()\n        for i in v\n    ):\n        raise ValueError(\n            \"all keys and values in tf_relationships must be within the \\\n                  bounds of the binding_data tensor's number of TFs\"\n        )\n    if not len(tf_relationships) == binding_enrichment_data.shape[1]:\n        raise ValueError(\n            \"tf_relationships must have the same number of TFs as the \\\n                binding_data tensor passed into the function\"\n        )\n\n    # Extract bound/unbound labels and enrichment scores\n    bound_labels = binding_enrichment_data[:, :, 0]  # shape: (num_genes, num_tfs)\n    enrichment_scores = binding_enrichment_data[:, :, 1]  # shape: (num_genes, num_tfs)\n\n    # we set all unbound scores to 0, then we will go through and also\n    # set any bound scores to unbound_mean if the related tfs are not also bound\n    adjusted_mean_matrix = torch.where(\n        bound_labels == 1, enrichment_scores, torch.zeros_like(enrichment_scores)\n    )  # shape: (num_genes, num_tfs)\n\n    for gene_idx in range(bound_labels.shape[0]):\n        for tf_index, related_tfs in tf_relationships.items():\n            if bound_labels[gene_idx, tf_index] == 1 and torch.all(\n                bound_labels[gene_idx, related_tfs] == 1\n            ):\n                # OLD: adjustment_multiplier = torch.rand(1)\n                # divide its enrichment score by the maximum magnitude possible to\n                # create an adjustment multipler that scales with increasing enrichment\n                adjustment_multiplier = enrichment_scores[gene_idx, tf_index] / abs(\n                    enrichment_scores.max()\n                )\n\n                # randomly adjust the gene by some portion of the max adjustment\n                adjusted_mean_matrix[gene_idx, tf_index] = bound_mean + (\n                    adjustment_multiplier * max_adjustment\n                )\n            else:\n                # related tfs are not all bound, set the enrichment score to unbound\n                # mean\n                adjusted_mean_matrix[gene_idx, tf_index] = unbound_mean\n\n    return adjusted_mean_matrix  # shape (num_genes, num_tfs)\n</code></pre>"},{"location":"probability_models/perturbation_effect_adjustment_function_with_tf_relationships_boolean_logic/","title":"Perturbation effect adjustment function with tf relationships boolean logic","text":"<p>Adjust the mean of the perturbation effect based on the enrichment score and the provided binary / boolean or unary relationships between TFs. For each gene, the mean of the TF-gene pair\u2019s perturbation effect will be adjusted if the TF is bound to the gene and all of the Relations associated with the TF are satisfied (ie they evaluate to True). These relations could be unary conditions or Ands or Ors between TFs. A TF being bound corresponds to a true value, which means And(4, 5) would be satisfied is both TF 4 and TF 5 are bound to the gene in question. The adjustment will be a random value not exceeding the maximum adjustment.</p> <p>Parameters:</p> Name Type Description Default <code>binding_enrichment_data</code> <code>Tensor</code> <p>A tensor of enrichment scores for each gene with dimensions [n_genes, n_tfs, 3] where the entries in the third dimension are a matrix with columns [label, enrichment, pvalue].</p> required <code>bound_mean</code> <code>float</code> <p>The mean for bound genes.</p> required <code>unbound_mean</code> <code>float</code> <p>The mean for unbound genes.</p> required <code>max_adjustment</code> <code>float</code> <p>The maximum adjustment to the base mean based on enrichment.</p> required <code>tf_relationships</code> <code>dict[int, list[Relation]]</code> <p>A dictionary where the keys are TF indices and the values are lists of Relation objects that represent the conditions that must be met for the mean of the perturbation effect associated with the TF-gene pair to be adjusted.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Adjusted mean as a tensor.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If tf_relationships is not a dictionary between ints and lists of Relations</p> <code>ValueError</code> <p>If the tf_relationships dict does not have the same number of TFs as the binding_data tensor passed into the function</p> <code>ValueError</code> <p>If the tf_relationships dict has any TFs in the values that are not also in the keys or any key or value TFs that are out of bounds for the binding_data tensor</p> Source code in <code>yeastdnnexplorer/probability_models/generate_data.py</code> <pre><code>def perturbation_effect_adjustment_function_with_tf_relationships_boolean_logic(\n    binding_enrichment_data: torch.Tensor,\n    bound_mean: float,\n    unbound_mean: float,\n    max_adjustment: float,\n    tf_relationships: dict[int, list[Relation]],\n) -&gt; torch.Tensor:\n    \"\"\"\n    Adjust the mean of the perturbation effect based on the enrichment score and the\n    provided binary / boolean or unary relationships between TFs. For each gene, the\n    mean of the TF-gene pair's perturbation effect will be adjusted if the TF is bound\n    to the gene and all of the Relations associated with the TF are satisfied (ie they\n    evaluate to True). These relations could be unary conditions or Ands or Ors between\n    TFs. A TF being bound corresponds to a true value, which means And(4, 5) would be\n    satisfied is both TF 4 and TF 5 are bound to the gene in question. The adjustment\n    will be a random value not exceeding the maximum adjustment.\n\n    :param binding_enrichment_data: A tensor of enrichment scores for each gene with\n        dimensions [n_genes, n_tfs, 3] where the entries in the third dimension are a\n        matrix with columns [label, enrichment, pvalue].\n    :type binding_enrichment_data: torch.Tensor\n    :param bound_mean: The mean for bound genes.\n    :type bound_mean: float\n    :param unbound_mean: The mean for unbound genes.\n    :type unbound_mean: float\n    :param max_adjustment: The maximum adjustment to the base mean based on enrichment.\n    :type max_adjustment: float\n    :param tf_relationships: A dictionary where the keys are TF indices and the values\n        are lists of Relation objects that represent the conditions that must be met for\n        the mean of the perturbation effect associated with the TF-gene pair to be\n        adjusted.\n    :type tf_relationships: dict[int, list[Relation]]\n    :return: Adjusted mean as a tensor.\n    :rtype: torch.Tensor\n    :raises ValueError: If tf_relationships is not a dictionary between ints and lists\n        of Relations\n    :raises ValueError: If the tf_relationships dict does not have the same number of\n        TFs as the binding_data tensor passed into the function\n    :raises ValueError: If the tf_relationships dict has any TFs in the values that are\n        not also in the keys or any key or value TFs that are out of bounds for the\n        binding_data tensor\n\n    \"\"\"\n    if (\n        not isinstance(tf_relationships, dict)\n        or not all(isinstance(v, list) for v in tf_relationships.values())\n        or not all(isinstance(k, int) for k in tf_relationships.keys())\n        or not all(\n            isinstance(i, Relation) for v in tf_relationships.values() for i in v\n        )\n    ):\n        raise ValueError(\n            \"tf_relationships must be a dictionary between \\\n                ints and lists of Relation objects\"\n        )\n    if not all(\n        k in range(binding_enrichment_data.shape[1]) for k in tf_relationships.keys()\n    ):\n        raise ValueError(\n            \"all TFs mentioned in tf_relationships must be within \\\n                the bounds of the binding_data tensor's number of TFs\"\n        )\n    if not len(tf_relationships) == binding_enrichment_data.shape[1]:\n        raise ValueError(\n            \"tf_relationships must have the same number of TFs as \\\n                the binding_data tensor passed into the function\"\n        )\n\n    # Extract bound/unbound labels and enrichment scores\n    bound_labels = binding_enrichment_data[:, :, 0]  # shape: (num_genes, num_tfs)\n    enrichment_scores = binding_enrichment_data[:, :, 1]  # shape: (num_genes, num_tfs)\n\n    # we set all unbound scores to 0, then we will go through and also set any\n    # bound scores to unbound_mean if the related boolean statements are not satisfied\n    adjusted_mean_matrix = torch.where(\n        bound_labels == 1, enrichment_scores, torch.zeros_like(enrichment_scores)\n    )  # shape: (num_genes, num_tfs)\n\n    for gene_idx in range(bound_labels.shape[0]):\n        for tf_index, relations in tf_relationships.items():\n            # check if all relations (boolean relationships)\n            # associated with TFs are satisfied\n            if bound_labels[gene_idx, tf_index] == 1 and all(\n                relation.evaluate(bound_labels[gene_idx].tolist())\n                for relation in relations\n            ):\n                # OLD: adjustment_multiplier = torch.rand(1)\n                # divide its enrichment score by the maximum magnitude possible to\n                # create an adjustment multipler that scales with increasing enrichment\n                adjustment_multiplier = enrichment_scores[gene_idx, tf_index] / abs(\n                    enrichment_scores.max()\n                )\n\n                # randomly adjust the gene by some portion of the max adjustment\n                adjusted_mean_matrix[gene_idx, tf_index] = bound_mean + (\n                    adjustment_multiplier * max_adjustment\n                )\n            else:\n                # related tfs are not all bound, set the enrichment score to unbound\n                # mean\n                adjusted_mean_matrix[gene_idx, tf_index] = unbound_mean\n\n    return adjusted_mean_matrix  # shape (num_genes, num_tfs)\n</code></pre>"},{"location":"probability_models/relation_classes/","title":"Relation classes","text":""},{"location":"probability_models/relation_classes/#yeastdnnexplorer.probability_models.relation_classes.And","title":"<code>And</code>","text":"<p>             Bases: <code>Relation</code></p> <p>Class for representing the logical AND of multiple conditions Allows nesed conditions, i.e. And(1, Or(2, 3))</p> Source code in <code>yeastdnnexplorer/probability_models/relation_classes.py</code> <pre><code>class And(Relation):\n    \"\"\"Class for representing the logical AND of multiple conditions Allows nesed\n    conditions, i.e. And(1, Or(2, 3))\"\"\"\n\n    def __init__(self, *conditions):\n        \"\"\"\n        :param conditions: List of conditions to be evaluated\n        :type conditions: List[float | Relation]\n        \"\"\"\n        self.conditions = conditions\n\n    def evaluate(self, bound_vec):\n        \"\"\"\n        Returns true if the And() condition evaluates to true Evaluates nested\n        conditions as needed.\n\n        :param bound_vec: Vector of TF indices (0 or 1) indicating which TFs are bound\n            for the gene in question\n        :type bound_vec: List[float]\n\n        \"\"\"\n        if type(bound_vec) is not list or not all(\n            isinstance(x, float) for x in bound_vec\n        ):\n            raise ValueError(\"bound_vec must be a list of floats\")\n\n        if not self.conditions:\n            return True\n\n        # Each condition can be an index or another Relation (And/Or)\n        return all(\n            c.evaluate(bound_vec) if isinstance(c, Relation) else bound_vec[c]\n            for c in self.conditions\n        )\n\n    def __str__(self):\n        return f\"AND({', '.join(str(c) for c in self.conditions)})\"\n</code></pre>"},{"location":"probability_models/relation_classes/#yeastdnnexplorer.probability_models.relation_classes.And.__init__","title":"<code>__init__(*conditions)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>conditions</code> <code>List[float | Relation]</code> <p>List of conditions to be evaluated</p> <code>()</code> Source code in <code>yeastdnnexplorer/probability_models/relation_classes.py</code> <pre><code>def __init__(self, *conditions):\n    \"\"\"\n    :param conditions: List of conditions to be evaluated\n    :type conditions: List[float | Relation]\n    \"\"\"\n    self.conditions = conditions\n</code></pre>"},{"location":"probability_models/relation_classes/#yeastdnnexplorer.probability_models.relation_classes.And.evaluate","title":"<code>evaluate(bound_vec)</code>","text":"<p>Returns true if the And() condition evaluates to true Evaluates nested conditions as needed.</p> <p>Parameters:</p> Name Type Description Default <code>bound_vec</code> <code>List[float]</code> <p>Vector of TF indices (0 or 1) indicating which TFs are bound for the gene in question</p> required Source code in <code>yeastdnnexplorer/probability_models/relation_classes.py</code> <pre><code>def evaluate(self, bound_vec):\n    \"\"\"\n    Returns true if the And() condition evaluates to true Evaluates nested\n    conditions as needed.\n\n    :param bound_vec: Vector of TF indices (0 or 1) indicating which TFs are bound\n        for the gene in question\n    :type bound_vec: List[float]\n\n    \"\"\"\n    if type(bound_vec) is not list or not all(\n        isinstance(x, float) for x in bound_vec\n    ):\n        raise ValueError(\"bound_vec must be a list of floats\")\n\n    if not self.conditions:\n        return True\n\n    # Each condition can be an index or another Relation (And/Or)\n    return all(\n        c.evaluate(bound_vec) if isinstance(c, Relation) else bound_vec[c]\n        for c in self.conditions\n    )\n</code></pre>"},{"location":"probability_models/relation_classes/#yeastdnnexplorer.probability_models.relation_classes.Or","title":"<code>Or</code>","text":"<p>             Bases: <code>Relation</code></p> Source code in <code>yeastdnnexplorer/probability_models/relation_classes.py</code> <pre><code>class Or(Relation):\n    def __init__(self, *conditions):\n        \"\"\"\n        :param conditions: List of conditions to be evaluated\n        :type conditions: List[int | Relation]\n        \"\"\"\n        self.conditions = conditions\n\n    def evaluate(self, bound_vec):\n        \"\"\"\n        Returns true if the Or() condition evaluates to true Evaluates nested conditions\n        as needed.\n\n        :param bound_vec: Vector of TF indices (0 or 1) indicating which TFs are bound\n            for the gene in question\n        :type bound_vec: List[int]\n\n        \"\"\"\n        if type(bound_vec) is not list or not all(\n            isinstance(x, float) for x in bound_vec\n        ):\n            raise ValueError(\"bound_vec must be a list of floats\")\n\n        if not self.conditions:\n            return True\n\n        # Each condition can be an index or another Relation (And/Or)\n        return any(\n            c.evaluate(bound_vec) if isinstance(c, Relation) else bound_vec[c]\n            for c in self.conditions\n        )\n\n    def __str__(self):\n        return f\"OR({', '.join(str(c) for c in self.conditions)})\"\n</code></pre>"},{"location":"probability_models/relation_classes/#yeastdnnexplorer.probability_models.relation_classes.Or.__init__","title":"<code>__init__(*conditions)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>conditions</code> <code>List[int | Relation]</code> <p>List of conditions to be evaluated</p> <code>()</code> Source code in <code>yeastdnnexplorer/probability_models/relation_classes.py</code> <pre><code>def __init__(self, *conditions):\n    \"\"\"\n    :param conditions: List of conditions to be evaluated\n    :type conditions: List[int | Relation]\n    \"\"\"\n    self.conditions = conditions\n</code></pre>"},{"location":"probability_models/relation_classes/#yeastdnnexplorer.probability_models.relation_classes.Or.evaluate","title":"<code>evaluate(bound_vec)</code>","text":"<p>Returns true if the Or() condition evaluates to true Evaluates nested conditions as needed.</p> <p>Parameters:</p> Name Type Description Default <code>bound_vec</code> <code>List[int]</code> <p>Vector of TF indices (0 or 1) indicating which TFs are bound for the gene in question</p> required Source code in <code>yeastdnnexplorer/probability_models/relation_classes.py</code> <pre><code>def evaluate(self, bound_vec):\n    \"\"\"\n    Returns true if the Or() condition evaluates to true Evaluates nested conditions\n    as needed.\n\n    :param bound_vec: Vector of TF indices (0 or 1) indicating which TFs are bound\n        for the gene in question\n    :type bound_vec: List[int]\n\n    \"\"\"\n    if type(bound_vec) is not list or not all(\n        isinstance(x, float) for x in bound_vec\n    ):\n        raise ValueError(\"bound_vec must be a list of floats\")\n\n    if not self.conditions:\n        return True\n\n    # Each condition can be an index or another Relation (And/Or)\n    return any(\n        c.evaluate(bound_vec) if isinstance(c, Relation) else bound_vec[c]\n        for c in self.conditions\n    )\n</code></pre>"},{"location":"probability_models/relation_classes/#yeastdnnexplorer.probability_models.relation_classes.Relation","title":"<code>Relation</code>","text":"<p>Base class for relations between TF indices.</p> Source code in <code>yeastdnnexplorer/probability_models/relation_classes.py</code> <pre><code>class Relation:\n    \"\"\"Base class for relations between TF indices.\"\"\"\n\n    def evaluate(self, bound_vec: list[int]):\n        raise NotImplementedError\n</code></pre>"},{"location":"tutorials/database_interface/","title":"Database Interface","text":"<pre><code>from yeastdnnexplorer.interface import *\nimport matplotlib.pyplot as plt\n</code></pre> <pre><code>regulator = RegulatorAPI()\n\nresult = await regulator.read()\nresult.get(\"metadata\")\n</code></pre> id uploader modifier regulator_locus_tag regulator_symbol upload_date modified_date under_development notes genomicfeature 0 1 chasem chasem YAL051W OAF1 2024-07-01 2024-07-01T12:47:18.619129-05:00 False none 24 1 2 chasem chasem YBL103C RTG3 2024-07-01 2024-07-01T12:47:19.667722-05:00 False none 140 2 3 chasem chasem YBL066C SEF1 2024-07-01 2024-07-01T12:47:20.523161-05:00 False none 186 3 4 chasem chasem YBL054W TOD6 2024-07-01 2024-07-01T12:47:21.309606-05:00 False none 199 4 5 chasem chasem YBL052C SAS3 2024-07-01 2024-07-01T12:47:22.161007-05:00 False none 201 ... ... ... ... ... ... ... ... ... ... ... 1809 1810 chasem chasem YOR262W GPN2 2024-07-01 2024-07-01T14:14:36.164403-05:00 False none 6387 1810 1811 chasem chasem YPR190C RPC82 2024-07-01 2024-07-01T14:14:38.921261-05:00 False none 7070 1811 1812 chasem chasem YPL228W CET1 2024-07-01 2024-07-01T14:15:51.518999-05:00 False none 6603 1812 1813 chasem chasem YKL049C CSE4 2024-07-01 2024-07-01T14:15:56.555122-05:00 False none 4083 1813 1814 chasem chasem YMR168C CEP3 2024-07-01 2024-07-01T14:22:14.060524-05:00 False none 5258 <p>1814 rows \u00d7 10 columns</p> <pre><code># First, retrieve only the records -- you'll want to filter these results down before\n# retrieving the files most likely\npss_api = PromoterSetSigAPI()\nresult = await pss_api.read()\nresult.get(\"metadata\")\n</code></pre> id uploader modifier single_binding composite_binding fileformat background upload_date modified_date file promoter source regulator_symbol regulator_locus_tag condition rank_recall data_usable background_name 0 1 chasem chasem 1.0 NaN 1 NaN 2024-07-01 2024-07-01T13:51:48.611781-05:00 https://yeastregulatorydb-htcf-data.s3.amazona... NaN harbison_chip OAF1 YAL051W YPD unreviewed unreviewed NaN 1 2 chasem chasem 2.0 NaN 1 NaN 2024-07-01 2024-07-01T13:51:49.643452-05:00 https://yeastregulatorydb-htcf-data.s3.amazona... NaN harbison_chip PDR3 YBL005W YPD unreviewed unreviewed NaN 2 3 chasem chasem 3.0 NaN 1 NaN 2024-07-01 2024-07-01T13:51:50.744384-05:00 https://yeastregulatorydb-htcf-data.s3.amazona... NaN harbison_chip HIR1 YBL008W YPD unreviewed unreviewed NaN 3 4 chasem chasem 4.0 NaN 1 NaN 2024-07-01 2024-07-01T13:51:51.507918-05:00 https://yeastregulatorydb-htcf-data.s3.amazona... NaN harbison_chip HAP3 YBL021C YPD unreviewed unreviewed NaN 4 5 chasem chasem 5.0 NaN 1 NaN 2024-07-01 2024-07-01T13:51:52.277595-05:00 https://yeastregulatorydb-htcf-data.s3.amazona... NaN harbison_chip TOD6 YBL054W YPD unreviewed unreviewed NaN ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2237 7011 admin admin NaN 145.0 5 1.0 2024-07-30 2024-07-30T16:39:36.457965-05:00 https://yeastregulatorydb-htcf-data.s3.amazona... 4.0 mitra_cc CBF1 YJR060W NaN unreviewed pass ad1 2238 7012 admin admin NaN 146.0 5 1.0 2024-07-30 2024-07-30T16:39:36.848168-05:00 https://yeastregulatorydb-htcf-data.s3.amazona... 4.0 mitra_cc GCN4 YEL009C NaN unreviewed pass ad1 2239 7013 admin admin NaN 147.0 5 1.0 2024-07-30 2024-07-30T16:39:37.234144-05:00 https://yeastregulatorydb-htcf-data.s3.amazona... 4.0 mitra_cc OAF1 YAL051W NaN unreviewed pass ad1 2240 7014 admin admin NaN 148.0 5 1.0 2024-07-30 2024-07-30T16:39:38.547155-05:00 https://yeastregulatorydb-htcf-data.s3.amazona... 4.0 mitra_cc YOX1 YML027W NaN unreviewed pass ad1 2241 7015 admin admin NaN 149.0 5 1.0 2024-07-30 2024-07-30T16:39:39.713590-05:00 https://yeastregulatorydb-htcf-data.s3.amazona... 4.0 mitra_cc LEU3 YLR451W NaN unreviewed pass ad1 <p>2242 rows \u00d7 18 columns</p> <pre><code>pss_api.push_params({\"regulator_symbol\": \"GZF3\",\n                     \"workflow\": \"nf_core_callingcards_1_0_0\",\n                     \"data_usable\": \"pass\"})\n</code></pre> <pre><code>pss_api.params\n</code></pre> <pre>\n<code>ParamsDict({'regulator_symbol': 'GZF3', 'workflow': 'nf_core_callingcards_1_0_0', 'data_usable': 'pass'})</code>\n</pre> <pre><code># note that retrieve_files is set to True\nresult = await pss_api.read(retrieve_files = True)\n\n# the metadata slot is the same as before\nresult.get(\"metadata\")\n</code></pre> id uploader modifier single_binding composite_binding fileformat background upload_date modified_date file promoter source regulator_symbol regulator_locus_tag condition rank_recall data_usable background_name 0 6577 admin admin 1837 NaN 5 1 2024-07-01 2024-07-01T16:43:50.145871-05:00 https://yeastregulatorydb-htcf-data.s3.amazona... 4 brent_nf_cc GZF3 YJL110C unknown pass pass ad1 1 6580 admin admin 1841 NaN 5 1 2024-07-01 2024-07-01T16:43:50.968078-05:00 https://yeastregulatorydb-htcf-data.s3.amazona... 4 brent_nf_cc GZF3 YJL110C unknown pass pass ad1 2 6642 admin admin 1902 NaN 5 1 2024-07-01 2024-07-01T16:43:54.969507-05:00 https://yeastregulatorydb-htcf-data.s3.amazona... 4 brent_nf_cc GZF3 YJL110C unknown pass pass ad1 3 6651 admin admin 1911 NaN 5 1 2024-07-01 2024-07-01T16:43:55.326651-05:00 https://yeastregulatorydb-htcf-data.s3.amazona... 4 brent_nf_cc GZF3 YJL110C unknown pass pass ad1 4 6717 admin admin 1960 NaN 5 1 2024-07-01 2024-07-01T16:44:00.500038-05:00 https://yeastregulatorydb-htcf-data.s3.amazona... 4 brent_nf_cc GZF3 YJL110C unknown pass pass ad1 <pre><code># but now the data slot is a dictionary where the `id` are keys and the values\n# are the files parsed into pandas dataframes\nresult.get(\"data\").get(\"6568\")\n</code></pre> <pre><code>pss_api.push_params({\"regulator_symbol\": \"GZF3,RTG3\"})\n\nprint(pss_api.params)\n</code></pre> <pre>\n<code>regulator_symbol: GZF3,RTG3, workflow: nf_core_callingcards_1_0_0, data_usable: pass\n</code>\n</pre> <p>Parameters can be removed one by one</p> <pre><code>print(pss_api.params)\n</code></pre> <pre>\n<code>regulator_symbol: GZF3,RTG3, workflow: nf_core_callingcards_1_0_0, data_usable: pass\n</code>\n</pre> <pre><code>pss_api.pop_params('data_usable')\n\nprint(pss_api.params)\n</code></pre> <pre>\n<code>regulator_symbol: GZF3,RTG3, workflow: nf_core_callingcards_1_0_0\n</code>\n</pre> <p>or cleared entirely</p> <pre><code>pss_api.pop_params(None)\n\nprint(pss_api.params)\n</code></pre> <pre>\n<code>\n</code>\n</pre> <pre><code>expression = ExpressionAPI()\n\nexpression.push_params({\"regulator_symbol\": \"GZF3\",\n                        \"lab\": \"mcisaac\",\n                        \"time\": \"15\"})\n\nexpression_res = await expression.read()\n\nexpression_res.get(\"metadata\")\n</code></pre> id uploader modifier regulator_id regulator_locus_tag regulator_symbol source_name assay upload_date modified_date ... replicate control mechanism restriction time file notes regulator source promotersetsig_processing 0 2516 chasem chasem 3578 YJL110C GZF3 mcisaac_oe overexpression 2024-07-01 2024-07-01T13:36:13.814201-05:00 ... 1 undefined gev N 15.0 https://yeastregulatorydb-htcf-data.s3.amazona... strain_id:SMY156n ; date:20150101 135 7 False 1 2510 chasem chasem 3578 YJL110C GZF3 mcisaac_oe overexpression 2024-07-01 2024-07-01T13:36:08.810276-05:00 ... 1 undefined gev P 15.0 https://yeastregulatorydb-htcf-data.s3.amazona... strain_id:SMY156 ; date:20150101 135 7 False <p>2 rows \u00d7 22 columns</p> <pre><code>rr_api = RankResponseAPI()\n\ndata = [\n    {\n        \"promotersetsig_ids\": [\"5555\"],\n        \"expression_ids\": [\"2510\"],\n        \"rank_by_binding_effect\": \"true\",\n    }\n]\n\ngroup_id = await rr_api.submit(post_dict=data)\n\nresult = await rr_api.retrieve(group_id)\n</code></pre> <pre><code>result.get(\"metadata\")\n</code></pre> regulator_symbol promotersetsig_ids expression_ids n_responsive total_expression_genes id 0 GZF3 5555 2510 810 6175.0 39e81d73-a8c5-46ca-8d66-be2c02ccf8b6 <pre><code>result.get(\"data\").get(result.get(\"metadata\").id[0])\n</code></pre> rank_bin n_responsive_in_rank random n_successes response_ratio pvalue ci_lower ci_upper 0 5 2 0.131174 2 0.400000 0.131211 0.052745 0.853367 1 10 3 0.131174 5 0.500000 0.005510 0.187086 0.812914 2 15 4 0.131174 9 0.600000 0.000027 0.322870 0.836636 3 20 0 0.131174 9 0.450000 0.000490 0.230578 0.684722 4 25 2 0.131174 11 0.440000 0.000149 0.244024 0.650718 5 30 1 0.131174 12 0.400000 0.000224 0.226558 0.593965 6 35 1 0.131174 13 0.371429 0.000296 0.214732 0.550769 7 40 0 0.131174 13 0.325000 0.001278 0.185729 0.491295 8 45 1 0.131174 14 0.311111 0.001363 0.181659 0.466491 9 50 0 0.131174 14 0.280000 0.004946 0.162311 0.424905 10 55 1 0.131174 15 0.272727 0.004378 0.161380 0.409619 11 60 0 0.131174 15 0.250000 0.011526 0.147186 0.378596 <pre><code>df = result.get(\"data\").get(result.get(\"metadata\").id[0]).iloc[1:50,:]\n\n# Plot\nplt.figure(figsize=(10, 6))\nplt.plot(df['rank_bin'], df['response_ratio'], marker='o')\nplt.title('GZF3 - promotersetsig_id 6577. McIsaac 15 2510')\nplt.xlabel('Rank Bin')\nplt.ylabel('Response Ratio')\nplt.title('Response Ratio vs Rank Bin')\nplt.grid(True)\nplt.show()\n</code></pre> <pre><code>pss_api.pop_params(None)\n\npss_api.push_params({\"source_name\": \"brent_nf_cc\", \"aggregated\": \"true\"})\n\ncallingcards_aggregated_meta_res = await pss_api.read()\n\ncallingcards_aggregated_meta_df = callingcards_aggregated_meta_res.get(\"metadata\")\n</code></pre> <pre><code>callingcards_aggregated_meta_df\n</code></pre> id uploader modifier single_binding composite_binding fileformat background upload_date modified_date file promoter source regulator_symbol regulator_locus_tag rank_recall data_usable background_name 0 6867 admin admin NaN 6 5 1 2024-07-06 2024-07-06T11:08:33.125644-05:00 https://yeastregulatorydb-htcf-data.s3.amazona... 4 brent_nf_cc WTM1 YOR230W pass pass ad1 1 6868 admin admin NaN 3 5 1 2024-07-06 2024-07-06T11:08:33.125640-05:00 https://yeastregulatorydb-htcf-data.s3.amazona... 4 brent_nf_cc MIG2 YGL209W pass pass ad1 2 6869 admin admin NaN 1 5 1 2024-07-06 2024-07-06T11:08:33.119818-05:00 https://yeastregulatorydb-htcf-data.s3.amazona... 4 brent_nf_cc CAT8 YMR280C pass pass ad1 3 6870 admin admin NaN 5 5 1 2024-07-06 2024-07-06T11:08:33.125605-05:00 https://yeastregulatorydb-htcf-data.s3.amazona... 4 brent_nf_cc PDR1 YGL013C pass pass ad1 4 6871 admin admin NaN 4 5 1 2024-07-06 2024-07-06T11:08:33.129564-05:00 https://yeastregulatorydb-htcf-data.s3.amazona... 4 brent_nf_cc PHO4 YFR034C pass pass ad1 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 66 6933 admin admin NaN 67 5 1 2024-07-06 2024-07-06T11:08:45.567814-05:00 https://yeastregulatorydb-htcf-data.s3.amazona... 4 brent_nf_cc ABF2 YMR072W fail pass ad1 67 6934 admin admin NaN 68 5 1 2024-07-06 2024-07-06T11:08:45.921894-05:00 https://yeastregulatorydb-htcf-data.s3.amazona... 4 brent_nf_cc USV1 YPL230W pass pass ad1 68 6935 admin admin NaN 69 5 1 2024-07-06 2024-07-06T11:08:46.166036-05:00 https://yeastregulatorydb-htcf-data.s3.amazona... 4 brent_nf_cc MGA1 YGR249W pass pass ad1 69 6936 admin admin NaN 70 5 1 2024-07-06 2024-07-06T11:08:46.246482-05:00 https://yeastregulatorydb-htcf-data.s3.amazona... 4 brent_nf_cc CIN5 YOR028C pass pass ad1 70 6937 admin admin NaN 71 5 1 2024-07-06 2024-07-06T11:08:47.987665-05:00 https://yeastregulatorydb-htcf-data.s3.amazona... 4 brent_nf_cc ROX1 YPR065W pass pass ad1 <p>71 rows \u00d7 17 columns</p> <pre><code>pss_api.push_params({\"regulator_symbol\": \"GZF3\"})\n\ncallingcards_res = await pss_api.read()\n\ngzf3_callingcards_res = callingcards_res.get(\"metadata\")\n\ngzf3_callingcards_res\n\n#gzf3_callingcards_res[~gzf3_callingcards_res.composite_binding_id.isna()]\n</code></pre> id uploader modifier single_binding composite_binding fileformat background upload_date modified_date file promoter source regulator_symbol regulator_locus_tag rank_recall data_usable background_name 0 6873 admin admin NaN 7 5 1 2024-07-06 2024-07-06T11:08:33.117009-05:00 https://yeastregulatorydb-htcf-data.s3.amazona... 4 brent_nf_cc GZF3 YJL110C pass pass ad1 <pre><code>data = [\n    {\n        \"promotersetsig_ids\": [\"6873\"],\n        \"expression_ids\": [\"2510\"],\n    }\n]\n\ngroup_id = await rr_api.submit(post_dict=data)\n\nagg_result = await rr_api.retrieve(group_id)\n\n</code></pre> <pre><code>agg_df = agg_result.get(\"data\").get(agg_result.get(\"metadata\").id[0]).iloc[1:50,:]\n\n# Plot\nplt.figure(figsize=(10, 6))\nplt.plot(agg_df['rank_bin'], agg_df['response_ratio'], marker='o')\nplt.title('Aggregated GZF3. McIsaac 15 2510')\nplt.xlabel('Rank Bin')\nplt.ylabel('Response Ratio')\nplt.title('Response Ratio vs Rank Bin')\nplt.grid(True)\nplt.show()\n</code></pre>"},{"location":"tutorials/database_interface/#the-database-interface-classes","title":"The Database Interface Classes","text":"<p>For each API endpoint exposed in the Django app, there is a corresponding class that provide methods to execute CRUD operations asynchronously.</p> <p>There are two types of API endpoints \u2013 those that contain only records data, and  those that store both records and pointers to files.</p>"},{"location":"tutorials/database_interface/#connecting-to-the-database","title":"Connecting to the Database","text":"<p>The database currently runs on HTCF service partition. This is a single node with 8 CPU and 30 GB that is meant for long running low resource jobs. The components that need to run are a postgres database, a redis instance and the django app. As long as these components are running on the service partition,  you can connect via an ssh tunnel with:</p> <pre><code>ssh username@login.htcf.wustl.edu -N -L 8001:n240:8000\n</code></pre> <p>where the <code>8001:n240:8000</code> takes the form of <code>local_port:cluster_node:app_port</code>. The django app will always be served on port <code>8000</code>, and <code>n240</code> is the only service partition node. You may choose a different local port.</p> <p>If you do this and cannot connect, let me know and I\u2019ll check the status of the jobs  on the cluster.</p>"},{"location":"tutorials/database_interface/#database-username-and-password","title":"Database username and password","text":"<p>Once you have a tunnel, you can access the database frontend at <code>127.0.0.1:8001</code> (or a different local port, if you changed that number). If you haven\u2019t already signed up, you\u2019ll need to click the \u2018sign up\u2019 button and follow the instructions. The e-mail server is not hooked up at the moment, so when it says \u201csee the e-mail\u201d, send a slack message and let me know. I\u2019ll give you a link to complete the sign up process. After that, you can just use the \u201csign in\u201d button.</p> <p>For computational tasks, including generating rank response data, celery workers must be launched on the HTCF general partition. There is currently a script that is meant to monitor the redis queue and launch/kill these workers automatically, but this functionality is new and largely untested. You can monitor the workers/tasks if you create another tunnel with:</p> <pre><code>ssh username@login.htcf.wustl.edu -N -L 8002:n240:5555\n</code></pre> <p>You\u2019d access this dashboard at <code>127.0.0.1:5555</code></p> <p>The login is currently:</p> <pre><code>username: \"flower\"\npassword: \"daisy\"\n</code></pre> <p>(yes, really \u2013 the security comes from the fact that you need to login with HTCF)</p>"},{"location":"tutorials/database_interface/#configuring-the-database-interface-classes","title":"Configuring the Database Interface Classes","text":"<p>The database classes expect the following environmental variables to be set. </p> <pre><code>BASE_URL='http://127.0.0.1:8001'\nTOKEN='&lt;your token=\"\"&gt;'\nBINDING_URL='http://127.0.0.1:8001/api/binding'\nBINDINGMANUALQC_URL='http://127.0.0.1:8001/api/bindingmanualqc'\nCALLINGCARDSBACKGROUND_URL='http://127.0.0.1:8001/api/callingcardsbackground'\nDATASOURCE_URL='http://127.0.0.1:8001/api/datasource'\nEXPRESSION_URL='http://127.0.0.1:8001/api/expression'\nEXPRESSIONMANUALQC_URL='http://127.0.0.1:8001/api/expressionmanualqc'\nFILEFORMAT_URL='http://127.0.0.1:8001/api/fileformat'\nGENOMICFEATURE_URL='http://127.0.0.1:8001/api/genomicfeature'\nPROMOTERSET_URL='http://127.0.0.1:8001/api/promoterset'\nPROMOTERSETSIG_URL='http://127.0.0.1:8001/api/promotersetsig'\nREGULATOR_URL='http://127.0.0.1:8001/api/regulator'\n</code></pre> <p>This can be achieved in the package during development with a <code>.env</code> file at the top most level of the package. The <code>.env</code> file is loaded in the package <code>__init__.py</code>.</p> <p>If you are importing <code>yeastdnnexplorer</code> into a different environment, then you\u2019ll  need to add the package <code>dotenv</code> and execute <code>load_dotenv(dotenv_path=env_path)</code>. If the <code>.env</code> file is in the same <code>PWD</code> in which you execute that command, there is no need to specify a path.</p>"},{"location":"tutorials/database_interface/#token-authentication","title":"Token Authentication","text":"<p>Once you have a username and password to the database, you can retrieve your token.  Make sure that you put this token, at least, in a <code>.env</code> file, and make sure that  <code>.env</code> file is in your <code>.gitignore</code>.</p> <p>Alternatively, you could retrieve and store in memory the token at the beginning of  each session \u2013 this is more secure if you are not using a <code>.env</code> file.  </p> <p>The <code>.env</code> file is already in the <code>yeastddnexplorer</code> <code>.gitignore</code></p> <pre><code>curl -X 'POST' \\\n  'http://127.0.0.1:8001/auth-token/' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"username\": \"username\",\n  \"password\": \"password\"\n}'\n</code></pre> <p>Or with python:</p> <pre><code>import requests\n\nurl = \"http://127.0.0.1:8001/auth-token/\"\nheaders = {\n    \"accept\": \"application/json\",\n    \"Content-Type\": \"application/json\",\n}\ndata = {\n    \"username\": \"username\",\n    \"password\": \"password\",\n}\n\nresponse = requests.post(url, json=data, headers=headers)\nprint(response.text)\n</code></pre> <p></p>"},{"location":"tutorials/database_interface/#using-the-interface-classes","title":"Using the Interface Classes","text":""},{"location":"tutorials/database_interface/#records-only-endpoints","title":"Records Only Endpoints","text":"<p>The records only endpoints are:</p> <ul> <li> <p>BindingManualQC</p> </li> <li> <p>DataSource</p> </li> <li> <p>ExpressionManualQC</p> </li> <li> <p>FileFormat</p> </li> <li> <p>GenomicFeature</p> </li> <li> <p>PromoterSetSig</p> </li> <li> <p>Regulator</p> </li> </ul> <p>When the <code>read()</code> method is called on the corresponding API classes, a dataframe will be returned in the response.</p> <p>All of the <code>read()</code> methods, for both types of API endpoints, return the result of a callable. By default, the callable returns a dictionary with two keys: <code>metadata</code> and <code>data</code>. For response only tables, the <code>metadata</code> value will be the records from the database as a pandas dataframe and the <code>data</code> will be `None.</p>"},{"location":"tutorials/database_interface/#example-regulatorapi","title":"Example \u2013 RegulatorAPI","text":""},{"location":"tutorials/database_interface/#record-and-file-endpoints","title":"Record and File Endpoints","text":"<p>The record and file endpoints are the following:</p> <ul> <li> <p>CallingCardsBackground</p> </li> <li> <p>Expression</p> </li> <li> <p>PromoterSet</p> </li> <li> <p>PromoterSetSig</p> </li> <li> <p>RankResponse *</p> </li> </ul> <p>The default <code>read()</code> method is the same as the Records only Endpoint API classes. However, there is an additional argument, <code>retrieve_files</code> which if set to <code>True</code> will retrieve the file for which each record provides metadata. The return value of <code>read()</code> is again a callable, and by default the <code>data</code> key will store a dictionary where the keys correspond to the <code>id</code> column in the <code>metadata</code>.</p>"},{"location":"tutorials/database_interface/#filtering","title":"Filtering","text":"<p>All API classes have a <code>params</code> attribute which stores the filtering parameters which will be applied to the HTTP requests.</p>"},{"location":"tutorials/database_interface/#retrieving-files-from-a-records-and-files-object","title":"Retrieving files from a Records and Files Object","text":"<p>To retrieve files from a Records and Files endpoint object, do the following:</p>"},{"location":"tutorials/database_interface/#filtering-on-multiple-items","title":"Filtering on multiple items","text":"<p>Some filters, and eventually all, will accept multiple arguments as a comma separated string without spaces. For example:</p>"},{"location":"tutorials/database_interface/#another-example-with-expression","title":"Another example with Expression","text":"<p>This is used to get an expression_id to use in the RankResponseAPI() below</p>"},{"location":"tutorials/database_interface/#rank-response","title":"Rank Response","text":"<p>The rank response endpoint is slightly different than the others. It is implemented asynchronously on the database side, and will run many tasks simultaneously. As such, it uses <code>submit()</code> and <code>retrieve()</code> methods.</p> <p>Additionally, it is a POST request and all parameters are passed in a json list. If you include, for a given dictionary, multiple items to promoetersetsig_ids and/or expression_ids, those datasets will be aggregated prior to calculating the rank response. The current parameters that may be passed for each are:</p> <ul> <li>promotersetsig_ids: a list of promotersetsig_ids. If more than 1, then the data will be aggregated prior to calcuating rank response</li> <li>expression_ids: a list of expression_ids. If more than 1, then the data will be aggregated prior to calcuating rank response</li> <li>expression_effect_colname: name of the column to use for the rank response effect</li> <li>expression_effect_threshold: The threshold to use on abs(effect) to label responsive/ unresponsive genes</li> <li>expression_pvalue_threshold: the threshold to use below which to label responsive/ unresponsive genes</li> <li>rank_bin_size: the size of the bins by which rank response is summarized.</li> <li>rank_by_binding_effect: if this is \u201ctrue\u201d, then rank by the binding effect first rather than pvalue. This is used for harbison_chip and mcisaac_oe</li> <li>summarize_by_rank_bin: if this is set to false, the unsummarized rank response (merged binding/response) is returned</li> </ul>"},{"location":"tutorials/database_interface/#callingcards-aggregated-data","title":"CallingCards Aggregated Data","text":"<p>For regulators which have data generated in the Brentlab and processed through the nf-core/callingcards:1.0.0 workflow, if that data has been manually (or eventually  automatically) QC reviewed, and if there are at least 2 samples which are labeled as data_usable, then there will exist a BindingConcatenated record to which both a BindingManualQC record and a PromoterSetSig record are foreign keyed.</p> <p>To view the set of samples for which there is aggregated data, you may do the following:</p>"},{"location":"tutorials/database_interface/#caveats","title":"Caveats","text":"<ol> <li> <p>I have written the scripts to automatically check the redis queue for work and to     both launch celery worker nodes, and kill them when they are finished. But, though    they work if I run them manually, they have not worked when scheduled through a    cronjob. I\u2019ll work with Brian and Eric next week to figure out why.</p> </li> <li> <p>I haven\u2019t tested each of the endpoint APIs individually. Help is welcome.</p> </li> </ol>"},{"location":"tutorials/exploring_perturbation_response_relationship/","title":"Exploring perturbation response relationship","text":"<pre><code>#imports \n\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import rankdata, pearsonr\nimport asyncio\nimport nest_asyncio\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom yeastdnnexplorer.interface import *\nnest_asyncio.apply()\nfrom typing import List, Optional\nimport warnings\nfrom patsy import dmatrices, dmatrix, demo_data\n</code></pre> <p>The cell below displays the first 3 TFs that contain aggregated Calling Cards binding data in the database. You can modify the command to return the entire list of TFs that contain aggregated data if needed. This code is mostly taken from the database_interface tutorial, refer to it if needed for more information on how to use the database. </p> <pre><code>pss_api = PromoterSetSigAPI()\n\npss_api.push_params({\"datasource\": \"brent_nf_cc\", \"aggregated\": \"true\"})\n\ncallingcards_aggregated_meta_res = await pss_api.read()\n\ncallingcards_aggregated_meta_df = callingcards_aggregated_meta_res.get(\"metadata\")\n\n# Prints the first 3 TFs that have aggregated data available. Modify as necessary to see the whole list of TFs\nprint(callingcards_aggregated_meta_df[\"regulator_symbol\"][:3])\n\n# Prints the total number of TFs that have aggregated data available.\nprint(\"Total number of TFs with aggregated data: \"+ str(len(callingcards_aggregated_meta_df[\"regulator_symbol\"])))\n</code></pre> <pre>\n<code>0    WTM1\n1    MIG2\n2    CAT8\nName: regulator_symbol, dtype: object\nTotal number of TFs with aggregated data: 71\n</code>\n</pre> <p>This method asynchronously retrieves and processes data for a given transcription factor. It combines the desired binding and perturbation data for the chosen TF and returns a DataFrame containing both the binding and perturbation data which we will use for further analysis.</p> <pre><code>#TODO: add a test for this method for future integration into the source code\n\nasync def process_transcription_factor_async(tf_name: str, is_aggregated: bool, binding_source: str, perturbation_source: str, pseudocount: int = 1) -&amp;gt; pd.DataFrame:    \n    \"\"\"\n    Process transcription factor data by retrieving and merging binding and perturbation datasets.\n\n    :param tf_name: The name of the transcription factor, e.g., \"AR080\".\n    :type tf_name: str\n    :param is_aggregated: Indicates whether the data is aggregated.\n    :type is_aggregated: bool\n    :param binding_source: The source of the binding data, e.g., \"cc\" or \"harbison\".\n    :type binding_source: str\n    :param perturbation_source: The source of the perturbation data, e.g., \"mcisaac\".\n    :type perturbation_source: str\n    :param pseudocount: The constant used in calculating enrichment and p-values scores to avoid division by zero, default is 1.\n    :type pseudocount: int, optional\n\n    :returns: A DataFrame containing the combined and processed binding and perturbation data.\n    :rtype: pd.DataFrame\n    \"\"\"\n    # Ensure the TF name is in uppercase to maintain consistency\n    tf_name_upper = tf_name.upper()\n\n    # Initialize API for binding data\n    pss_api_tf = PromoterSetSigAPI()\n\n    # Access the relevant data depending on the binding source and aggregation status\n    if binding_source == \"cc\":\n        if is_aggregated:\n            pss_api_tf.push_params({'regulator_symbol': tf_name_upper, \"datasource\": \"brent_nf_cc\", \"aggregated\": \"true\"})\n        else:\n            pss_api_tf.push_params({'regulator_symbol': tf_name_upper, \"workflow\": \"nf_core_callingcards_1_0_0\", \"data_usable\": \"pass\"})\n    elif binding_source == \"harbison\":\n        pss_api_tf.push_params({'regulator_symbol': tf_name_upper, \"source\": \"4\"})\n    elif binding_source == \"mitra\":\n        pss_api_tf.push_params({'regulator_symbol': tf_name_upper, \"source\": \"2\"})\n    elif binding_source == \"chip_exo\":\n        pss_api_tf.push_params({'regulator_symbol': tf_name_upper, \"source\": \"3\"})\n\n    # Asynchronously read the binding data from the API\n    tf_pss = await pss_api_tf.read(retrieve_files=True)\n    # Get the ID of the retrieved binding data\n    id = tf_pss.get(\"metadata\")[\"id\"][0]\n    # Extract the binding data using the ID\n    binding_df = tf_pss.get(\"data\").get(str(id))\n\n    # Initialize API for perturbation data\n    expression = ExpressionAPI()\n\n    # Map perturbation source to corresponding source number\n    source_mapping = {\n        \"mcisaac\": \"7\",\n        \"hu_reimann\": \"5\",\n        \"kemmeren\": \"6\"\n    }\n    source_number = source_mapping.get(perturbation_source, \"unknown\")\n\n    # Push parameters to retrieve the perturbation data\n    if perturbation_source == \"mcisaac\":\n        expression.push_params({\"regulator_symbol\": tf_name_upper, \"source\": source_number, \"time\": \"15\"})\n    else:\n        expression.push_params({\"regulator_symbol\": tf_name_upper, \"source\": source_number})\n\n    # Asynchronously read the perturbation data from the API\n    expression_res = await expression.read(retrieve_files=True)\n    # Get the ID of the retrieved perturbation data\n    id = expression_res.get(\"metadata\")[\"id\"][0]\n    # Extract the perturbation data using the ID\n    expression_df = expression_res.get(\"data\").get(str(id))\n\n    # Read perturbation data\n    perturbation_data = expression_df\n    # Read binding data\n    binding_data = binding_df\n\n    # Rename columns in binding data for consistency and clarity\n    if binding_source == \"cc\":\n        binding_data.rename(columns={\"callingcards_enrichment\": \"effect\", \"poisson_pval\": \"pvalue\"}, inplace=True)\n    elif binding_source == \"harbison\":\n        binding_data.rename(columns={\"pval\": \"pvalue\"}, inplace=True)\n    elif binding_source == \"mitra\":\n        binding_data.rename(columns={\"callingcards_enrichment\": \"effect\", \"poisson_pval\": \"pvalue\"}, inplace=True)\n    elif binding_source == \"chip_exo\":\n        binding_data.rename(columns={\"max_fc\": \"effect\", \"min_pval\": \"pvalue\"}, inplace=True)\n\n    # Optional: here you can modify the pseudocount as needed. The default pseudocount is set to 1.\n    # Calculate the effect size for binding data using the provided formula\n    if binding_source == \"cc\":\n        binding_data['effect'] = (binding_data['experiment_hops'] / binding_data['experiment_total_hops']) / \\\n                             ((binding_data['background_hops'] + pseudocount) / binding_data['background_total_hops'])\n\n    # Merge the binding data and perturbation data on the 'target_locus_tag' column\n    combined_data = pd.merge(binding_data, perturbation_data, on='target_locus_tag', suffixes=('_binding', '_perturbation'))\n\n    # # Assert that the length of combined_data is the minimum of the lengths of binding_data and perturbation_data\n    # assert len(combined_data) &amp;lt;= min(len(binding_data), len(perturbation_data)), \\\n    #     f\"Length of combined_data ({len(combined_data)}) is not equal to the minimum of lengths of binding_data ({len(binding_data)}) and perturbation_data ({len(perturbation_data)})\"\n\n    # Keep only the necessary columns in the combined data\n    combined_data = combined_data[['target_locus_tag', 'effect_binding', 'effect_perturbation', 'pvalue_binding']]\n\n    # Reorder the combined data by the smallest 'pvalue_binding' values\n    combined_data = combined_data.sort_values(by='pvalue_binding')\n\n    # Apply transformations:\n    # - Take the absolute value of 'effect_perturbation'\n    # - Calculate the negative log10 of 'pvalue_binding'\n    # - Calculate the log10 of 'effect_binding'\n    combined_data['effect_perturbation'] = combined_data['effect_perturbation'].abs()\n    combined_data['neg_log_pvalue_binding'] = -np.log10(combined_data['pvalue_binding'])\n    combined_data['log_enrichment'] = np.log10(combined_data['effect_binding'])\n\n    # Return the processed combined data as a DataFrame\n    return combined_data\n</code></pre> <p>The process_transcription_factor method below is used to call the asynchronous process_transcription_factor_async function above in a way that works with regular, step-by-step code. We need this function to use the method above in loops or other structures that don\u2019t handle asynchronous functions well.</p> <pre><code>def process_transcription_factor(tf_name: str, is_aggregated: bool, binding_source: str, perturbation_data: str, pseudocount: int = 1) -&amp;gt; pd.DataFrame:\n    \"\"\"\n    Processes transcription factor data synchronously by invoking an asynchronous function.\n\n    This function runs the asynchronous `process_transcription_factor_async` function synchronously to handle \n    transcription factor data processing. It retrieves the event loop, runs the asynchronous function, \n    and returns the processed DataFrame.\n\n    :param tf_name: The name of the transcription factor.\n    :type tf_name: str\n    :param is_aggregated: A boolean flag indicating whether the data is aggregated.\n    :type is_aggregated: bool\n    :param perturbation_source: The source of the perturbation data.\n    :type perturbation_source: str\n    :param pseudocount: The constant used in calculating enrichment and p-values scores to avoid division by zero, default is 1.\n    :type pseudocount: int, optional\n\n    :returns: A DataFrame containing the processed transcription factor data.\n    :rtype: pd.DataFrame\n    \"\"\"\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n\n        loop = asyncio.get_event_loop()\n        return loop.run_until_complete(process_transcription_factor_async(tf_name, is_aggregated, binding_source, perturbation_data, pseudocount))\n</code></pre> <p>Here is an applied example of using the methods above to obtain the combined data for the transcription factor AFT1 from the Calling Cards binding data and the hu_reimann perturbation data.</p> <pre><code>process_transcription_factor(\"AFT1\", False, \"cc\", \"hu_reimann\")\n</code></pre> target_locus_tag effect_binding effect_perturbation pvalue_binding neg_log_pvalue_binding log_enrichment 4773 YEL065W 40.645338 0.307065 0.000000 inf 1.609011 4772 YEL067C 31.354975 0.309224 0.000000 inf 1.496306 1033 YLR136C 90.116521 0.395634 0.000000 inf 1.954804 3828 YCL018W 167.226533 0.055596 0.000000 inf 2.223305 2657 YOR203W 92.903630 0.179912 0.000000 inf 1.968033 ... ... ... ... ... ... ... 5685 YGR240C 0.000000 0.070498 0.992207 0.003398 -inf 2542 YOR092W 0.000000 0.041109 0.994933 0.002206 -inf 941 YLR044C 0.000000 0.044537 0.995450 0.001980 -inf 3554 YBR082C 0.130850 0.185228 0.995484 0.001966 -0.883226 4441 YDR233C 0.000000 0.132318 0.997615 0.001037 -inf <p>6249 rows \u00d7 6 columns</p> <p>Since various TFs will have different distributions of enrichment scores and poisson pvalues, it is helpful to use a ranking of these values as opposed to the magnitudes themselves to make for easier comparisons across multiple TFs. Since there are thousands of data points in each TF, using a log will shrink the scale of these ranks, allowing us to focus better on the trends in the data. We are specifically interested in how the poisson pvalue associated with TF binding relates to the actual perturbation effect. </p> <pre><code>def process_dataframe(df: pd.DataFrame) -&amp;gt; pd.DataFrame:\n    \"\"\"\n    Processes a DataFrame further by calculating ranks and log transformations for expression and binding data to elucidate certain trends.\n\n    :param df: The input DataFrame containing 'effect_perturbation' and 'pvalue_binding' columns.\n    :type df: pd.DataFrame\n\n    :returns: A DataFrame that includes the original data along with new columns for expression ranks, log-transformed ranks, \n              binding ranks, and is sorted by the negative log-transformed binding rank.\n    :rtype: pd.DataFrame\n    \"\"\"\n    # Calculate expression rank with average ties method\n    df['expression_rank'] = rankdata(-abs(df['effect_perturbation']), method='average')\n\n    # Log transform the expression rank\n    df['neg_expression_rank_log'] = -np.log10(df['expression_rank'])\n\n    # Calculate binding rank with average ties method\n    df['binding_rank'] = rankdata(df['pvalue_binding'], method='average')\n\n    # Calculate log transform of the binding rank\n    df['neg_log_rank_binding'] = -np.log10(df['binding_rank'])\n\n    # Select specific columns\n    plotting_df = df[['target_locus_tag','effect_perturbation', 'expression_rank', 'neg_expression_rank_log', \n                      'pvalue_binding', 'binding_rank', 'neg_log_rank_binding']]\n\n    # Arrange (sort) by neg_log_rank_binding in descending order\n    plotting_df = plotting_df.sort_values(by='neg_log_rank_binding', ascending=False)\n\n    return plotting_df\n</code></pre> <p>The create_bins method is designed to create \u201cbins\u201d of data for a specified column in the input DataFrame. Binning data is a method of grouping continuous values into discrete intervals or \u201cbins.\u201d This process can help in reducing variance and revealing general trends in a dataset, which is needed in this instance to better see the trend between the log rank binding (LRB) and log rank perturbation response (LRR) across various TFs.</p> <p>This method gives you the option of adjusting the size of each bin, or selecting the number of bins you want to create. For example, choosing bin_size = None and num_bins = 5 will create 5 bins of equal size based on the range of LRB values, but note that the number of data points in each bin may vary as the data is not uniformly distributed.</p> <pre><code>def create_bins(data: pd.DataFrame, column: str, bin_size: Optional[float] = None, num_bins: Optional[int] = None) -&amp;gt; pd.Series:\n    \"\"\"\n    Creates bins for a specified column in a DataFrame.\n\n    :param data: A DataFrame containing the data to be binned.\n    :type data: pd.DataFrame\n    :param column: The name of the column to be binned.\n    :type column: str\n    :param bin_size: The size of each bin (optional, default is None). If specified, the range of the data will be partitioned into bins of this size.\n    :type bin_size: Optional[float]\n    :param num_bins: The number of bins (optional, default is None). If specified, the range of the data will be partitioned into this number of bins.\n    :type num_bins: Optional[int]\n\n    :returns: A set of bins that partitions the data along the desired column value.\n    :rtype: pd.Series\n    \"\"\"\n    if bin_size is not None:\n        bin_edges = np.arange(data[column].min(), data[column].max() + bin_size, bin_size)\n    elif num_bins is not None:\n        bin_edges = np.linspace(data[column].min(), data[column].max(), num_bins + 1)\n    else:\n        raise ValueError(\"Either bin_size or num_bins must be specified\")\n    return pd.cut(data[column], bins=bin_edges, include_lowest=True, right=False)\n</code></pre> <p>The plot_with_custom_bins method creates a scatter plot to visualize the effects of this binning process on the data.</p> <pre><code>def plot_with_custom_bins(data: pd.DataFrame, bin_size: Optional[float] = None, num_bins: Optional[int] = None) -&amp;gt; None:\n    \"\"\"\n    Bins the 'neg_log_rank_binding' column using the specified bin size or number of bins, calculates the mean of 'neg_expression_rank_log' for each bin, and plots these means against the bin centers. It also fits a LOESS line on the binned data and displays the number of data points in each bin.\n\n    :param data: The input DataFrame containing 'neg_log_rank_binding' and 'neg_expression_rank_log' columns.\n    :type data: pd.DataFrame\n    :param bin_size: The size of each bin (optional, default is None). If specified, the range of the data will be partitioned into bins of this size.\n    :type bin_size: Optional[float]\n    :param num_bins: The number of bins (optional, default is None). If specified, the range of the data will be partitioned into this number of bins.\n    :type num_bins: Optional[int]\n\n    :returns: None\n    :rtype: None\n    \"\"\"\n\n    # Suppress specific runtime warnings\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n\n        # Create bins for the 'neg_log_rank_binding' column using the specified bin size or number of bins\n        data['bin'] = create_bins(data, 'neg_log_rank_binding', bin_size, num_bins)\n\n        # Calculate the mean of 'neg_expression_rank_log' for each bin\n        binned_means = data.groupby('bin', observed=True)['neg_expression_rank_log'].mean().reset_index()\n\n        # Calculate the center of each bin for the 'neg_log_rank_binding' column\n        bin_centers = data.groupby('bin', observed=True)['neg_log_rank_binding'].mean().reset_index()\n\n        # Count the number of data points in each bin and sort the counts by bin order\n        bin_counts = data['bin'].value_counts().sort_index().reset_index(drop=True)\n\n        # Plotting the data\n        plt.figure(figsize=(10, 6))  # Set the figure size\n        # Create a scatter plot of bin centers vs. binned means\n        plt.scatter(bin_centers['neg_log_rank_binding'], binned_means['neg_expression_rank_log'], color='blue', label='Binned Means')\n\n        # Add the number of data points for each bin as text labels on the plot\n        for i in range(len(bin_centers)):\n            plt.text(bin_centers['neg_log_rank_binding'][i], binned_means['neg_expression_rank_log'][i] - 0.007, str(bin_counts[i]), \n                     color='black', ha='left', va='top', fontsize=9)\n\n        # Set the labels and title for the plot\n        plt.xlabel('Negative Log10 Rank Binding')  # Label for the x-axis\n        plt.ylabel('Negative Log10 Expression Rank')  # Label for the y-axis\n        plt.title('Negative Log Rank Binding vs Negative Log Expression Rank on Binned Data')  # Title of the plot\n        plt.legend()\n        plt.show()\n</code></pre> <p>Now that we have covered the main methods, here is an example of them in action. In this case, we are interested in displaying the binning for the single transcription factor \u201cAFT1.\u201d We are using a pseudocount of 10.3922 to calculate the enrichment scores and p values. Then we bin the data using 5 bins and graph the scatterplot.</p> <pre><code>combined_data = process_transcription_factor(\"AFT1\", False, \"cc\", \"mcisaac\", 10.3922)\nplotting_df = process_dataframe(combined_data)\nplot_with_custom_bins(plotting_df, num_bins=5)\n</code></pre> <p>The x-axis of this plot is the negative log of the binding poisson pvalue ranks. Recall that ties are handled by taking an average across data points with the same value to assign ranks. Data with smaller pvalue magnitudes are presumed to be more significant, therefore they occupy the highest ranks (i.e. 1, 2, 3, etc.). When taking the negative log of these ranks, points that are closer to 0 constitute points with these higher ranks, and points are that more negative correspond to lower ranked and therefore larger pvalues for binding. </p> <p>The y-axis of this plot is the negative log of the perturbation effect ranks. A similar line of logic with regard to the rankings applies here as it does with the x-axis. However, in this instance, data with larger perturbation effects are more important, and therefore they occupy the higher ranks. Thus, data that is closer to 0 corresponds to higher perturbation effects, and those that are more negative correspond to perturbation effects that are ranked lower and tend to have smaller magnitude of effect.</p> <p>Also, it is important to notice that the number of datapoints in each of the bins here is not the same. There appear to be significantly more points that occupy the leftmost bin. This means that there are likely many ties among points that have larger binding pvalues, therefore there is a greater concentration of these points in the leftmost bin in contrast to the bins on the right. By the same virtue, there is a significantly smaller quantity of points in the rightmost bin, indicating less ties among points that have smaller binding pvalues.</p> <p>The general trend of these binned means shows a postive, upward response. This finding aligns with our hypothesis that more significant binding effects (which are measured here using the poisson pvalue of binding) should correlate with points that have more significant perturbation effects (measured here using the magnitude of the perturbation effect). However, we want the trend to be consistent across all TFs, as we seek to identify a general relationship that the eventual model can learn from in hopes of enhancing its predictive power.</p> <p>The process_and_plot_tfs method processes and plots data for a list of transcription factors (TFs). For each TF, it retrieves and processes the data, and then creates a plot using custom bins. This method serves as a comprehensive pipeline to handle multiple TFs, from data retrieval and processing to visualization.</p> <pre><code>def process_and_plot_tfs(tf_list: List[str], boolean_list: List[bool], binding_source: str, perturbation_source: str, bins: Optional[int] = 5, pseudocount: Optional[int] = 1) -&amp;gt; None:\n    \"\"\"\n    Processes and plots data for a list of transcription factors (TFs). This function iterates over each TF in the list, accounting for \n    whether or not they possess aggregate data, then processes this data and generates plots with the specified number of bins for each.\n\n    :param tf_list: A list of transcription factor names.\n    :type tf_list: List[str]\n    :param boolean_list: A list of boolean values indicating whether the data is aggregated for each transcription factor.\n    :type boolean_list: List[bool]\n    :param perturbation_source: The source of the perturbation data.\n    :type perturbation_source: str\n    :param bins: The number of bins for plotting (optional, default is 5).\n    :type bins: Optional[int]\n    :param pseudocount: The constant used in calculating enrichment and p-values scores to avoid division by zero, default is 1.\n    :type pseudocount: Optional[int]\n\n    :returns: None\n    :rtype: None\n    \"\"\"\n    # Suppress RuntimeWarnings for the duration of the following operations\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n\n        # Iterate over each transcription factor in the list\n        for i in range(len(tf_list)):\n            print(f\"Processing and plotting for TF: {tf_list[i]}\")\n\n            # Access the transcription factor data\n            combined_data = process_transcription_factor(tf_list[i], boolean_list[i], binding_source, perturbation_source, pseudocount)\n\n            # Process the combined data to calculate ranks and transformations\n            plotting_df = process_dataframe(combined_data)\n\n            # Plot the processed data with custom bins\n            plot_with_custom_bins(plotting_df, num_bins=bins)\n</code></pre> <p>Below is an example of a list of 4 more TFs we want to visualize this trend for, which we can do by utilizing this method</p> <pre><code>tfs = ['MIG2', 'CAT8', 'PDR1', 'PHO4']\nboolean_list = [True] * 4\nprocess_and_plot_tfs(tfs, boolean_list, \"cc\", \"mcisaac\")\n</code></pre> <pre>\n<code>Processing and plotting for TF: MIG2\n</code>\n</pre> <pre>\n<code>Processing and plotting for TF: CAT8\n</code>\n</pre> <pre>\n<code>Processing and plotting for TF: PDR1\n</code>\n</pre> <pre>\n<code>Processing and plotting for TF: PHO4\n</code>\n</pre> <p>While the latter 3 plots tend to exhibit the same upward trend as the plot above, the first plot for the TF MIG2 looks more unusual. The second point from the right on this plot representing the binned mean data for that range of negative log rank binding pvalues has a mean negative log expression rank that is significantly lower than expected. This means that for the data points within that bin, which is second only to the bin containing the points with the smallest binding pvalues, has on average the lowest ranked perturbation effects out of all of the bins. However, it is also important here to recognize the scale of the y-axis. Neither the positive nor negative trends we have observed exhibit a large change. Even through the graphs can look quite significant, it is important to recognize that the scale of the y-axis is actually quite small.</p> <p>For further exploration of this trend across more TFs, below is example of code that will plot the trend for 71 TFs including the 5 shown above. You can continue exploring how this trend persists across TFs.</p> <pre><code>#tfs = ['WTM1', 'MIG2', 'CAT8', 'PDR1', 'PHO4', 'RIM101', 'GZF3', 'VHR1', 'ASH1', 'GAT3','FHL1', 'TEC1', 'SIP3', 'SKN7', 'WTM2','PHO2', 'HAA1', 'ADR1', 'MET31', 'CRZ1', 'RPH1', 'CHA4', 'CAD1', 'ZAP1', 'SKO1', 'ACA1', 'FZF1', 'HAP2', 'HAP3', 'HAP5','INO4', 'ERT1', 'TOG1', 'MET4', 'PPR1', 'RTG1', 'GLN3', 'MOT3', 'AFT1', 'GIS1', 'CBF1', 'SUM1', 'MSN2', 'DAL80', 'UPC2','RTG3', 'GAL80', 'RSF2', 'RME1', 'HIR2', 'SIP4', 'GCR1', 'HAP4', 'UME1', 'MET32', 'USV1', 'MGA1', 'CIN5', 'ROX1','XBP1', 'ZNF1', 'YHP1', 'RDR1', 'PDR3', 'RLM1', 'SFL1', 'SMP1', 'SUT2', 'HAC1', 'PHD1', 'ARO80']\n#len(tfs)\n#boolean_list = [True] * 59 + [False] * 12\n#process_and_plot_tfs(tfs, boolean_list, \"cc\", \"mcisaac\")\n</code></pre> <p>As we have seen above, even after transforming the data and applying this method of binning, the general trend that we expect to see isn\u2019t present in every transcription factor. While looking at the graph of each TF can be helpful, it would be important to investigate how this trend holds up in general across all of the TFs in our database, especially as that number becomes quite large and manual inspection of each TF is not optimal.</p> <p>The following method adjacent_differences_box_plot will create four boxplots on the same plot based on the binning process used above. For our exploratory data analysis, we settled on using 5 bins as the initial graphs for all TFs exhibited the best trends when usin this number of bins. Thus, for each TF, the method will calculate the difference between two adjacent bin means, doing this four times total for the five binned means. For each of the four values, the data across all TFs is aggregated, and a boxplot is made for each of these four datasets to visualize the trend in across all TFs.</p> <pre><code>def adjacent_differences_store_data(tfs: List[str], boolean_list: List[bool], binding_source: List[str], perturbation_sources: List[str], bins: int, pseudocount: Optional[int] = 1) -&amp;gt; dict:\n    \"\"\"\n    Stores the differences between adjacent bins for a list of transcription factors.\n\n    This function processes transcription factor data, calculates differences between the means of adjacent bins,\n    and stores these differences across multiple transcription factors.\n\n    :param tfs: A list of transcription factors that you want to plot.\n    :type tfs: List[str]\n    :param boolean_list: A list of boolean values indicating whether the data is aggregated for each transcription factor.\n    :type boolean_list: List[bool]\n    :param perturbation_sources: A list of sources of the perturbation data.\n    :type perturbation_sources: List[str]\n    :param bins: The number of bins to create.\n    :type bins: int\n    :param pseudocount: The constant used in calculating enrichment and p-values scores to avoid division by zero, default is 1.\n    :type pseudocount: Optional[int]\n\n    :returns: A dictionary containing the stored data for each perturbation source.\n    :rtype: dict\n    \"\"\"\n    # Initialize a dictionary to store differences between adjacent bins for each perturbation source\n    diff_data = {source: [[] for _ in range(bins - 1)] for source in perturbation_sources}\n\n    # Suppress RuntimeWarnings for the duration of the following operations\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n\n        # Iterate over each transcription factor in the list\n        for i in range(len(tfs)):\n            for source in perturbation_sources:\n                # Process the transcription factor data\n                combined_data = process_transcription_factor(str(tfs[i]), boolean_list[i], binding_source[i], source, pseudocount)\n\n                # Further process the combined data to calculate ranks and transformations\n                plotting_df = process_dataframe(combined_data)\n\n                # Create bins for the 'neg_log_rank_binding' column using the specified number of bins\n                plotting_df['bin'] = create_bins(plotting_df, 'neg_log_rank_binding', num_bins=bins)\n\n                # Calculate the mean of 'neg_expression_rank_log' for each bin\n                binned_means = plotting_df.groupby('bin', observed=True)['neg_expression_rank_log'].mean().reset_index()\n\n                # Initialize a list to store the differences between adjacent bins\n                binned_mean_diffs = []\n\n                # Calculate the differences between the means of adjacent bins\n                for j in range(bins - 1):\n                    binned_mean_diffs.append(binned_means[\"neg_expression_rank_log\"][j+1] - binned_means[\"neg_expression_rank_log\"][j])\n\n                # Append the differences to the corresponding list in diff_data\n                for j in range(bins - 1):\n                    diff_data[source][j].append(binned_mean_diffs[j])\n\n    # Remove NaN values from all bin difference lists\n    for source in diff_data:\n        diff_data[source] = [[x for x in bin_diff if not pd.isnull(x)] for bin_diff in diff_data[source]]\n\n    return diff_data\n</code></pre> <pre><code>def compare_adjacent_stored_data_box_plots(stored_data_list: List[dict], labels: List[str]) -&amp;gt; None:\n    \"\"\"\n    Generates a box plot comparing multiple sets of stored data and adds a horizontal line at y=0.\n\n    :param stored_data_list: A list of dictionaries containing stored data for each perturbation source.\n    :type stored_data_list: List[dict]\n    :param labels: A list of labels corresponding to each set of stored data.\n    :type labels: List[str]\n\n    :returns: None\n    :rtype: None\n    \"\"\"\n    plt.figure(figsize=(20, 13))\n    boxplot_data = []\n    source_labels = []\n    bin_labels = []\n\n    for j in range(4):\n        for idx, stored_data in enumerate(stored_data_list):\n            for source, data in stored_data.items():\n                boxplot_data.append(data[j])\n                source_labels.append(labels[idx])\n                bin_labels.append(f'Bins {j+1} and {j+2}')\n\n    box_positions = list(range(1, len(boxplot_data) + 1))\n\n    plt.boxplot(boxplot_data, positions=box_positions, widths=0.6)\n    plt.axhline(y=0, color='gray', linestyle='--')  \n\n    # Set the primary x-axis labels (perturbation sources)\n    plt.xticks(ticks=box_positions, labels=source_labels, rotation=90, ha='center')\n\n    # Add bin labels below the primary x-axis\n    ax = plt.gca()\n    ax2 = ax.secondary_xaxis('bottom')\n    unique_bins = list(set(bin_labels))\n    bin_label_positions = []\n    for unique_bin in unique_bins:\n        positions = [box_positions[i] for i, bin_label in enumerate(bin_labels) if bin_label == unique_bin]\n        center_position = sum(positions) / len(positions)\n        bin_label_positions.append(center_position)\n    ax2.set_xticks(bin_label_positions)\n    ax2.set_xticklabels(unique_bins, rotation=0, ha='center', weight='bold', fontsize=12)\n\n    # Adjust the position of the primary x-axis labels and the secondary x-axis labels\n    plt.subplots_adjust(bottom=0.3)  # Adjust bottom to make space for source labels\n    ax.tick_params(axis='x', which='major', pad=25)  # Increase the padding for the source labels\n\n    plt.xlabel('Perturbation Sources and Bins')  \n    plt.ylabel('Difference in Binned LRR Means')  \n    plt.title('Comparison of Adjacent Binned LRR Mean Differences Between Perturbation Sources')\n\n    plt.show()\n</code></pre> <p>Here, we use this method to plot the boxplots for the above TF data. Note that we are augmenting the Calling Cards binding data with data from the mitra lab. You can see the breakdown of which TFs come from which set by referencing the index of the all_tfs list with the cc_to_mitra_ratio_in_all list.</p> <pre><code>all_tfs = ['WTM1','MIG2','RIM101','GZF3','ASH1','GAT3','TEC1','SIP3','SKN7','WTM2','HAA1','MET31','CRZ1','CHA4','ZAP1','SKO1','ACA1','FZF1','HAP2','HAP3','HAP5','INO4','ERT1','PPR1','RTG1','MOT3','CBF1','MSN2','DAL80','RTG3','GAL80','RSF2','RME1','HIR2','SIP4','HAP4','UME1','USV1','MGA1','CIN5','ROX1','XBP1','RDR1','PDR3','RLM1','SFL1','SMP1','SUT2','PHD1','SUT1','SOK2','STP2','YRR1','GAL4','LEU3','OAF1','SWI6','ACE2','TYE7','RGM1','GCN4','MIG3','STB5','RFX1','ARG80','ARG81','CST6','AZF1','SFP1','GTS1','FKH1','YOX1','FKH2','DIG1','MET28','RGT1','GCR2']\nboolean_list = [True]*41 + [False]*37\ncc_to_mitra_ratio_in_all = [\"cc\"]*49+[\"mitra\"]*29\n</code></pre> <pre><code>cc_mcisaac_adjacent_boxplot_data = adjacent_differences_store_data(all_tfs, boolean_list, cc_to_mitra_ratio_in_all, [\"mcisaac\"]*78, 5)\n</code></pre> <pre><code>compare_adjacent_stored_data_box_plots([cc_mcisaac_adjacent_boxplot_data], [\"cc/mcisaac\"])\n</code></pre> <p>This boxplot illustrates the differences in the binned mean LRR values (as shown on the y-axis) between adjacent bins (on the x-axis) across 71 transcription factors. For instance, \u201cBins 1 and 2\u201d shows the difference in expression ranks between the first and second bins, \u201cBins 2 and 3\u201d between the second and third bins, and so on. In the first boxplot, representing \u201cBins 1 and 2,\u201d the interquartile range (IQR) is very narrow, and the median difference is around zero, indicating little change in expression ranks between these bins for most TFs. </p> <p>As we move to \u201cBins 2 and 3,\u201d a similar trend is observed, with the median still close to zero and a narrow spread of the data near 0, suggesting minimal differences in expression ranks between these bins. However, as we progress to \u201cBins 3 and 4\u201d and \u201cBins 4 and 5,\u201d we see an increasing spread in the data. The median differences for these bins are also higher, especially in \u201cBins 4 and 5,\u201d where the median is significantly above zero. This indicates that for many TFs, there is a noticeable change in expression ranks in the later bins, with the differences becoming more pronounced. The highest quartile in \u201cBins 4 and 5\u201d indicates that the binned mean LRR changes considerably for some TFs, with the top of the boxplot extending to around 0.8, corresponding to a factor of 10^0.8 or approximately 6.3. This means that the expression rank could change by this factor from one bin to the next, demonstrating a significant shift. The presence of outliers further indicates that there are some TFs with even more substantial changes between these bins. </p> <p>Overall, this boxplot reveals that the differences between adjacent bins become more pronounced as we move to higher bins. This trend signifies that the expression ranks of TFs change more significantly in the later bins, highlighting a general pattern of increasing variability in the data as we move from earlier to later bins. So, while for some individual TFs the trend may not be evident (like the example shown above) it seems that across the entire dataset, this trend is generally consistent and can serve as a decent relationship that our models can use to learn from the data and result in more accurate predictions.</p> <p>For a broader look at how the binned means differ, this is an additional method that takes the difference in the binned mean values between the first and last bin for each TF only. Then the data across all TFs is aggregated, and a boxplot is made to visualize the overall difference across all TFs. We can use this to determine further if there truly is a significant increase in the binned means across the first and last bins. Additionally, these boxplots are much simpler to look at for a given binding/perturbation data set compared to the 4 boxplots shown on the adjacent bin differences plot above. Thus, we can compare how these single boxplots differ across all binding and perturbation datasets by plotting them in an array that makes it easy to visualize comparisons.</p> <p>The following method first_last_bin_difference_box_plot_comparisons will save the data needed to create boxplots of the first and last bin mean differences, which can then be plotted on the same plot to enable comparison between binding/perturbation dataset combinations. For each TF, the method will calculate the difference between the first and last bin means. For each of the four values, the data across all TFs is aggregated.</p> <pre><code>def first_last_bin_difference_box_plot_comparisons(tfs: List[str], boolean_list: List[bool], binding_source: List[str], perturbation_sources: List[str], bins: int, pseudocount: Optional[int] = 1) -&amp;gt; dict:\n    \"\"\"\n    Generates a box plot of the differences between the first and last bins for a list of transcription factors.\n\n    This function processes transcription factor data, calculates differences between the means of the first and last bins,\n    and generates a box plot to visualize these differences across multiple transcription factors.\n\n    :param tfs: A list of transcription factors that you want to plot.\n    :type tfs: List[str]\n    :param boolean_list: A list of boolean values indicating whether the data is aggregated for each transcription factor.\n    :type boolean_list: List[bool]\n    :param perturbation_sources: A list of sources of the perturbation data.\n    :type perturbation_sources: List[str]\n    :param bins: The number of bins to create.\n    :type bins: int\n    :param pseudocount: The constant used in calculating enrichment and p-values scores to avoid division by zero, default is 1.\n    :type pseudocount: Optional[int]\n\n    :returns: A dictionary containing the stored data for each perturbation source.\n    :rtype: dict\n    \"\"\"\n    # Initialize a dictionary to store differences between the first and last bins for each perturbation source\n    diff_data = {source: [] for source in perturbation_sources}\n\n    # Suppress RuntimeWarnings for the duration of the following operations\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n\n        # Iterate over each transcription factor in the list\n        for i in range(len(tfs)):\n            for source in perturbation_sources:\n                #print(str(i+1) +\": \"+tfs[i])\n                # Process the transcription factor data\n                combined_data = process_transcription_factor(str(tfs[i]), boolean_list[i], binding_source[i], source, pseudocount)\n\n                # Further process the combined data to calculate ranks and transformations\n                plotting_df = process_dataframe(combined_data)\n\n                # Create bins for the 'neg_log_rank_binding' column using the specified number of bins\n                plotting_df['bin'] = create_bins(plotting_df, 'neg_log_rank_binding', num_bins = bins)\n\n                # Calculate the mean of 'neg_expression_rank_log' for each bin\n                binned_means = plotting_df.groupby('bin', observed=True)['neg_expression_rank_log'].mean().reset_index()\n\n                # Calculate the difference between the first and last bin means\n                first_last_diff = binned_means[\"neg_expression_rank_log\"].iloc[-1] - binned_means[\"neg_expression_rank_log\"].iloc[0]\n\n                # Append the difference to the corresponding list in diff_data\n                diff_data[source].append(first_last_diff)\n\n    # Remove NaN values from all bin difference lists\n    for source in diff_data:\n        diff_data[source] = [x for x in diff_data[source] if not pd.isnull(x)]\n\n    return diff_data\n</code></pre> <p>The compare_first_and_last_stored_data_box_plots method below plots all of the boxplots on the same plot to enable easy comparison. It takes in data generated by the method directly above and uses the data to plot all of the boxplots. Note that you need to supply labels and make sure they correspond to the correct dataset. </p> <pre><code>def plot_boxplots(data: List[List[float]], binding_labels: List[str], perturbation_labels: List[str], data_type: str) -&amp;gt; None:\n    \"\"\"\n    Plots an array of boxplots with specified binding and perturbation labels.\n\n    :param data: A list of lists containing numerical data for each boxplot.\n    :type data: List[List[float]]\n    :param binding_labels: A list containing the labels for the binding data (\"cc\" and \"harbison\").\n    :type binding_labels: List[str]\n    :param perturbation_labels: A list containing the labels for the perturbation data (\"mcisaac\", \"kemmeren\", \"hu_reimann\").\n    :type perturbation_labels: List[str]\n\n    :returns: None\n    :rtype: None\n    \"\"\"\n    # Flatten the data to find the global min and max\n    all_data = np.concatenate(data)\n\n    # Calculate global min and max, rounding to the nearest 0.5\n    global_min = np.floor(np.min(all_data) * 2) / 2  # Round down to nearest 0.5\n    global_max = np.ceil(np.max(all_data) * 2) / 2   # Round up to nearest 0.5\n\n    # Determine number of rows and columns\n    num_datasets = len(data)\n    if num_datasets == 1:\n        nrows, ncols = 1, 1\n    else:\n        ncols = 3\n        nrows = (num_datasets + ncols - 1) // ncols  # Calculate the required number of rows\n\n    # Create subplots\n    fig, axs = plt.subplots(nrows, ncols, figsize=(5 * ncols, 5 * nrows))\n    axs = np.array(axs).reshape(-1)  # Flatten the 2D array of axes to iterate over them easily\n\n    for i, ax in enumerate(axs):\n        if i &amp;lt; num_datasets:\n            ax.boxplot(data[i])\n            row = i // ncols\n            col = i % ncols\n            if col == 0 and len(binding_labels) &amp;gt; row:\n                ax.set_ylabel(binding_labels[row])\n            if row == nrows - 1 and len(perturbation_labels) &amp;gt; col:\n                ax.set_xlabel(perturbation_labels[col])\n            ax.axhline(y=0, color='red', linestyle='--')  # Add dashed line at y=0\n\n            # Set y-axis limits based on global min and max\n            ax.set_ylim(global_min, global_max)\n        else:\n            ax.axis('off')  # Turn off any unused subplot axes\n    if data_type == \"diffs\":\n        fig.suptitle(f'Boxplot of Differences in Means Between First and Last Bins Across {len(data[0])} TFs')  \n    elif data_type == \"correlations\":\n        fig.suptitle(f'Boxplot of Pearson Correlations Between LRR/LRB Across {len(data[0])} TFs')  \n    plt.tight_layout()\n    plt.show()\n</code></pre> <p>We will first plot the boxplot for a single dataset that uses the CC+mitra binding data as well as the mcisaac perturbation data.</p> <pre><code>all_tfs = ['WTM1','MIG2','RIM101','GZF3','ASH1','TEC1','SIP3','SKN7','WTM2','HAA1','MET31','CRZ1','CHA4','ZAP1','SKO1','FZF1','HAP2','HAP3','HAP5','INO4','RTG1','MOT3','CBF1','MSN2','RTG3','RSF2','HIR2','SIP4','UME1','CIN5','ROX1','XBP1','RDR1','PDR3','RLM1','SFL1','SMP1','PHD1','SUT1','SOK2','STP2','AFT2','YRR1','GAL4','LEU3','SWI6','ACE2','RGM1','GCN4','MIG3','STB5','RFX1','ARG81','AZF1','SFP1','GTS1','FKH1','YOX1','FKH2','DIG1','MET28','RGT1']\nboolean_list = [True]*31 + [False]*31\ncc_to_mitra_ratio_in_all = [\"cc\"]*38+[\"mitra\"]*24\n</code></pre> <pre><code>cc_mcisaac_first_last_data = first_last_bin_difference_box_plot_comparisons(all_tfs, boolean_list, cc_to_mitra_ratio_in_all, perturbation_sources = [\"mcisaac\"], bins = 5)\ndata = [cc_mcisaac_first_last_data['mcisaac']]\nplot_boxplots(data, [\"Difference in Binned LRR Means between First and Last Bin\"], [\"Data from 78 TFs\"])\n</code></pre> <p>This graph has almost identical axes to the previous graph, except now the x-axis only shows a single value which is just the binned mean differences between the first and last bins. The y-axis is still the binned mean LRR difference between the first and last bin, and the data that generates the boxplot is taken from the 71 TFs used above. This boxplot further reinforces the argument that this general trend holds up across transcription factors. The bottom fence of the boxplot is slightly below zero, indicating that the minimum binned mean difference between the first and last bins can be slightly negative. The third quartile is around 0.5, which, when converted from the logarithmic scale (10^0.5), equates to approximately 3.3. This means that moving from the first bin to the last bin, the average response rank decreases by a factor of about 3.3. For example, if the average response rank in the first bin is 1,000, it decreases to around 300 in the last bin. While this shift may not seem drastic, it is significant enough to be noticeable and demonstrates a consistent trend across different TFs. This trend indicates that the response rank generally decreases as we move from the first bin to the last, highlighting a pattern of decreasing average response ranks in the dataset that contributes to the general upward trend we desire.</p> <p>Now, let\u2019s visualize all of the boxplots creating using combinations of the binding or perturbation dataset in an array format to enable easier visual comparison of how all of the different datasets affect the outcomes of the differences in the first and last bins.</p> <pre><code>chip_exo_kemmeren_first_last_data = first_last_bin_difference_box_plot_comparisons(all_tfs, boolean_list, [\"chip_exo\"]*100, perturbation_sources = [\"kemmeren\"], bins = 5)\nchip_exo_mcisaac_first_last_data = first_last_bin_difference_box_plot_comparisons(all_tfs, boolean_list, [\"chip_exo\"]*100, perturbation_sources = [\"mcisaac\"], bins = 5)\nchip_exo_hu_reimann_first_last_data = first_last_bin_difference_box_plot_comparisons(all_tfs, boolean_list, [\"chip_exo\"]*100, perturbation_sources = [\"hu_reimann\"], bins = 5)\n</code></pre> <pre><code>new_cc_kemmeren_first_last_data = first_last_bin_difference_box_plot_comparisons(all_tfs, boolean_list, cc_to_mitra_ratio_in_all, perturbation_sources = [\"kemmeren\"], bins = 5)\nnew_cc_mcisaac_first_last_data = first_last_bin_difference_box_plot_comparisons(all_tfs, boolean_list, cc_to_mitra_ratio_in_all, perturbation_sources = [\"mcisaac\"], bins = 5)\nnew_cc_hu_reimann_first_last_data = first_last_bin_difference_box_plot_comparisons(all_tfs, boolean_list, cc_to_mitra_ratio_in_all, perturbation_sources = [\"hu_reimann\"], bins = 5)\n</code></pre> <pre><code>new_harbison_kemmeren_first_last_data = first_last_bin_difference_box_plot_comparisons(all_tfs, boolean_list, [\"harbison\"]*100, perturbation_sources = [\"kemmeren\"], bins = 5)\nnew_harbison_mcisaac_first_last_data = first_last_bin_difference_box_plot_comparisons(all_tfs, boolean_list, [\"harbison\"]*100, perturbation_sources = [\"mcisaac\"], bins = 5)\nnew_harbison_hu_reimann_first_last_data = first_last_bin_difference_box_plot_comparisons(all_tfs, boolean_list, [\"harbison\"]*100, perturbation_sources = [\"hu_reimann\"], bins = 5)\n</code></pre> <pre><code>data = [new_cc_mcisaac_first_last_data['mcisaac'], new_cc_kemmeren_first_last_data['kemmeren'],  new_cc_hu_reimann_first_last_data['hu_reimann'], new_harbison_mcisaac_first_last_data['mcisaac'], new_harbison_kemmeren_first_last_data['kemmeren'], new_harbison_hu_reimann_first_last_data['hu_reimann'], chip_exo_mcisaac_first_last_data[\"mcisaac\"], chip_exo_kemmeren_first_last_data[\"kemmeren\"], chip_exo_hu_reimann_first_last_data[\"hu_reimann\"]]\nbinding_labels = [\"cc+mitra\", \"harbison\", \"chip_exo\"]\nperturbation_labels = [\"mcisaac\", \"kemmeren\", \"hu_reimann\"]\nplot_boxplots(data, binding_labels, perturbation_labels, \"diffs\")\n</code></pre> <p>This 3x3 array of boxplots displays the distribution of first and last bin mean differences across a common pool of 62 TFs for the 9 combinations of binding and perturbation data. The x and y-axis labels represent the corresponding binding or perturbation dataset used, respectively. For instance, the boxplot at the top right represents the boxplot of the data of the first and last bin mean differences across 62 TFs using the Calling Cards binding data and the hu_reimann perturbation data. So, in short, you can trace the binding and perturbation data used for a particular boxplot by identifying the corresponding labels on the x and y-axis which intersect at this particular boxplot. </p> <p>Now, let us examine these plots closer. For convenience, we will look at the trends within a particular row, which means looking at how the binding datasets perform relative to one another across the 3 perturbation datasets. Starting with the top row, in which all of the boxplots are created using the Calling Cards binding data, It is evident that a majority of the data in each boxplot displays a positive difference in the first and last bin means. This is important because it helps to confirm that across the various perturbation datasets used, when using the Calling Cards binding data, a generally positive difference is observed across the first and last bin means. </p> <p>Looking at the second row, which represents using the Harbison data, overall it seems to hold up the same trend. However, for the boxplot which uses the mcisaac perturbation data, it is interesting to note that around half of the data does not depict a positive trend. Additionally, for the other two boxplots in this row, which less of the data is positive, there is a greater spread of positive data than there was in the previous row using the Calling Cards data. </p> <p>Lastly, in the bottom row, which uses the chip_exo binding data, it is clear that using this binding data set produces the least desirable trends. In the boxplot which uses the mcisaac dataset, it appears that nearly half of the data is non-positive similar to the boxplot directly above it. Additionally, for the other two boxplots within this row, while more than half of the data shows a positive difference, it seems to be barely more than half, and in general seems to depict a weaker positive bin mean difference in comparison to the other two rows above it.</p> <p>Overall, however, these boxplots help to show that the various combinations of binding and perturbation datasets do show a positive bin mean difference, which is desirable. Of them, the Calling Cards binding dataset and the harbison binding dataset produce more positive distributions of the first and last bin mean difference data compared to the chip_exo binding dataset.</p> <p>Therefore, might also be interested in seeing how the chip_exo binding data in particular can be augmented to give a potentially more accurate look at its binding data in combination with the perturbation datasets. This is because with the chip_exo binding data in particular, when accessing the data, it only returns genes that were responsive to the transcription factor binding. This essentially means that in comparison to the Calling Cards or harbison data, which may have ~6000 rows of data when initially accessed, the chip_exo data often has less than 100 rows of data. To make the chip_exo data more reflective of both responsive and non-responsive transcription factor binding events, we can add additional rows of data for each gene not included in the original chip_exo dataset for a particular TF based on genes that exist in the perturbation dataset chosen, setting the enrichment score to be 0, and the p-value of binding to be the smallest insignificant p-value. Our modification to the process_transcription_factor_async method below achieves this goal. We can then graph the boxplots again to evaluate how these boxplots incorporating the new chip_exo augmented dataset are different from the ones above.</p> <pre><code>async def process_transcription_factor_async(tf_name: str, is_aggregated: bool, binding_source: str, perturbation_source: str, pseudocount: int = 1) -&amp;gt; pd.DataFrame:    \n    \"\"\"\n    Process transcription factor data by retrieving and merging binding and perturbation datasets.\n\n    :param tf_name: The name of the transcription factor, e.g., \"AR080\".\n    :type tf_name: str\n    :param is_aggregated: Indicates whether the data is aggregated.\n    :type is_aggregated: bool\n    :param binding_source: The source of the binding data, e.g., \"cc\" or \"harbison\".\n    :type binding_source: str\n    :param perturbation_source: The source of the perturbation data, e.g., \"mcisaac\".\n    :type perturbation_source: str\n    :param pseudocount: The constant used in calculating enrichment and p-values scores to avoid division by zero, default is 1.\n    :type pseudocount: int, optional\n\n    :returns: A DataFrame containing the combined and processed binding and perturbation data.\n    :rtype: pd.DataFrame\n    \"\"\"\n    # Ensure the TF name is in uppercase to maintain consistency\n    tf_name_upper = tf_name.upper()\n\n    # Initialize API for binding data\n    pss_api_tf = PromoterSetSigAPI()\n\n    # Access the relevant data depending on the binding source and aggregation status\n    if binding_source == \"cc\":\n        if is_aggregated:\n            pss_api_tf.push_params({'regulator_symbol': tf_name_upper, \"datasource\": \"brent_nf_cc\", \"aggregated\": \"true\"})\n        else:\n            pss_api_tf.push_params({'regulator_symbol': tf_name_upper, \"workflow\": \"nf_core_callingcards_1_0_0\", \"data_usable\": \"pass\"})\n    elif binding_source == \"harbison\":\n        pss_api_tf.push_params({'regulator_symbol': tf_name_upper, \"source\": \"4\"})\n    elif binding_source == \"mitra\":\n        pss_api_tf.push_params({'regulator_symbol': tf_name_upper, \"source\": \"2\"})\n    elif binding_source == \"chip_exo\":\n        pss_api_tf.push_params({'regulator_symbol': tf_name_upper, \"source\": \"3\"})\n\n    # Asynchronously read the binding data from the API\n    tf_pss = await pss_api_tf.read(retrieve_files=True)\n    # Get the ID of the retrieved binding data\n    id = tf_pss.get(\"metadata\")[\"id\"][0]\n    # Extract the binding data using the ID\n    binding_df = tf_pss.get(\"data\").get(str(id))\n\n    # Initialize API for perturbation data\n    expression = ExpressionAPI()\n\n    # Map perturbation source to corresponding source number\n    source_mapping = {\n        \"mcisaac\": \"7\",\n        \"hu_reimann\": \"5\",\n        \"kemmeren\": \"6\"\n    }\n    source_number = source_mapping.get(perturbation_source, \"unknown\")\n\n    # Push parameters to retrieve the perturbation data\n    if perturbation_source == \"mcisaac\":\n        expression.push_params({\"regulator_symbol\": tf_name_upper, \"source\": source_number, \"time\": \"15\"})\n    else:\n        expression.push_params({\"regulator_symbol\": tf_name_upper, \"source\": source_number})\n\n    # Asynchronously read the perturbation data from the API\n    expression_res = await expression.read(retrieve_files=True)\n    # Get the ID of the retrieved perturbation data\n    id = expression_res.get(\"metadata\")[\"id\"][0]\n    # Extract the perturbation data using the ID\n    expression_df = expression_res.get(\"data\").get(str(id))\n\n    # Read perturbation data\n    perturbation_data = expression_df\n    # Read binding data\n    binding_data = binding_df\n\n    # Rename columns in binding data for consistency and clarity\n    if binding_source == \"cc\":\n        binding_data.rename(columns={\"callingcards_enrichment\": \"effect\", \"poisson_pval\": \"pvalue\"}, inplace=True)\n    elif binding_source == \"harbison\":\n        binding_data.rename(columns={\"pval\": \"pvalue\"}, inplace=True)\n    elif binding_source == \"mitra\":\n        binding_data.rename(columns={\"callingcards_enrichment\": \"effect\", \"poisson_pval\": \"pvalue\"}, inplace=True)\n    elif binding_source == \"chip_exo\":\n        binding_data.rename(columns={\"max_fc\": \"effect\", \"min_pval\": \"pvalue\"}, inplace=True)\n\n    # Optional: here you can modify the pseudocount as needed. The default pseudocount is set to 1.\n    # Calculate the effect size for binding data using the provided formula\n    if binding_source == \"cc\":\n        binding_data['effect'] = (binding_data['experiment_hops'] / binding_data['experiment_total_hops']) / \\\n                             ((binding_data['background_hops'] + pseudocount) / binding_data['background_total_hops'])\n\n    missing_values = set(perturbation_data[\"target_locus_tag\"]) - set(binding_data[\"target_locus_tag\"])\n\n    # Add missing rows to the binding data with enrichment = 0 and pvalue = 1\n    if missing_values:\n        missing_rows = pd.DataFrame({\n            'target_locus_tag': list(missing_values),\n            'effect': 0,\n            'pvalue': -4.322 #since this is for the chipexo data, we find log2 (0.05) \n        })\n        binding_data = pd.concat([binding_data, missing_rows], ignore_index=True)\n\n    # Merge the binding data and perturbation data on the 'target_locus_tag' column\n    combined_data = pd.merge(binding_data, perturbation_data, on='target_locus_tag', suffixes=('_binding', '_perturbation'))\n\n    # # Assert that the length of combined_data is the minimum of the lengths of binding_data and perturbation_data\n    # assert len(combined_data) &amp;lt;= min(len(binding_data), len(perturbation_data)), \\\n    #     f\"Length of combined_data ({len(combined_data)}) is not equal to the minimum of lengths of binding_data ({len(binding_data)}) and perturbation_data ({len(perturbation_data)})\"\n\n    # Keep only the necessary columns in the combined data\n    combined_data = combined_data[['target_locus_tag', 'effect_binding', 'effect_perturbation', 'pvalue_binding']]\n\n    # Reorder the combined data by the smallest 'pvalue_binding' values\n    combined_data = combined_data.sort_values(by='pvalue_binding')\n\n    # Apply transformations:\n    # - Take the absolute value of 'effect_perturbation'\n    # - Calculate the negative log10 of 'pvalue_binding'\n    # - Calculate the log10 of 'effect_binding'\n    combined_data['effect_perturbation'] = combined_data['effect_perturbation'].abs()\n    combined_data['neg_log_pvalue_binding'] = -np.log10(combined_data['pvalue_binding'])\n    combined_data['log_enrichment'] = np.log10(combined_data['effect_binding'])\n\n    # Return the processed combined data as a DataFrame\n    return combined_data\n\n</code></pre> <pre><code>filled_chip_exo_kemmeren_first_last_data = first_last_bin_difference_box_plot_comparisons(all_tfs, boolean_list, [\"chip_exo\"]*100, perturbation_sources = [\"kemmeren\"], bins = 5)\nfilled_chip_exo_mcisaac_first_last_data = first_last_bin_difference_box_plot_comparisons(all_tfs, boolean_list, [\"chip_exo\"]*100, perturbation_sources = [\"mcisaac\"], bins = 5)\nfilled_chip_exo_hu_reimann_first_last_data = first_last_bin_difference_box_plot_comparisons(all_tfs, boolean_list, [\"chip_exo\"]*100, perturbation_sources = [\"hu_reimann\"], bins = 5)\n</code></pre> <pre><code># Keeping the first 2 rows of boxplots the same, we now plot the updated chip_exo related boxplots on the bottom row to compare\ndata = [new_cc_mcisaac_first_last_data['mcisaac'], new_cc_kemmeren_first_last_data['kemmeren'],  new_cc_hu_reimann_first_last_data['hu_reimann'], new_harbison_mcisaac_first_last_data['mcisaac'], new_harbison_kemmeren_first_last_data['kemmeren'], new_harbison_hu_reimann_first_last_data['hu_reimann'], filled_chip_exo_mcisaac_first_last_data[\"mcisaac\"], filled_chip_exo_kemmeren_first_last_data[\"kemmeren\"], filled_chip_exo_hu_reimann_first_last_data[\"hu_reimann\"]]\nbinding_labels = [\"cc+mitra\", \"harbison\", \"chip_exo w/ filled rows\"]\nperturbation_labels = [\"mcisaac\", \"kemmeren\", \"hu_reimann\"]\nplot_boxplots(data, binding_labels, perturbation_labels, \"diffs\")\n</code></pre> <p>This 3x3 array of boxplots is almost identical to the one above, save for the last row which has been updated to reflect the changes made to the chip_exo dataset. However, these changes seem to produce a more positive bin mean difference which is evident by comparing each boxplot with its previous boxplot in the above 3x3 array. For instance, the previous boxplot which utilized the mcisaac perturbation data had less than half of the data depict a positive differnece. Here, however, it is evident that more than half - nearly 75% of the data - depicts a positive difference between the first and last bin means. Similarly, in the boxplots using the kemmeren and hu_reimann perturbation datasets, the lower quartiles of the data in both boxplots are positive, which compared to the previous boxplots is an improvement, as those boxplots showed less than 75% of the data having a positive bin mean difference. Overall, this change to the chip_exo binding data in which the binning process is performed on tends to produce more positive differences, suggesting a potentially better correlation between the LRR and LRB.</p> <p>We can alternatively explore using the Pearson correlation coefficient as another means of documenting this relationship between the LRR and LRB. We will plot a similar array of boxplots like the one above, however, this time we will aggregate the Pearson correlation coefficients for the LRR vs. LRB across all TFs to determine whether there exists a similar positive trend here. To do this, we will need to define some new ways to process the data and obtain the correlation coefficients. Then, we will again create an array of these boxplots in the same format as above to compare the results.</p> <pre><code>def save_pearson_correlation_box_plot_comparisons(tfs: List[str], boolean_list: List[bool], binding_source: List[str], perturbation_sources: List[str], pseudocount: Optional[int] = 1) -&amp;gt; dict:\n    \"\"\"\n    Calculates the Pearson correlation coefficient between the 'LRR' and 'LRB' columns for each transcription factor (TF) across multiple perturbation sources.\n\n    :param tfs: A list of transcription factors to analyze.\n    :type tfs: List[str]\n    :param boolean_list: A list of boolean values indicating whether the data is aggregated for each transcription factor.\n    :type boolean_list: List[bool]\n    :param binding_source: A list of sources for the binding data.\n    :type binding_source: List[str]\n    :param perturbation_sources: A list of sources for the perturbation data.\n    :type perturbation_sources: List[str]\n    :param pseudocount: The constant used in calculating enrichment and p-values scores to avoid division by zero, default is 1.\n    :type pseudocount: Optional[int]\n\n    :returns: A dictionary where keys are perturbation sources and values are lists of Pearson correlation coefficients for each TF.\n    :rtype: Dict[str, List[float]]\n    \"\"\"\n    # Initialize a dictionary to store Pearson correlation coefficients for each perturbation source\n    correlation_data = {source: [] for source in perturbation_sources}\n\n    # Suppress RuntimeWarnings for the duration of the following operations\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n\n        # Iterate over each transcription factor in the list\n        for i in range(len(tfs)):\n            for source in perturbation_sources:\n                # Process the transcription factor data\n                combined_data = process_transcription_factor(str(tfs[i]), boolean_list[i], binding_source[i], source, pseudocount)\n\n                # Further process the combined data to calculate ranks and transformations\n                plotting_df = process_dataframe(combined_data)\n\n                # Ensure there are no NaN values in the 'LRR' and 'LRB' columns before calculating Pearson correlation\n                plotting_df = plotting_df.dropna(subset=['neg_log_rank_binding', 'neg_expression_rank_log'])\n\n                # Calculate Pearson correlation if there are at least two valid data points\n                if len(plotting_df) &amp;gt;= 2:\n                    correlation, _ = pearsonr(plotting_df['neg_log_rank_binding'], plotting_df['neg_expression_rank_log'])\n                    correlation_data[source].append(correlation)\n                else:\n                    correlation_data[source].append(float('nan'))\n                print(\"TF: {}, correlation: {}\".format(tfs[i], correlation))\n    # Remove NaN values from all correlation lists\n    for source in correlation_data:\n        correlation_data[source] = [x for x in correlation_data[source] if not pd.isnull(x)]\n\n    return correlation_data\n\ndef compare_pearson_correlation_stored_data_box_plots(stored_data_list: List[dict], labels: List[str]) -&amp;gt; None:\n    \"\"\"\n    Generates a box plot comparing multiple sets of stored data.\n\n    :param stored_data_list: A list of dictionaries containing stored data for each perturbation source.\n    :type stored_data_list: List[dict]\n    :param labels: A list of labels corresponding to each set of stored data.\n    :type labels: List[str]\n\n    :returns: None\n    :rtype: None\n    \"\"\"\n    plt.figure(figsize=(10, 6))\n    boxplot_data = []\n    xtick_labels = []\n\n    for idx, stored_data in enumerate(stored_data_list):\n        for source, data in stored_data.items():\n            boxplot_data.append(data)\n            xtick_labels.append(f'{labels[idx]}')\n\n    plt.boxplot(boxplot_data, widths=0.6)\n    plt.axhline(y=0, color='gray', linestyle='--')  # Add a horizontal dotted line at y=0\n    plt.xlabel('Perturbation Sources')  \n    plt.ylabel('Pearson Correlation Between LRB and LRR')  \n    plt.title('Comparison of Pearson Correlation Between LRB and LRR Across Multiple TFs')\n\n    plt.xticks(ticks=range(1, len(xtick_labels) + 1), labels=xtick_labels, rotation=90)\n    plt.show()\n</code></pre> <p>Now that we\u2019ve defined the methods, let\u2019s run them on the dataset of 62 TFs and compare the boxplots between the combinations of perturbating and binding data.</p> <pre><code>all_tfs = ['WTM1','MIG2','RIM101','GZF3','ASH1','TEC1','SIP3','SKN7','WTM2','HAA1','MET31','CRZ1','CHA4','ZAP1','SKO1','FZF1','HAP2','HAP3','HAP5','INO4','RTG1','MOT3','CBF1','MSN2','RTG3','RSF2','HIR2','SIP4','UME1','CIN5','ROX1','XBP1','RDR1','PDR3','RLM1','SFL1','SMP1','PHD1','SUT1','SOK2','STP2','AFT2','YRR1','GAL4','LEU3','SWI6','ACE2','RGM1','GCN4','MIG3','STB5','RFX1','ARG81','AZF1','SFP1','GTS1','FKH1','YOX1','FKH2','DIG1','MET28','RGT1']\nboolean_list = [True]*31 + [False]*31\ncc_to_mitra_ratio_in_all = [\"cc\"]*38+[\"mitra\"]*24\n</code></pre> <pre><code>#saving the data to plot as a joint boxplot\nnew_cc_kemmeren_pearson_correlations = save_pearson_correlation_box_plot_comparisons(all_tfs, boolean_list, cc_to_mitra_ratio_in_all, perturbation_sources = [\"kemmeren\"])\nnew_cc_mcisaac_pearson_correlations = save_pearson_correlation_box_plot_comparisons(all_tfs, boolean_list, cc_to_mitra_ratio_in_all, perturbation_sources = [\"mcisaac\"])\nnew_cc_hu_reimann_pearson_correlations = save_pearson_correlation_box_plot_comparisons(all_tfs, boolean_list, cc_to_mitra_ratio_in_all, perturbation_sources = [\"hu_reimann\"])\n</code></pre> <pre><code>new_harbison_kemmeren_pearson_correlations = save_pearson_correlation_box_plot_comparisons(all_tfs, boolean_list, [\"harbison\"]*100, perturbation_sources = [\"kemmeren\"])\nnew_harbison_mcisaac_pearson_correlations = save_pearson_correlation_box_plot_comparisons(all_tfs, boolean_list, [\"harbison\"]*100, perturbation_sources = [\"mcisaac\"])\nnew_harbison_hu_reimann_pearson_correlations = save_pearson_correlation_box_plot_comparisons(all_tfs, boolean_list, [\"harbison\"]*100, perturbation_sources = [\"hu_reimann\"])\n</code></pre> <pre><code>chip_exo_kemmeren_pearson_correlations = save_pearson_correlation_box_plot_comparisons(all_tfs, boolean_list, [\"chip_exo\"]*100, perturbation_sources = [\"kemmeren\"])\nchip_exo_mcisaac_pearson_correlations = save_pearson_correlation_box_plot_comparisons(all_tfs, boolean_list, [\"chip_exo\"]*100, perturbation_sources = [\"mcisaac\"])\nchip_exo_reimann_pearson_correlations = save_pearson_correlation_box_plot_comparisons(all_tfs, boolean_list, [\"chip_exo\"]*100, perturbation_sources = [\"hu_reimann\"])\n</code></pre> <pre><code>data = [new_cc_mcisaac_pearson_correlations['mcisaac'], new_cc_kemmeren_pearson_correlations['kemmeren'], new_cc_hu_reimann_pearson_correlations['hu_reimann'], new_harbison_mcisaac_pearson_correlations['mcisaac'], new_harbison_kemmeren_pearson_correlations['kemmeren'], new_harbison_hu_reimann_pearson_correlations['hu_reimann'],chip_exo_mcisaac_pearson_correlations['mcisaac'], chip_exo_kemmeren_pearson_correlations['kemmeren'], chip_exo_reimann_pearson_correlations['hu_reimann']]\nbinding_labels = [\"cc+mitra\", \"harbison\", \"chip_exo\"]\nperturbation_labels = [\"mcisaac\", \"kemmeren\", \"hu_reimann\"]\nplot_boxplots(data, binding_labels, perturbation_labels, \"correlations\")\n</code></pre> <p>This boxplot is organized exactly the same as the boxplots above. However, in this case, the data being plotted by the boxplots is using the pearson correlation between the LRR and LRB values for a particular TF, aggregating this data across all 62 TFs.</p> <p>This exhibits a similar overall trend with the 3x3 arrays of boxplots above containing the binned mean differences between the first and last bins. This is good news! It means that even without binning, there is somewhat of a trend observed between the LRR and LRB, even if it is not very significant. Looking at the boxplots, it appears that this trend is generally most evident in the chip_exo binding data. However, it is important to clarify that the chip_exo data has less than 100 rows of data in general. This means that for example, in the boxplot using the kemmeren perturbation data, a perfect correlation of 1.00 is achieved. Upon closer examination, this is due to the fact that the chip_exo data had only 2 rows of data associated with it, and when ranking the LRR and LRB, they both were assigned the same ranks resulting in a perfect pearson correlation. As such, it is important to keep in mind the property of the chip_exo binding data when assessing the plots in the bottom row as they may look much better than reality.</p> <p>Looking at the other two rows, it is more evident that the Calling Cards + mitra binding datasets produce better correlations than using the harbison binding data, as the medians of the boxplots in the top row are all greater than zero, whereas only one median in the bottom row is positive. It also seems that using the CC + mitra binding data and the kemmeren perturbation data yields the most identifiable trend within that row, while the harbison binding and mcisaac perturbation data produce the most positive correlations in that row. </p> <p>To obtain a more accurate idea of how the Pearson correlation boxplots should look like by incorporating data on the non-responsive genes for the chip_exo binding data, we can take the same approach used to create the boxplots above and modify the method in which the chip_exo data is accessed in order to include non-responsive data points for all of the non-responsive genes. Keep in mind that the enrichment and pvalues are chosen to be 0 and the smallest insignificant pvalue of 0.05, respectively. We will re-plot the 3x3 array of Pearson correlation boxplots to determine how the bottom row of boxplots will change.</p> <pre><code>async def process_transcription_factor_async(tf_name: str, is_aggregated: bool, binding_source: str, perturbation_source: str, pseudocount: int = 1) -&amp;gt; pd.DataFrame:    \n    \"\"\"\n    Process transcription factor data by retrieving and merging binding and perturbation datasets.\n\n    :param tf_name: The name of the transcription factor, e.g., \"AR080\".\n    :type tf_name: str\n    :param is_aggregated: Indicates whether the data is aggregated.\n    :type is_aggregated: bool\n    :param binding_source: The source of the binding data, e.g., \"cc\" or \"harbison\".\n    :type binding_source: str\n    :param perturbation_source: The source of the perturbation data, e.g., \"mcisaac\".\n    :type perturbation_source: str\n    :param pseudocount: The constant used in calculating enrichment and p-values scores to avoid division by zero, default is 1.\n    :type pseudocount: int, optional\n\n    :returns: A DataFrame containing the combined and processed binding and perturbation data.\n    :rtype: pd.DataFrame\n    \"\"\"\n    # Ensure the TF name is in uppercase to maintain consistency\n    tf_name_upper = tf_name.upper()\n\n    # Initialize API for binding data\n    pss_api_tf = PromoterSetSigAPI()\n\n    # Access the relevant data depending on the binding source and aggregation status\n    if binding_source == \"cc\":\n        if is_aggregated:\n            pss_api_tf.push_params({'regulator_symbol': tf_name_upper, \"datasource\": \"brent_nf_cc\", \"aggregated\": \"true\"})\n        else:\n            pss_api_tf.push_params({'regulator_symbol': tf_name_upper, \"workflow\": \"nf_core_callingcards_1_0_0\", \"data_usable\": \"pass\"})\n    elif binding_source == \"harbison\":\n        pss_api_tf.push_params({'regulator_symbol': tf_name_upper, \"source\": \"4\"})\n    elif binding_source == \"mitra\":\n        pss_api_tf.push_params({'regulator_symbol': tf_name_upper, \"source\": \"2\"})\n    elif binding_source == \"chip_exo\":\n        pss_api_tf.push_params({'regulator_symbol': tf_name_upper, \"source\": \"3\"})\n\n    # Asynchronously read the binding data from the API\n    tf_pss = await pss_api_tf.read(retrieve_files=True)\n    # Get the ID of the retrieved binding data\n    id = tf_pss.get(\"metadata\")[\"id\"][0]\n    # Extract the binding data using the ID\n    binding_df = tf_pss.get(\"data\").get(str(id))\n\n    # Initialize API for perturbation data\n    expression = ExpressionAPI()\n\n    # Map perturbation source to corresponding source number\n    source_mapping = {\n        \"mcisaac\": \"7\",\n        \"hu_reimann\": \"5\",\n        \"kemmeren\": \"6\"\n    }\n    source_number = source_mapping.get(perturbation_source, \"unknown\")\n\n    # Push parameters to retrieve the perturbation data\n    if perturbation_source == \"mcisaac\":\n        expression.push_params({\"regulator_symbol\": tf_name_upper, \"source\": source_number, \"time\": \"15\"})\n    else:\n        expression.push_params({\"regulator_symbol\": tf_name_upper, \"source\": source_number})\n\n    # Asynchronously read the perturbation data from the API\n    expression_res = await expression.read(retrieve_files=True)\n    # Get the ID of the retrieved perturbation data\n    id = expression_res.get(\"metadata\")[\"id\"][0]\n    # Extract the perturbation data using the ID\n    expression_df = expression_res.get(\"data\").get(str(id))\n\n    # Read perturbation data\n    perturbation_data = expression_df\n    # Read binding data\n    binding_data = binding_df\n\n    # Rename columns in binding data for consistency and clarity\n    if binding_source == \"cc\":\n        binding_data.rename(columns={\"callingcards_enrichment\": \"effect\", \"poisson_pval\": \"pvalue\"}, inplace=True)\n    elif binding_source == \"harbison\":\n        binding_data.rename(columns={\"pval\": \"pvalue\"}, inplace=True)\n    elif binding_source == \"mitra\":\n        binding_data.rename(columns={\"callingcards_enrichment\": \"effect\", \"poisson_pval\": \"pvalue\"}, inplace=True)\n    elif binding_source == \"chip_exo\":\n        binding_data.rename(columns={\"max_fc\": \"effect\", \"min_pval\": \"pvalue\"}, inplace=True)\n\n    # Optional: here you can modify the pseudocount as needed. The default pseudocount is set to 1.\n    # Calculate the effect size for binding data using the provided formula\n    if binding_source == \"cc\":\n        binding_data['effect'] = (binding_data['experiment_hops'] / binding_data['experiment_total_hops']) / \\\n                             ((binding_data['background_hops'] + pseudocount) / binding_data['background_total_hops'])\n\n    missing_values = set(perturbation_data[\"target_locus_tag\"]) - set(binding_data[\"target_locus_tag\"])\n\n    # Add missing rows to the binding data with enrichment = 0 and pvalue = 1\n    if missing_values:\n        missing_rows = pd.DataFrame({\n            'target_locus_tag': list(missing_values),\n            'effect': 0,\n            'pvalue': -4.322 #since this is for the chipexo data, we find log2 (0.05) \n        })\n        binding_data = pd.concat([binding_data, missing_rows], ignore_index=True)\n\n    # Merge the binding data and perturbation data on the 'target_locus_tag' column\n    combined_data = pd.merge(binding_data, perturbation_data, on='target_locus_tag', suffixes=('_binding', '_perturbation'))\n\n    # # Assert that the length of combined_data is the minimum of the lengths of binding_data and perturbation_data\n    # assert len(combined_data) &amp;lt;= min(len(binding_data), len(perturbation_data)), \\\n    #     f\"Length of combined_data ({len(combined_data)}) is not equal to the minimum of lengths of binding_data ({len(binding_data)}) and perturbation_data ({len(perturbation_data)})\"\n\n    # Keep only the necessary columns in the combined data\n    combined_data = combined_data[['target_locus_tag', 'effect_binding', 'effect_perturbation', 'pvalue_binding']]\n\n    # Reorder the combined data by the smallest 'pvalue_binding' values\n    combined_data = combined_data.sort_values(by='pvalue_binding')\n\n    # Apply transformations:\n    # - Take the absolute value of 'effect_perturbation'\n    # - Calculate the negative log10 of 'pvalue_binding'\n    # - Calculate the log10 of 'effect_binding'\n    combined_data['effect_perturbation'] = combined_data['effect_perturbation'].abs()\n    combined_data['neg_log_pvalue_binding'] = -np.log10(combined_data['pvalue_binding'])\n    combined_data['log_enrichment'] = np.log10(combined_data['effect_binding'])\n\n    # Return the processed combined data as a DataFrame\n    return combined_data\n\n</code></pre> <pre><code>filled_chip_exo_kemmeren_pearson_correlations = save_pearson_correlation_box_plot_comparisons(all_tfs, boolean_list, [\"chip_exo\"]*100, perturbation_sources = [\"kemmeren\"])\nfilled_chip_exo_mcisaac_pearson_correlations = save_pearson_correlation_box_plot_comparisons(all_tfs, boolean_list, [\"chip_exo\"]*100, perturbation_sources = [\"mcisaac\"])\nfilled_chip_exo_reimann_pearson_correlations = save_pearson_correlation_box_plot_comparisons(all_tfs, boolean_list, [\"chip_exo\"]*100, perturbation_sources = [\"hu_reimann\"])\n</code></pre> <pre><code>#new updating process dataframe\ndata = [new_cc_mcisaac_pearson_correlations['mcisaac'], new_cc_kemmeren_pearson_correlations['kemmeren'], new_cc_hu_reimann_pearson_correlations['hu_reimann'], new_harbison_mcisaac_pearson_correlations['mcisaac'], new_harbison_kemmeren_pearson_correlations['kemmeren'], new_harbison_hu_reimann_pearson_correlations['hu_reimann'], filled_chip_exo_mcisaac_pearson_correlations['mcisaac'], filled_chip_exo_kemmeren_pearson_correlations['kemmeren'], filled_chip_exo_reimann_pearson_correlations['hu_reimann']]\nbinding_labels = [\"cc+mitra\", \"harbison\", \"chip_exo w/ filled rows\"]\nperturbation_labels = [\"mcisaac\", \"kemmeren\", \"hu_reimann\"]\nplot_boxplots(data, binding_labels, perturbation_labels, \"correlations\")\n</code></pre> <pre><code>data = [new_cc_mcisaac_pearson_correlations['mcisaac'], new_cc_kemmeren_pearson_correlations['kemmeren'], new_cc_hu_reimann_pearson_correlations['hu_reimann'], new_harbison_mcisaac_pearson_correlations['mcisaac'], new_harbison_kemmeren_pearson_correlations['kemmeren'], new_harbison_hu_reimann_pearson_correlations['hu_reimann'], filled_chip_exo_mcisaac_pearson_correlations['mcisaac'], filled_chip_exo_kemmeren_pearson_correlations['kemmeren'], filled_chip_exo_reimann_pearson_correlations['hu_reimann']]\nbinding_labels = [\"cc+mitra\", \"harbison\", \"chip_exo w/ filled rows\"]\nperturbation_labels = [\"mcisaac\", \"kemmeren\", \"hu_reimann\"]\nplot_boxplots(data, binding_labels, perturbation_labels, \"correlations\")\n</code></pre> <p>Before we compare the last row of updated chip_exo data with the 3x3 array of boxplots above, it is important to note that the scale on these two boxplots is not the same. On the above array of boxplots, the vertical scale ranges from -1.00 to 1.00. Here, however, the scale is halved, ranging only from -0.50 to 0.50. With this in mind, it is more apparent that adding the non-responsive genes to the chip_exp binding data results in weaker positive correlations between the LRR and LRB. This is likely due to the fact that many new rows of data representing non-responsible genes have been added, which greatly outnumber the original amount of data in the chip_exo binding data. Thus, these boxplots seem more plausible, and it is good that they continue to show a somewhat positive correlation, albeit not as extreme as before.</p> <p>Our above analysis focused on ranking the binding and pertubation data in the same way each time. However, due to the nature of the data, there are often identical values which are assigned the same ranking in both the binding and perturbation data.</p> <p>As a reminder, the binding data was ranked according to the poisson pvalue, even through two other metrics exist: the binding enrichment score and the hypergeometric pvalue. It is worth considering whether alternative ranking approaches can result in less ties and ultimately, more desirable trends as observed on the boxplots of the binned mean data differences and Pearson correlations.</p> <p>In the data exploration above, we solely chose to rank the perturbation data according to the magnitude of the perturbation effect. For the mcisaac perturbation data specifically, there exist mutiple timepoints in which there is effect data. The idea of assigning ranks to the data by averaging between two perturbation sets is another method that we can explore to determine whether this improve the binned boxplots above. </p> <p>It\u2019s interesting to consider whether taking an average ranking might result in a stronger trend being depicted on the boxplots for the Pearson correlation or first and last binned mean differences. The motivation for this comes from the fact that in the plots above, the highest value along the y-axis is around -3, and when taking the reverse of the negative log this results in a rank around 1000. If the highest rank is 1000, this implies many ties occuring between data points which can result in a lower resolution of the desired trend. Thus, it is worth exploring whether assigning each gene an average rank across the different timepoints reported in the mcisaac data may produce better rankings. The following methods below will perform this averaging of the ranks on the mcisaac perturbation data across 4 timepoints and then combine the perturbation and binding datasets since the original methods do not support the implementation of this ranking method. Note that we will use the CC + mitra data as the binding data in all of the perturbation ranking experiments.</p> <pre><code>async def process_perturbation_data_async(tf_name: str, perturbation_source: str) -&amp;gt; pd.DataFrame:\n    \"\"\"\n    Process perturbation data by retrieving data for different timepoints, ranking genes,\n    and calculating the average rank for each gene across timepoints.\n\n    :param tf_name: The name of the transcription factor, e.g., \"AR080\".\n    :type tf_name: str\n    :param perturbation_source: The source of the perturbation data, e.g., \"mcisaac\".\n    :type perturbation_source: str\n\n    :returns: A DataFrame containing the genes and their average rankings.\n    :rtype: pd.DataFrame\n    \"\"\"\n    # Ensure the TF name is in uppercase to maintain consistency\n    tf_name_upper = tf_name.upper()\n\n    # Initialize API for perturbation data\n    expression = ExpressionAPI()\n\n    if perturbation_source == \"mcisaac\":\n        timepoints = [\"15\", \"30\", \"45\", \"90\"]\n        all_timepoint_dfs = []\n\n        for time in timepoints:\n            source_mapping = {\n                \"mcisaac\": \"7\",\n                \"hu_reimann\": \"5\",\n                \"kemmeren\": \"6\"\n            }\n            source_number = source_mapping.get(perturbation_source, \"unknown\")\n\n            # Push parameters to retrieve the perturbation data\n            if perturbation_source == \"mcisaac\":\n                expression.push_params({\"regulator_symbol\": tf_name_upper, \"source\": source_number, \"time\": \"15\"})\n            else:\n                expression.push_params({\"regulator_symbol\": tf_name_upper, \"source\": source_number})\n            expression_res = await expression.read(retrieve_files=True)\n            id = expression_res.get(\"metadata\")[\"id\"][0]\n            expression_df = expression_res.get(\"data\").get(str(id))\n            expression_df['time'] = time\n            expression_df['effect'] = expression_df['effect'].abs()\n            all_timepoint_dfs.append(expression_df)\n\n        combined_expression_df = pd.concat(all_timepoint_dfs)\n\n        # Rank genes based on the perturbation effect within each timepoint\n        ranked_dfs = []\n        for time in timepoints:\n            timepoint_df = combined_expression_df[combined_expression_df['time'] == time].copy()\n            timepoint_df['rank'] = rankdata(-abs(timepoint_df['effect']), method='average') \n            ranked_dfs.append(timepoint_df)\n\n        # Combine ranked dataframes\n        ranked_combined_df = pd.concat(ranked_dfs)\n\n        avg_ranks = {}\n        for gene in ranked_combined_df['target_locus_tag'].unique():\n            gene_data = ranked_combined_df[ranked_combined_df['target_locus_tag'] == gene]\n            avg_rank = gene_data['rank'].mean()\n            avg_effect = gene_data['effect'].mean()\n            avg_ranks[gene] = (avg_effect, avg_rank)\n\n        avg_ranks_df = pd.DataFrame(list(avg_ranks.items()), columns=['target_locus_tag', 'values'])\n        avg_ranks_df[['effect', 'avg_rank']] = pd.DataFrame(avg_ranks_df['values'].tolist(), index=avg_ranks_df.index)\n        avg_ranks_df = avg_ranks_df.drop(columns=['values'])\n        avg_ranks_df['neg_expression_rank_log'] = -np.log10(avg_ranks_df['avg_rank'])\n\n        return avg_ranks_df\n</code></pre> <pre><code>def process_perturbation_data(tf_name: str, perturbation_source: str) -&amp;gt; pd.DataFrame:\n    \"\"\"\n    Processes transcription factor data synchronously by invoking an asynchronous function.\n\n    This function runs the asynchronous `process_transcription_factor_async` function synchronously to handle \n    transcription factor data processing. It retrieves the event loop, runs the asynchronous function, \n    and returns the processed DataFrame.\n\n    :param tf_name: The name of the transcription factor.\n    :type tf_name: str\n    :param is_aggregated: A boolean flag indicating whether the data is aggregated.\n    :type is_aggregated: bool\n    :param perturbation_source: The source of the perturbation data.\n    :type perturbation_source: str\n    :param pseudocount: The constant used in calculating enrichment and p-values scores to avoid division by zero, default is 1.\n    :type pseudocount: int, optional\n\n    :returns: A DataFrame containing the processed transcription factor data.\n    :rtype: pd.DataFrame\n    \"\"\"\n    loop = asyncio.get_event_loop()\n    return loop.run_until_complete(process_perturbation_data_async(tf_name, perturbation_source))\n</code></pre> <pre><code>async def access_binding_data_async(tf_name: str, is_aggregated: bool, binding_source: str, pseudocount: Optional[int] = 1) -&amp;gt; pd.DataFrame:    \n    \"\"\"\n    Process transcription factor data by retrieving and merging binding and perturbation datasets.\n\n    :param tf_name: The name of the transcription factor, e.g., \"AR080\".\n    :type tf_name: str\n    :param is_aggregated: Indicates whether the data is aggregated. You can check if the TF belongs to the list above.\n    :type is_aggregated: bool\n    :param perturbation_source: The source of the perturbation data, e.g., \"mcisaac\".\n    :type perturbation_source: str\n    :param pseudocount: The constant used in calculating enrichment and p-values scores to avoid division by zero, default is 1.\n    :type pseudocount: int, optional\n\n    :returns: A DataFrame containing the combined and processed binding and perturbation data.\n    :rtype: pd.DataFrame\n    \"\"\"\n    # Ensure the TF name is in uppercase to maintain consistency\n    tf_name_upper = tf_name.upper()\n\n    # Initialize API for binding data\n    pss_api_tf = PromoterSetSigAPI()\n\n    # Access the relevant data depending on whether the data is aggregated or not\n    if binding_source == \"cc\":\n        if is_aggregated:\n            pss_api_tf.push_params({'regulator_symbol': tf_name_upper, \"datasource\": \"brent_nf_cc\", \"aggregated\": \"true\"})\n        else:\n            pss_api_tf.push_params({'regulator_symbol': tf_name_upper, \"workflow\": \"nf_core_callingcards_1_0_0\", \"data_usable\": \"pass\"})\n    elif binding_source == \"harbison\":\n        pss_api_tf.push_params({'regulator_symbol': tf_name_upper, \"source\": \"4\"})\n    elif binding_source == \"mitra\":\n        pss_api_tf.push_params({'regulator_symbol': tf_name_upper, \"source\": \"2\"})\n\n    # Asynchronously read the binding data from the API\n    tf_pss = await pss_api_tf.read(retrieve_files=True)\n    # Get the ID of the retrieved binding data\n    id = tf_pss.get(\"metadata\")[\"id\"][0]\n    # Extract the binding data using the ID\n    binding_df = tf_pss.get(\"data\").get(str(id))\n\n    # Calculate binding rank with average ties method\n    if binding_source == \"cc\":\n        binding_df['binding_rank'] = rankdata(binding_df['poisson_pval'], method='average')\n    elif binding_source == \"harbison\":\n        binding_df['binding_rank'] = rankdata(binding_df['pval'], method='average')\n    elif binding_source == \"mitra\":\n        binding_df['binding_rank'] = rankdata(binding_df['poisson_pval'], method='average')\n\n    # Calculate log transform of the binding rank\n    binding_df['neg_log_rank_binding'] = -np.log10(rankdata(binding_df['binding_rank'], method='average'))\n\n    return binding_df\n</code></pre> <pre><code>def access_binding_data(tf_name: str, is_aggregated: bool, binding_source: str, pseudocount: Optional[int] = 1) -&amp;gt; pd.DataFrame:\n    \"\"\"\n    Processes transcription factor data synchronously by invoking an asynchronous function.\n\n    This function runs the asynchronous `process_transcription_factor_async` function synchronously to handle \n    transcription factor data processing. It retrieves the event loop, runs the asynchronous function, \n    and returns the processed DataFrame.\n\n    :param tf_name: The name of the transcription factor.\n    :type tf_name: str\n    :param is_aggregated: A boolean flag indicating whether the data is aggregated.\n    :type is_aggregated: bool\n    :param perturbation_source: The source of the perturbation data.\n    :type perturbation_source: str\n    :param pseudocount: The constant used in calculating enrichment and p-values scores to avoid division by zero, default is 1.\n    :type pseudocount: int, optional\n\n    :returns: A DataFrame containing the processed transcription factor data.\n    :rtype: pd.DataFrame\n    \"\"\"\n    loop = asyncio.get_event_loop()\n    return loop.run_until_complete(access_binding_data_async(tf_name, is_aggregated, binding_source, pseudocount))\n</code></pre> <pre><code>def process_and_merge_data(tf_name: str, is_aggregated: bool, binding_source: str, perturbation_source: str, pseudocount: Optional[int] = 1) -&amp;gt; pd.DataFrame:\n    \"\"\"\n    Process binding and perturbation data and merge them.\n\n    :param tf_name: The name of the transcription factor, e.g., \"AR080\".\n    :type tf_name: str\n    :param is_aggregated: Indicates whether the data is aggregated.\n    :type is_aggregated: bool\n    :param perturbation_source: The source of the perturbation data, e.g., \"mcisaac\".\n    :type perturbation_source: str\n    :param pseudocount: The constant used in calculating enrichment and p-values scores to avoid division by zero, default is 1.\n    :type pseudocount: int, optional\n\n    :returns: Merged DataFrame with the specified columns.\n    :rtype: pd.DataFrame\n    \"\"\"\n    binding_df = access_binding_data(tf_name, is_aggregated, binding_source, pseudocount)\n    perturbation_df = process_perturbation_data(tf_name, perturbation_source)\n    # Merge the dataframes on 'regulator_locus_tag'\n    merged_df = pd.merge(binding_df, perturbation_df, on='target_locus_tag')\n\n    # Select the desired columns\n    result_df = merged_df[['target_locus_tag', 'neg_log_rank_binding', 'neg_expression_rank_log']]    \n    return result_df\n</code></pre> <pre><code>#this is how you would access the combined binding / perturbation data using this ranking approach\ncombined_data = process_and_merge_data(\"ARO80\", False, \"cc\", \"mcisaac\")\ncombined_data.head()\n</code></pre> target_locus_tag neg_log_rank_binding neg_expression_rank_log 0 YAL069W -3.231470 -3.495128 1 YAL068C -3.056524 -3.495128 2 YAL067C -3.160769 -3.495128 3 YAL066W -2.957847 -3.495128 4 YAL065C -2.548389 -3.495128 <p>Let\u2019s investigate the correlations between the LRR and LRB when ranking according to this scheme. We will need to create slightly different methods to accomodate for this new way of ranking the data.</p> <pre><code>def save_ranked_pearson_correlation_box_plot_comparisons(tfs: List[str], boolean_list: List[bool], binding_source: List[str], perturbation_sources: List[str], pseudocount: Optional[int] = 1) -&amp;gt; dict:\n    \"\"\"\n    Calculates the Pearson correlation coefficient between the 'LRR' and 'LRB' columns for each transcription factor (TF) across multiple perturbation sources.\n\n    :param tfs: A list of transcription factors to analyze.\n    :type tfs: List[str]\n    :param boolean_list: A list of boolean values indicating whether the data is aggregated for each transcription factor.\n    :type boolean_list: List[bool]\n    :param binding_source: A list of sources for the binding data.\n    :type binding_source: List[str]\n    :param perturbation_sources: A list of sources for the perturbation data.\n    :type perturbation_sources: List[str]\n    :param pseudocount: The constant used in calculating enrichment and p-values scores to avoid division by zero, default is 1.\n    :type pseudocount: Optional[int]\n\n    :returns: A dictionary where keys are perturbation sources and values are lists of Pearson correlation coefficients for each TF.\n    :rtype: Dict[str, List[float]]\n    \"\"\"\n    # Initialize a dictionary to store Pearson correlation coefficients for each perturbation source\n    correlation_data = {source: [] for source in perturbation_sources}\n\n    # Suppress RuntimeWarnings for the duration of the following operations\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n\n        # Iterate over each transcription factor in the list\n        for i in range(len(tfs)):\n            for source in perturbation_sources:\n                # Further process the combined data to calculate ranks and transformations\n                plotting_df = process_and_merge_data(str(tfs[i]), boolean_list[i], binding_source[i], source, pseudocount)\n\n                # Ensure there are no NaN values in the 'LRR' and 'LRB' columns before calculating Pearson correlation\n                plotting_df = plotting_df.dropna(subset=['neg_log_rank_binding', 'neg_expression_rank_log'])\n\n                # Calculate Pearson correlation if there are at least two valid data points\n                if len(plotting_df) &amp;gt;= 2:\n                    correlation, _ = pearsonr(plotting_df['neg_log_rank_binding'], plotting_df['neg_expression_rank_log'])\n                    correlation_data[source].append(correlation)\n                else:\n                    correlation_data[source].append(float('nan'))\n\n    # Remove NaN values from all correlation lists\n    for source in correlation_data:\n        correlation_data[source] = [x for x in correlation_data[source] if not pd.isnull(x)]\n\n    return correlation_data\n\n</code></pre> <pre><code>def adjacent_differences_store_ranked_data(tfs: List[str], boolean_list: List[bool], binding_source: List[str], perturbation_sources: List[str], bins: int, pseudocount: Optional[int] = 1) -&amp;gt; dict:\n    \"\"\"\n    Stores the differences between adjacent bins for a list of transcription factors.\n\n    This function processes transcription factor data, calculates differences between the means of adjacent bins,\n    and stores these differences across multiple transcription factors.\n\n    :param tfs: A list of transcription factors that you want to plot.\n    :type tfs: List[str]\n    :param boolean_list: A list of boolean values indicating whether the data is aggregated for each transcription factor.\n    :type boolean_list: List[bool]\n    :param perturbation_sources: A list of sources of the perturbation data.\n    :type perturbation_sources: List[str]\n    :param bins: The number of bins to create.\n    :type bins: int\n    :param pseudocount: The constant used in calculating enrichment and p-values scores to avoid division by zero, default is 1.\n    :type pseudocount: Optional[int]\n\n    :returns: A dictionary containing the stored data for each perturbation source.\n    :rtype: dict\n    \"\"\"\n    # Initialize a dictionary to store differences between adjacent bins for each perturbation source\n    diff_data = {source: [[] for _ in range(bins - 1)] for source in perturbation_sources}\n\n    # Suppress RuntimeWarnings for the duration of the following operations\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n\n        # Iterate over each transcription factor in the list\n        for i in range(len(tfs)):\n            print(str(i) + tfs[i])\n            for source in perturbation_sources:\n                # Process the transcription factor data\n                plotting_df = process_and_merge_data(str(tfs[i]), boolean_list[i], binding_source[i], source, pseudocount)\n\n                # Create bins for the 'neg_log_rank_binding' column using the specified number of bins\n                plotting_df['bin'] = create_bins(plotting_df, 'neg_log_rank_binding', num_bins=bins)\n\n                # Calculate the mean of 'neg_expression_rank_log' for each bin\n                binned_means = plotting_df.groupby('bin', observed=True)['neg_expression_rank_log'].mean().reset_index()\n\n                # Initialize a list to store the differences between adjacent bins\n                binned_mean_diffs = []\n\n                # Calculate the differences between the means of adjacent bins\n                for j in range(bins - 1):\n                    binned_mean_diffs.append(binned_means[\"neg_expression_rank_log\"][j+1] - binned_means[\"neg_expression_rank_log\"][j])\n\n                # Append the differences to the corresponding list in diff_data\n                for j in range(bins - 1):\n                    diff_data[source][j].append(binned_mean_diffs[j])\n\n    # Remove NaN values from all bin difference lists\n    for source in diff_data:\n        diff_data[source] = [[x for x in bin_diff if not pd.isnull(x)] for bin_diff in diff_data[source]]\n\n    return diff_data\n</code></pre> <pre><code>def first_last_differences_store_ranked_data(tfs: List[str], boolean_list: List[bool], binding_source: List[str], perturbation_sources: List[str], bins: int, pseudocount: Optional[int] = 1) -&amp;gt; dict:\n    \"\"\"\n    Stores the differences between adjacent bins for a list of transcription factors.\n\n    This function processes transcription factor data, calculates differences between the means of adjacent bins,\n    and stores these differences across multiple transcription factors.\n\n    :param tfs: A list of transcription factors that you want to plot.\n    :type tfs: List[str]\n    :param boolean_list: A list of boolean values indicating whether the data is aggregated for each transcription factor.\n    :type boolean_list: List[bool]\n    :param perturbation_sources: A list of sources of the perturbation data.\n    :type perturbation_sources: List[str]\n    :param bins: The number of bins to create.\n    :type bins: int\n    :param pseudocount: The constant used in calculating enrichment and p-values scores to avoid division by zero, default is 1.\n    :type pseudocount: Optional[int]\n\n    :returns: A dictionary containing the stored data for each perturbation source.\n    :rtype: dict\n    \"\"\"\n    # Initialize a dictionary to store differences between the first and last bins for each perturbation source\n    diff_data = {source: [] for source in perturbation_sources}\n\n    # Suppress RuntimeWarnings for the duration of the following operations\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n\n        # Iterate over each transcription factor in the list\n        for i in range(len(tfs)):\n            print(tfs[i])\n            for source in perturbation_sources:\n                # Further process the combined data to calculate ranks and transformations\n                plotting_df = process_and_merge_data(str(tfs[i]), boolean_list[i], binding_source[i], source, pseudocount)\n\n                # Create bins for the 'neg_log_rank_binding' column using the specified number of bins\n                plotting_df['bin'] = create_bins(plotting_df, 'neg_log_rank_binding', num_bins=bins)\n\n                # Calculate the mean of 'neg_expression_rank_log' for each bin\n                binned_means = plotting_df.groupby('bin', observed=True)['neg_expression_rank_log'].mean().reset_index()\n\n                # Calculate the difference between the first and last bin means\n                first_last_diff = binned_means[\"neg_expression_rank_log\"].iloc[-1] - binned_means[\"neg_expression_rank_log\"].iloc[0]\n\n                # Append the difference to the corresponding list in diff_data\n                diff_data[source].append(first_last_diff)\n\n    # Remove NaN values from all bin difference lists\n    for source in diff_data:\n        diff_data[source] = [x for x in diff_data[source] if not pd.isnull(x)]\n\n    return diff_data\n</code></pre> <pre><code>all_tfs = ['WTM1','MIG2','RIM101','GZF3','ASH1','GAT3','TEC1','SIP3','SKN7','WTM2','HAA1','MET31','CRZ1','CHA4','ZAP1','SKO1','ACA1','FZF1','HAP2','HAP3','HAP5','INO4','ERT1','PPR1','RTG1','MOT3','CBF1','MSN2','DAL80','RTG3','GAL80','RSF2','RME1','HIR2','SIP4','HAP4','UME1','USV1','MGA1','CIN5','ROX1','XBP1','RDR1','PDR3','RLM1','SFL1','SMP1','SUT2','PHD1','SUT1','SOK2','STP2','YRR1','GAL4','LEU3','OAF1','SWI6','ACE2','TYE7','RGM1','GCN4','MIG3','STB5','RFX1','ARG80','ARG81','CST6','AZF1','SFP1','GTS1','FKH1','YOX1','FKH2','DIG1','MET28','RGT1','GCR2']\nboolean_list = [True]*41 + [False]*36\ncc_to_mitra_ratio_in_all = [\"cc\"]*49+[\"mitra\"]*28\n</code></pre> <p>First we will store the data for boxplots based on ranking the perturbation data by taking the average across timepoints in the mcisaac dataset.</p> <pre><code>cc_mcisaac_averagebymcisaac_pearson_correlations = save_ranked_pearson_correlation_box_plot_comparisons(all_tfs, boolean_list, cc_to_mitra_ratio_in_all, perturbation_sources = [\"mcisaac\"])\n</code></pre> <pre><code>cc_mcisaac_averagebymcisaactimes_first_last_data = first_last_differences_store_ranked_data(all_tfs, boolean_list, cc_to_mitra_ratio_in_all, ['mcisaac'], bins = 5)\n</code></pre> <p>Next, we will rank the perturbation data normally by the mcisaac data and store it as well.</p> <pre><code>cc_mcisaac_regular_first_last_data = first_last_bin_difference_box_plot_comparisons(all_tfs, boolean_list, cc_to_mitra_ratio_in_all, ['mcisaac'], 5)\n</code></pre> <p>An alternative approach to ranking the perturbation data is to average the rank assigned to a particular gene/TF pair between both the mcisaac 15 minute and kemmeren perturbation data. This approach now incorporates two separate perturbation datasets, with the same philosophy of averaging the ranks to ideally reduce noise and produce better data.</p> <p>To perform a ranking using both the mcisaac 15 minute data and the kemmeren perturbation data, we need to modify the following method so that it performs the correct operation. Then, we will rerun the same method to see how taking this average affects the outcomes in the data.</p> <pre><code>async def process_perturbation_data_async(tf_name: str, perturbation_source: str) -&amp;gt; pd.DataFrame:\n    \"\"\"\n    Process perturbation data by retrieving data from McIsaac and Kemmeren datasets,\n    ranking genes, and calculating the average rank for each gene across both datasets.\n\n    :param tf_name: The name of the transcription factor, e.g., \"AR080\".\n    :type tf_name: str\n    :param perturbation_source: The source of the perturbation data, e.g., \"mcisaac\".\n    :type perturbation_source: str\n\n    :returns: A DataFrame containing the genes, their average rankings, and the negative log of these rankings.\n    :rtype: pd.DataFrame\n    \"\"\"\n    # Ensure the TF name is in uppercase to maintain consistency\n    tf_name_upper = tf_name.upper()\n\n    # Initialize API for perturbation data\n    expression = ExpressionAPI()\n\n    # Get the McIsaac data\n    expression.push_params({\"regulator_symbol\": tf_name_upper, \"source\": \"7\", \"time\": \"15\"})\n    expression_res = await expression.read(retrieve_files=True)\n    id = expression_res.get(\"metadata\")[\"id\"][0]\n    mcisaac_df = expression_res.get(\"data\").get(str(id))\n    mcisaac_df['rank'] = rankdata(-abs(mcisaac_df['effect']), method='average')\n\n    # Get the Kemmeren data\n    expression2 = ExpressionAPI()\n    expression2.push_params({\"regulator_symbol\": tf_name_upper, \"source\": \"6\"})\n    expression_res2 = await expression2.read(retrieve_files=True)\n    id = expression_res2.get(\"metadata\")[\"id\"][0]\n    kemmeren_df = expression_res2.get(\"data\").get(str(id))\n    kemmeren_df['rank'] = rankdata(-abs(kemmeren_df['effect']), method='average')\n\n    # Merge dataframes on 'target_locus_tag'\n    merged_df = pd.merge(mcisaac_df[['target_locus_tag', 'rank']], \n                         kemmeren_df[['target_locus_tag', 'rank']], \n                         on='target_locus_tag', \n                         suffixes=('_mcisaac', '_kemmeren'))\n\n    # Calculate average rank\n    merged_df['avg_rank'] = merged_df[['rank_mcisaac', 'rank_kemmeren']].mean(axis=1)\n\n    # Calculate negative log of average rank\n    merged_df['neg_expression_rank_log'] = -np.log10(merged_df['avg_rank'])\n\n    # Select and return the desired columns\n    result_df = merged_df[['target_locus_tag', 'avg_rank', 'neg_expression_rank_log']]\n\n    return result_df\n</code></pre> <pre><code>cc_mcisaac_averagemcisaackemmeren_pearson_correlations = save_ranked_pearson_correlation_box_plot_comparisons(all_tfs, boolean_list, cc_to_mitra_ratio_in_all, perturbation_sources = [\"mcisaac\"])\n</code></pre> <pre><code>cc_mcisaac_averagemcisaackemmeren_first_last_data = first_last_differences_store_ranked_data(all_tfs, boolean_list, cc_to_mitra_ratio_in_all, ['mcisaac'], bins = 5)\n</code></pre> <p>Next, we will rank the perturbation data normally by the kemmeren data and store it as well.</p> <pre><code>cc_kemmeren_regular_first_last_data = first_last_bin_difference_box_plot_comparisons(all_tfs, boolean_list, cc_to_mitra_ratio_in_all, ['kemmeren'], 5)\n</code></pre> <p>Now let\u2019s compare the boxplots across all of the various ranking methods. We include both the data obtained from ranking using the mcisaac 15 minutes and kemmeren data as references. As a reminder, the CC + mitra dataset is used as the binding dataset across all approaches to ensure comparability. We will plot the array of boxplots for both the Pearson correlation between LRR/LRB and the boxplots for the first and last binned mean differences.</p> <pre><code>def plot_combined_boxplot(data: List[List[float]], labels: List[str]) -&amp;gt; None:\n    \"\"\"\n    Plots a single boxplot with specified labels for each dataset.\n\n    :param data: A list of lists containing numerical data for each boxplot.\n    :type data: List[List[float]]\n    :param labels: A list containing labels for each dataset.\n    :type labels: List[str]\n\n    :returns: None\n    :rtype: None\n    \"\"\"\n    # Create a boxplot for all datasets combined\n    plt.figure(figsize=(12, 8))\n    plt.boxplot(data, labels=labels)\n\n    # Add a dashed line at y=0\n    plt.axhline(y=0, color='grey', linestyle='--')\n\n    # Set y-axis limits\n    plt.ylim(-0.1, 0.4)\n\n    # Set labels\n    plt.ylabel('Pearson Correlations Between LRR and LRB')\n    plt.xlabel('Ranking Method')\n    plt.title('Comparison of Pearson Correlations Between LRR and LRB Across Various Ranking Methods for 78 TFs')\n\n    # Show plot\n    plt.tight_layout()\n    plt.show()\n</code></pre> <pre><code>data = [cc_mcisaac_pearson_correlations['mcisaac'], cc_mcisaac_averagebymcisaac_pearson_correlations['mcisaac'], cc_mcisaac_averagemcisaackemmeren_pearson_correlations['mcisaac'], cc_kemmeren_pearson_correlations['kemmeren']]\nbinding_labels = [\"cc+mitra\", \"harbison\"]\nperturbation_labels = [\"mcisaac\", \"avg: mcisaac timepts\", \"avg: mcisaac &amp;amp; kemmeren\",\"kemmeren\"]\nplot_combined_boxplot(data, perturbation_labels)\n</code></pre> <pre>\n<code>/var/folders/25/6s7q5c9j40373whzd3q5s20m0000gn/T/ipykernel_1992/4116603215.py:14: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n  plt.boxplot(data, labels=labels)\n</code>\n</pre> <p>The plot above compares the boxplots of the Pearson correlations between the LRR and LRB across the various perturbation ranking approaches. On the x-axis, the label below the boxplot indicates the method in which the perturbation data was ranked to produce the final LRR/LRB correlation value. The y-axis then plots a partial scale of the Pearson correlation values from -0.1 to 0.4 as all of the data in the boxplots is confined to this region. Upon immediate observation, it is evident that using the normal ranking of the perturbation magnitude for the mcisaac 15 minute data (leftmost boxplot) yields a similar boxplot to that of averaging the ranks of the perturbation magnitudes across the 4 mcisaac timepoints (second to left boxplot). The other two boxplots suggest that the Pearson correlations generated according to those approaches tend to show weaker Pearson correlations as the overall boxplots are shifted vertically downwards suggusting a spread of correlations that are closer to 0. However, given that the boxplots of the normal perturbation ranking on the mcisaac 15 minute data is so similar to that of averaging the rankings across the 4 mcisaac timepoints, it suggests that taking the extra step to perform this average ranking is not enough to produce a noticeable improvement in the Pearson correlations. </p> <p>We can also plot the boxplots of the first and last binned mean differences from binning the data.</p> <pre><code>compare_first_and_last_stored_data_box_plots([cc_mcisaac_regular_first_last_data, cc_mcisaac_averagebymcisaactimes_first_last_data,cc_mcisaac_averagemcisaackemmeren_first_last_data, cc_kemmeren_regular_first_last_data], ['mcisaac','avg: mcisaac timepts', 'avg: mcisaac &amp;amp; kem', \"kemmeren\"])\n</code></pre> <p>Here, it appears that using the normal ranking of the perturbation effects on the mcisaac 15 minute data (leftmost boxplot) or the normal ranking of the perturbation effects on the kemmeren data (rightmost boxplot) surprisingly yield the best results. This is because we would expect the more complex ranking approaches to hopefully reduce ties in the perturbation data, leading to better rankings and clearer trends. Yet, these boxplots suggest that either using the normal mcisaac ranking, which produces a greater spread of data, or using the kemmeren normal ranking, which produces a smaller spread of data but has a lower minimum and maximum, could both be good options to rank the data. </p> <p>Overall, from these two boxplots, it appears that performing the extra steps to re-rank the data either by averaging ranks across various mcisaac timepoints or by averaging between the mcisaac and kemmeren perturbation data do not result in better trends in the data as evidenced by the boxplot comparisons. </p> <p>On the binding side, it\u2019s worth further exploring the which ranking method is most optimal. We essentially have 3 options: using the enrichment values, poisson pvalues, or hypergeometric pvalues to rank the binding data by. We have currently been ranking the data according to the poisson pvalues, but given that multiple experiment outcomes can produce the same pvalue, resulting to many ties that may decrease the resolution of the trends we are graphing, it is worth considering if the other two ranking methods can produce higher average ranks that result in less ties, hopefully producing more observable trends. The boxplots below are generated by taking the highest rank by each of the three methods on each TF, and generating this data across the entire Calling Cards TF pool to determine which may produce the highest average ranks.</p> <pre><code>async def fetch_binding_data_async(tf_name: str, is_aggregated: bool, pseudocount: int = 1) -&amp;gt; pd.DataFrame:\n    \"\"\"\n    Fetch binding data for a transcription factor and compute the 'effect' size.\n\n    :param tf_name: The name of the transcription factor.\n    :type tf_name: str\n    :param is_aggregated: Indicates whether the data is aggregated.\n    :type is_aggregated: bool\n    :param pseudocount: The constant used in calculating enrichment scores to avoid division by zero, default is 1.\n    :type pseudocount: int\n\n    :returns: A DataFrame containing the binding data with 'effect' computed.\n    :rtype: pd.DataFrame\n    \"\"\"\n\n    tf_name_upper = tf_name.upper()\n    pss_api_tf = PromoterSetSigAPI()\n\n    if is_aggregated:\n        pss_api_tf.push_params({'regulator_symbol': tf_name_upper, \"datasource\": \"brent_nf_cc\", \"aggregated\": \"true\"})\n    else:\n        pss_api_tf.push_params({'regulator_symbol': tf_name_upper, \"workflow\": \"nf_core_callingcards_1_0_0\", \"data_usable\": \"pass\"})\n\n    tf_pss = await pss_api_tf.read(retrieve_files=True)\n    id = tf_pss.get(\"metadata\")[\"id\"][0]\n    binding_df = tf_pss.get(\"data\").get(str(id))\n\n    binding_data = binding_df\n    binding_data.rename(columns={\"callingcards_enrichment\": \"effect\"}, inplace=True)\n    binding_data['effect'] = (binding_data['experiment_hops'] / binding_data['experiment_total_hops']) / \\\n                             ((binding_data['background_hops'] + pseudocount) / binding_data['background_total_hops'])\n\n    return binding_data\n\ndef fetch_binding_data(tf_name: str, is_aggregated: bool, pseudocount: int = 1) -&amp;gt; pd.DataFrame:\n    loop = asyncio.get_event_loop()\n    return loop.run_until_complete(fetch_binding_data_async(tf_name, is_aggregated, pseudocount))\n\ndef assign_rankings(binding_data: pd.DataFrame) -&amp;gt; pd.DataFrame:\n    \"\"\"\n    Assign rankings to the binding data based on 'effect', 'poisson_pvalue', and 'hypergeometric_pvalue'.\n\n    :param binding_data: The binding data to rank.\n    :type binding_data: pd.DataFrame\n\n    :returns: A DataFrame with rankings assigned.\n    :rtype: pd.DataFrame\n    \"\"\"\n    binding_data['rank_effect'] = binding_data['effect'].rank(ascending=False, method='average')\n    binding_data['rank_poisson'] = binding_data['poisson_pval'].rank(ascending=True, method='average')\n    binding_data['rank_hypergeometric'] = binding_data['hypergeometric_pval'].rank(ascending=True, method='average')\n\n    return binding_data\n\ndef find_smallest_ranks(binding_data: pd.DataFrame) -&amp;gt; pd.Series:\n    \"\"\"\n    Find the smallest rank for each ranking method.\n\n    :param binding_data: The binding data with rankings.\n    :type binding_data: pd.DataFrame\n\n    :returns: A Series containing the smallest ranks for each method.\n    :rtype: pd.Series\n    \"\"\"\n    smallest_ranks = {\n        'rank_effect': binding_data['rank_effect'].min(),\n        'rank_poisson': binding_data['rank_poisson'].min(),\n        'rank_hypergeometric': binding_data['rank_hypergeometric'].min()\n    }\n\n    return pd.Series(smallest_ranks)\n\ndef compare_tf_rankings(tfs: list, bool_list: list, binding_source: str, pseudocount: int = 1):\n    \"\"\"\n    Compare the smallest ranks across multiple transcription factors and generate boxplots.\n\n    :param tfs: A list of transcription factor names.\n    :type tfs: list\n    :param is_aggregated: Indicates whether the data is aggregated.\n    :type is_aggregated: bool\n    :param pseudocount: The constant used in calculating enrichment scores to avoid division by zero, default is 1.\n    :type pseudocount: int\n    \"\"\"\n    all_ranks_effect = []\n    all_ranks_poisson = []\n    all_ranks_hypergeometric = []\n\n    for i in range(len(tfs)):\n        binding_data = fetch_binding_data(tfs[i], bool_list[i], pseudocount)\n        ranked_data = assign_rankings(binding_data)\n        smallest_ranks = find_smallest_ranks(ranked_data)\n\n        all_ranks_effect.append(smallest_ranks['rank_effect'])\n        all_ranks_poisson.append(smallest_ranks['rank_poisson'])\n        all_ranks_hypergeometric.append(smallest_ranks['rank_hypergeometric'])\n\n    # Generate boxplots\n    plt.figure(figsize=(12, 8))\n    plt.boxplot([all_ranks_effect, all_ranks_poisson, all_ranks_hypergeometric], labels=['Effect', 'Poisson p-value', 'Hypergeometric p-value'])\n    plt.axhline(y=0, color='gray', linestyle='--')  # Add a horizontal dotted line at y=0\n    plt.xlabel('Ranking Method')\n    plt.ylabel('Smallest Rank')\n    plt.title(f'Comparison of Smallest Ranks Across TFs on {binding_source} data')\n    plt.show()\n</code></pre> <p>Here we are plotting the original boxplots but ranking the binding data using the enrichment values instead of the poisson pvalues to see how the boxplots will change. First, we need to make a slight change to the process_dataframe method so that it ranks the enrichment scores instead of the pvalues</p> <pre><code>def process_dataframe(df: pd.DataFrame) -&amp;gt; pd.DataFrame:\n    \"\"\"\n    Processes a DataFrame further by calculating ranks and log transformations for expression and binding data to elucidate certain trends.\n\n    :param df: The input DataFrame containing 'effect_perturbation' and 'pvalue_binding' columns.\n    :type df: pd.DataFrame\n\n    :returns: A DataFrame that includes the original data along with new columns for expression ranks, log-transformed ranks, binding ranks, and is sorted by the negative log-transformed binding rank.\n    :rtype: pd.DataFrame\n    \"\"\"\n    # Calculate expression rank with average ties method\n    df['expression_rank'] = rankdata(-abs(df['effect_perturbation']), method='average')\n\n    # Log transform the expression rank\n    df['neg_expression_rank_log'] = -np.log10(df['expression_rank'])\n\n    # Calculate binding rank with average ties method\n    df['binding_rank'] = rankdata(-df['effect_binding'], method='average')\n\n    # Calculate log transform of the binding rank\n    df['neg_log_rank_binding'] = -np.log10(rankdata(df['binding_rank'], method='average'))\n\n    # Select specific columns\n    plotting_df = df[['effect_perturbation', 'expression_rank', 'neg_expression_rank_log', \n                      'pvalue_binding', 'binding_rank', 'neg_log_rank_binding']]\n\n    # Arrange (sort) by neg_log_rank_binding in descending order\n    plotting_df = plotting_df.sort_values(by='neg_log_rank_binding', ascending=False)\n\n    return plotting_df\n</code></pre> <pre><code>#Using this on CallingCards Data\ntfs = ['WTM1', 'MIG2', 'CAT8', 'PDR1', 'PHO4', 'RIM101', 'GZF3', 'VHR1', 'ASH1', 'GAT3','FHL1', 'TEC1', 'SIP3', 'SKN7', 'WTM2','PHO2', 'HAA1', 'ADR1', 'MET31', 'CRZ1', 'RPH1', 'CHA4', 'CAD1', 'ZAP1', 'SKO1', 'ACA1', 'FZF1', 'HAP2', 'HAP3', 'HAP5','INO4', 'ERT1', 'TOG1', 'MET4', 'PPR1', 'RTG1', 'GLN3', 'MOT3', 'AFT1', 'GIS1', 'CBF1', 'SUM1', 'MSN2', 'DAL80', 'UPC2','RTG3', 'GAL80', 'RSF2', 'RME1', 'HIR2', 'SIP4', 'GCR1', 'HAP4', 'UME1', 'MET32', 'USV1', 'MGA1', 'CIN5', 'ROX1','XBP1', 'ZNF1', 'YHP1', 'RDR1', 'PDR3', 'RLM1', 'SFL1', 'SMP1', 'SUT2', 'HAC1', 'PHD1', 'ARO80']\n#Assigning the correct boolean to each TF based on whether the TF contains aggregated data in the database or not\nboolean_list = [True] * 59 + [False] * 12\n\ncompare_tf_rankings(tfs, boolean_list, \"CallingCards\")\n</code></pre> <pre>\n<code>/var/folders/25/6s7q5c9j40373whzd3q5s20m0000gn/T/ipykernel_6092/2867473121.py:94: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n  plt.boxplot([all_ranks_effect, all_ranks_poisson, all_ranks_hypergeometric], labels=['Effect', 'Poisson p-value', 'Hypergeometric p-value'])\n</code>\n</pre> <p>This is quite interesting. Immediately, your attention might be drawn to the leftmost boxplot for the enrichment values. Since the y-axis is plotting the smallest rank across each TF, it appears that for the effect boxplot, almost the entire TF dataset produces a rank of 1 as the smallest rank. This means that there is a clear difference in enrichment values that reuslts in a higher rank as opposed to the other two pvalue methods, whose medians and lower quartiles hover closer to 10 than 0, suggesting greater ties at the top resulting in larger highest ranks. Seeing that using enrichment values to rank produces higher ranks in general, it is worth plotting the original data but using the negative log rank of the enrichment on the x-axis to see how the boxplots will differ. We do that now.</p> <p>We can use our methods introduced above to compare the boxplots for the mcisaac data ranked using enrichment scores vs. perturbation scores.</p> <pre><code>tfs = ['WTM1','MIG2','CAT8','PDR1','PHO4','RIM101','GZF3','ASH1','GAT3','TEC1','SIP3','SKN7','WTM2','PHO2','HAA1','ADR1','MET31','CRZ1','RPH1','CHA4','CAD1','ZAP1','SKO1','ACA1','FZF1','HAP2','HAP3','HAP5','INO4','ERT1','TOG1','PPR1','RTG1','GLN3','MOT3','AFT1','CBF1','SUM1','MSN2','DAL80','UPC2','RTG3','GAL80','RSF2','RME1','HIR2','SIP4','HAP4','UME1','MET32','USV1','MGA1','CIN5','ROX1','XBP1','ZNF1','YHP1','RDR1','PDR3','RLM1','SFL1','SMP1','SUT2','HAC1','PHD1','ARO80']\nboolean_list = [True]*54 + [False]*12\ncc_mcisaac_adjacent_enrichment_data = adjacent_differences_store_data(tfs, boolean_list, \"cc\", perturbation_sources = [\"mcisaac\"], bins = 5)\n</code></pre> <pre><code>stored_data_list = [cc_mcisaac_adjacent_enrichment_data, cc_mcisaac_adjacent_data]\nlabels = ['enrichment', 'pvalues']\ncompare_adjacent_stored_data_box_plots(stored_data_list, labels)\n</code></pre> <p>This comparison is interesting for several reasons. First, the boxplots generated by ranking the data using enrichment values are considerably more spread out, suggesting greater variability using this method, While the medians for the first three adjacent bin differences using ranking by enrichment are slightly higher than ranking by poisson pvalues, the comparison of the rightbox boxplots shows that ranking by poisson pvalues is more optimal as the boxplot spread is mainly positive whereas the boxplot generating by ranking using enrichment has an overall neutral spread. Let us also look at how the first and last bin mean difference boxplots compare against one another.</p> <pre><code>cc_mcisaac_first_last_enrichment_data = first_last_bin_difference_box_plot_comparisons(tfs, boolean_list, \"cc\", perturbation_sources = [\"mcisaac\"], bins = 5)\n</code></pre> <pre><code>stored_data_list = [cc_mcisaac_first_last_enrichment_data, cc_mcisaac_first_last_data]\nlabels = [']enrichment', 'pvalues']\ncompare_first_and_last_stored_data_box_plots(stored_data_list, labels)\n</code></pre> <p>By ranking with poisson pvalues, we obtain a boxplot with both a smaller spread and a greater distribution of data that is above 0. Again, since the data in the lower quartile to the median is roughly 0 when ranking by enrichment, this is less optimal when compared to the boxplot ranking by poisson pvalues as more of the data generally exhibits a positive trend. Thus, we conclude continuing to plot by the poisson p values is likely the most optimal way to produce an observable trend for the eventual model to learn from.</p> <p>We have shown above multiple comparisons between various binding and peturbation sources, as well as methods for ranking data. Now, we want to compile a comprehensive comparison of the first and last bin mean difference boxplots across the various binding and perturbation sources. We will use our regular method of ranking the perturbation data, and continue using the poisson pvalues when ranking the binding data. Having these boxplots together will give us the best visual comparison of how the combinations of data hold up against one another.</p> <p>Lastly, we are interested in understanding how certain variables in the data are related to one another. Specifically, we are interested in 3 predictor variables and 1 outcome variable: </p> <p>1) gene_symbol (predictor): the gene at which a particular TF binds to 2) TF_symbol (predictor): the transcription factor itself 3) LRB (predictor): the negative log rank of the poisson pvalues for binding 4) LRR (outcome): the negative log rank of the perturbation effect magnitudes </p> <p>In order to understand how these predictors relate to the LRR, we need to build linear models that utilize combinations of these predictors to better understand how they correlate with the outcome. In order to do this, we will aggregate the data across the 78 TFs we have previously worked with to create a large dataframe that sorts the data first by the TF_symbol, then by the gene_symbol. The LRR ranking will be performed by first assigning a rank across all of the perturbation effect magnitudes in the entire dataframe, and then taking the negative log. Since we are assigning ranks across the entire aggregated dataframe, we call this a \u201cglobal\u201d ranking. For the LRB, we will rank the data in two ways. The first way will be performed similar to the LRR in which a global ranking across all of the binding poisson pvalues will be performed. However, we will also perform a separate ranking of the pvalues in which we will only assign ranks within a particular TF_symbol value, meaning that for a particular TF, we will rank the binding pvalues in the same way we have perviously, and then aggregate all of these LRB values. We call this method of ranking according to segmentations in the TF_symbol data a \u201clocal\u201d ranking. Thus, we will obtain two separate dataframes. In both, the LRR will be ranked globally, while the LRB will be ranked globally for one and locally for the other. For this example, we will use data for the 78 TFs derived from the CC+mitra binding dataset and the mcisaac perturbation dataset.</p> <p>Once we have obtained the data, we want to create the following linear models:</p> <p>1) Single variable models to predict the LRR 2) A joint variable model to predict the LRR 3) Models in which the effects of gene_symbol, TF_symbol, or both are removed to determine how much of the remaining variance explained is acccounted for by the LRB</p> <p>We will obtain the correlation coefficients of these models to better determine the strength of each predictor in assessing the outcome.</p> <pre><code>async def process_transcription_factor_async(tf_name: str, is_aggregated: bool, binding_source: str, perturbation_source: str, pseudocount: int = 1) -&amp;gt; pd.DataFrame:    \n    \"\"\"\n    Process transcription factor data by retrieving and merging binding and perturbation datasets.\n\n    :param tf_name: The name of the transcription factor, e.g., \"AR080\".\n    :type tf_name: str\n    :param is_aggregated: Indicates whether the data is aggregated.\n    :type is_aggregated: bool\n    :param binding_source: The source of the binding data, e.g., \"cc\" or \"harbison\".\n    :type binding_source: str\n    :param perturbation_source: The source of the perturbation data, e.g., \"mcisaac\".\n    :type perturbation_source: str\n    :param pseudocount: The constant used in calculating enrichment and p-values scores to avoid division by zero, default is 1.\n    :type pseudocount: int, optional\n\n    :returns: A DataFrame containing the combined and processed binding and perturbation data.\n    :rtype: pd.DataFrame\n    \"\"\"\n    # Ensure the TF name is in uppercase to maintain consistency\n    tf_name_upper = tf_name.upper()\n\n    # Initialize API for binding data\n    pss_api_tf = PromoterSetSigAPI()\n\n    # Access the relevant data depending on the binding source and aggregation status\n    if binding_source == \"cc\":\n        if is_aggregated:\n            pss_api_tf.push_params({'regulator_symbol': tf_name_upper, \"datasource\": \"brent_nf_cc\", \"aggregated\": \"true\"})\n        else:\n            pss_api_tf.push_params({'regulator_symbol': tf_name_upper, \"workflow\": \"nf_core_callingcards_1_0_0\", \"data_usable\": \"pass\"})\n    elif binding_source == \"harbison\":\n        pss_api_tf.push_params({'regulator_symbol': tf_name_upper, \"source\": \"4\"})\n    elif binding_source == \"mitra\":\n        pss_api_tf.push_params({'regulator_symbol': tf_name_upper, \"source\": \"2\"})\n\n    # Asynchronously read the binding data from the API\n    tf_pss = await pss_api_tf.read(retrieve_files=True)\n    # Get the ID of the retrieved binding data\n    id = tf_pss.get(\"metadata\")[\"id\"][0]\n    # Extract the binding data using the ID\n    binding_df = tf_pss.get(\"data\").get(str(id))\n\n    # Initialize API for perturbation data\n    expression = ExpressionAPI()\n\n    # Map perturbation source to corresponding source number\n    source_mapping = {\n        \"mcisaac\": \"7\",\n        \"hu_reimann\": \"5\",\n        \"kemmeren\": \"6\"\n    }\n    source_number = source_mapping.get(perturbation_source, \"unknown\")\n\n    # Push parameters to retrieve the perturbation data\n    if perturbation_source == \"mcisaac\":\n        expression.push_params({\"regulator_symbol\": tf_name_upper, \"source\": source_number, \"time\": \"15\"})\n    else:\n        expression.push_params({\"regulator_symbol\": tf_name_upper, \"source\": source_number})\n\n    # Asynchronously read the perturbation data from the API\n    expression_res = await expression.read(retrieve_files=True)\n    # Get the ID of the retrieved perturbation data\n    id = expression_res.get(\"metadata\")[\"id\"][0]\n    # Extract the perturbation data using the ID\n    expression_df = expression_res.get(\"data\").get(str(id))\n\n    # Read perturbation data\n    perturbation_data = expression_df\n    # Read binding data\n    binding_data = binding_df\n\n    # Rename columns in binding data for consistency and clarity\n    if binding_source == \"cc\":\n        binding_data.rename(columns={\"callingcards_enrichment\": \"effect\", \"poisson_pval\": \"pvalue\"}, inplace=True)\n    elif binding_source == \"harbison\":\n        binding_data.rename(columns={\"pval\": \"pvalue\"}, inplace=True)\n    elif binding_source == \"mitra\":\n        binding_data.rename(columns={\"callingcards_enrichment\": \"effect\", \"poisson_pval\": \"pvalue\"}, inplace=True)\n\n    # Optional: here you can modify the pseudocount as needed. The default pseudocount is set to 1.\n    # Calculate the effect size for binding data using the provided formula\n    if binding_source == \"cc\":\n        binding_data['effect'] = (binding_data['experiment_hops'] / binding_data['experiment_total_hops']) / \\\n                             ((binding_data['background_hops'] + pseudocount) / binding_data['background_total_hops'])\n\n    # Merge the binding data and perturbation data on the 'target_locus_tag' column\n    combined_data = pd.merge(binding_data, perturbation_data, on='target_locus_tag', suffixes=('_binding', '_perturbation'))\n\n    # # Assert that the length of combined_data is the minimum of the lengths of binding_data and perturbation_data\n    # assert len(combined_data) &amp;lt;= min(len(binding_data), len(perturbation_data)), \\\n    #     f\"Length of combined_data ({len(combined_data)}) is not equal to the minimum of lengths of binding_data ({len(binding_data)}) and perturbation_data ({len(perturbation_data)})\"\n\n    # Keep only the necessary columns in the combined data\n    # combined_data = combined_data[['target_locus_tag', 'effect_binding', 'effect_perturbation', 'pvalue_binding']]\n\n    # Reorder the combined data by the smallest 'pvalue_binding' values\n    combined_data = combined_data.sort_values(by='pvalue_binding')\n\n    # Apply transformations:\n    # - Take the absolute value of 'effect_perturbation'\n    # - Calculate the negative log10 of 'pvalue_binding'\n    # - Calculate the log10 of 'effect_binding'\n    combined_data['effect_perturbation'] = combined_data['effect_perturbation'].abs()\n    combined_data['neg_log_pvalue_binding'] = -np.log10(combined_data['pvalue_binding'])\n    combined_data['log_enrichment'] = np.log10(combined_data['effect_binding'])\n\n    # Return the processed combined data as a DataFrame\n    return combined_data\n\ndef aggregate_tf_data(tfs: List[str], boolean_list: List[bool], binding_source: List[str], perturbation_source:str) -&amp;gt; pd.DataFrame:\n    \"\"\"\n    Aggregates data for a list of transcription factors by calling the\n    process_transcription_factor method and combining the resulting DataFrames.\n\n    :param tfs: A list of transcription factors.\n    :type tfs: List[str]\n\n    :returns: A DataFrame containing the aggregated data.\n    :rtype: pd.DataFrame\n    \"\"\"\n    aggregated_data = pd.DataFrame()  # Initialize an empty DataFrame\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n\n        for i in range(len(tfs)):\n            #print(\"current tf:\" + str(tfs[i]))\n            tf_data = process_transcription_factor(tfs[i],boolean_list[i], binding_source[i], perturbation_source)  # Process each TF to get its DataFrame\n            aggregated_data = pd.concat([aggregated_data, tf_data], ignore_index=True)  # Aggregate the DataFrame\n\n    return aggregated_data\n\ndef resort_and_rank_dataframe(df: pd.DataFrame, method: str) -&amp;gt; pd.DataFrame:\n    \"\"\"\n    Resorts the dataframe by 'target_symbol' and 'regulator_symbol', then calculates\n    the ranks for 'effect_binding' and 'effect_perturbation' columns using a global ranking, and filters the\n    dataframe to include only the specified columns.\n\n    :param df: The raw dataframe to be processed.\n    :type df: pd.DataFrame\n\n    :returns: The processed dataframe with sorted and ranked columns.\n    :rtype: pd.DataFrame\n    \"\"\"\n    # Sort by 'target_symbol' and then by 'regulator_symbol'\n    df = df.sort_values(by=['regulator_symbol_perturbation','target_symbol_perturbation'])\n\n    df['expression_rank'] = rankdata(-abs(df['effect_perturbation']), method='average')\n\n    # Log transform the expression rank\n    df['LRR'] = -np.log10(df['expression_rank'])\n\n    if method == \"global\":\n        # Calculate binding rank globally\n        df['binding_rank'] = rankdata(-abs(df['pvalue_binding']), method='average')\n\n    elif method == \"local\":\n        # Calculate binding rank locally within each TF\n        df['binding_rank'] = df.groupby('regulator_symbol_perturbation')['pvalue_binding'].transform(lambda x: rankdata(-abs(x), method='average'))\n    # Calculate log transform of the binding rank\n    df['LRB'] = -np.log10(df['binding_rank'])\n\n    df.rename(columns={\n    'target_symbol_perturbation': 'gene_symbol',\n    'regulator_symbol_perturbation': 'TF_symbol',\n    }, inplace=True)\n\n    # Filter the dataframe to include only the specified columns\n    df_filtered = df[['TF_symbol', 'gene_symbol','LRB', 'LRR']]\n\n    return df_filtered\n</code></pre> <pre><code>raw_combined_cc_mcisaac_data = aggregate_tf_data(all_tfs, boolean_list, cc_to_mitra_ratio_in_all, \"mcisaac\")\n#tfs = ['WTM1','MIG2','CAT8','PDR1','PHO4','RIM101','GZF3','ASH1','GAT3','TEC1','SIP3','SKN7','WTM2','PHO2','HAA1','ADR1','MET31','CRZ1','RPH1','CHA4','CAD1','ZAP1','SKO1','ACA1','FZF1','HAP2','HAP3','HAP5','INO4','ERT1','TOG1','PPR1','RTG1','GLN3','MOT3','AFT1','CBF1','SUM1','MSN2','DAL80','UPC2','RTG3','GAL80','RSF2','RME1','HIR2','SIP4','HAP4','UME1','MET32','USV1','MGA1','CIN5','ROX1','XBP1','ZNF1','YHP1','RDR1','PDR3','RLM1','SFL1','SMP1','SUT2','HAC1','PHD1','ARO80']\n\nfiltered_cc_msisaac_data_global = resort_and_rank_dataframe(raw_combined_cc_mcisaac_data)\nfiltered_cc_msisaac_data_local = resort_and_rank_dataframe(raw_combined_cc_mcisaac_data)\n\nfiltered_cc_msisaac_data_global.head()\n</code></pre> <pre><code>raw_combined_cc_mcisaac_data = aggregate_tf_data([\"MIG2\"], boolean_list, cc_to_mitra_ratio_in_all, \"mcisaac\")\n\nfiltered_cc_msisaac_data_local_MIG2 = resort_and_rank_dataframe(raw_combined_cc_mcisaac_data, \"local\")\n\nfiltered_cc_msisaac_data_local_MIG2.head()\n</code></pre> TF_symbol gene_symbol LRB LRR 1325 MIG2 AAC1 -3.683362 -2.193125 1131 MIG2 AAC3 -3.700141 -3.502291 3546 MIG2 AAD10 -3.398808 -3.502291 2369 MIG2 AAD14 -3.574263 -3.502291 5240 MIG2 AAD15 -2.982271 -3.502291 <pre><code>\n\ndef run_linear_model(formula, data):\n    print(f\"Creating the model for formula: {formula}\")\n    y, X = patsy.dmatrices(formula, data=data)\n    model = sm.OLS(y, X)\n    fit_model = model.fit()\n\n    # Print the summary of the model\n    print(f\"Summary for model {formula}:\\n{fit_model.summary()}\\n\")\n\n    return fit_model\n\n\ndef calculate_residuals(formula, data):\n    print(f\"Calculating residuals for formula: {formula}\")\n    y, X = patsy.dmatrices(formula, data=data)\n    model = sm.OLS(y, X)\n    fit_model = model.fit()\n    residuals = fit_model.resid\n    return residuals\n\n\ndef run_residual_model(residuals, independent_var, data, input_filename):\n    print(f\"Running model with residuals against {independent_var}\")\n    data = data.assign(residuals=residuals)  # Add residuals to the data frame\n    formula = f\"residuals ~ {independent_var}\"\n    fit_model = run_linear_model(formula, data)\n    plot_diagnostics(\n        fit_model, f\"residuals_vs_{independent_var}_residual\", input_filename\n    )\n\n\ndef plot_diagnostics(fit_model, title_suffix, input_filename):\n    # Extract the base name of the input file without extension\n    input_basename = os.path.splitext(os.path.basename(input_filename))[0]\n\n    # Extract fitted values and residuals\n    fitted_values = fit_model.fittedvalues\n    residuals = fit_model.resid\n    standardized_residuals = residuals / np.std(residuals)\n\n    # Plot 1: Residuals vs. Fitted\n    plt.figure(figsize=(15, 10))\n    plt.scatter(fitted_values, residuals, alpha=0.5)\n    plt.axhline(0, color=\"gray\", linestyle=\"--\")\n    plt.xlabel(\"Fitted values\")\n    plt.ylabel(\"Residuals\")\n    plt.title(\"Residuals vs. Fitted \" + title_suffix)\n    plt.show()\n\nformulas = [\"LRR ~ TF_symbol\", \"LRR ~ LRB\"]\nfor formula in formulas:\n    fit_model = run_linear_model(formula, filtered_cc_msisaac_data_local_MIG2)\n    plot_diagnostics(fit_model, f\"({formula})_single\", \"filtered_cc_msisaac_data_local_MIG2\")\n</code></pre> <pre>\n<code>Creating the model for formula: LRR ~ TF_symbol\nSummary for model LRR ~ TF_symbol:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    LRR   R-squared:                      -0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                       nan\nDate:                Tue, 13 Aug 2024   Prob (F-statistic):                nan\nTime:                        13:13:01   Log-Likelihood:                -1328.4\nNo. Observations:                6151   AIC:                             2659.\nDf Residuals:                    6150   BIC:                             2666.\nDf Model:                           0                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -3.4482      0.004   -900.481      0.000      -3.456      -3.441\n==============================================================================\nOmnibus:                     6737.108   Durbin-Watson:                   1.881\nProb(Omnibus):                  0.000   Jarque-Bera (JB):           358938.603\nSkew:                           5.837   Prob(JB):                         0.00\nKurtosis:                      38.556   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n</code>\n</pre> <pre>\n<code>Creating the model for formula: LRR ~ LRB\nSummary for model LRR ~ LRB:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    LRR   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.6444\nDate:                Tue, 13 Aug 2024   Prob (F-statistic):              0.422\nTime:                        13:13:02   Log-Likelihood:                -1328.1\nNo. Observations:                6151   AIC:                             2660.\nDf Residuals:                    6149   BIC:                             2674.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -3.4720      0.030   -115.919      0.000      -3.531      -3.413\nLRB           -0.0071      0.009     -0.803      0.422      -0.024       0.010\n==============================================================================\nOmnibus:                     6735.852   Durbin-Watson:                   1.881\nProb(Omnibus):                  0.000   Jarque-Bera (JB):           358578.542\nSkew:                           5.836   Prob(JB):                         0.00\nKurtosis:                      38.537   Cond. No.                         28.7\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n</code>\n</pre> <pre><code>\n</code></pre> <p>This is an example of how the dataframe looks based on the description above. The data is organized first by the TF_symbol, then by the gene_symbol. For this dataframe, we performed a global ranking to obtain the LRB values.</p> <pre><code>#save the data locally\nfiltered_cc_msisaac_data_global.to_csv(\"~/Downloads/filtered_cc_msisaac_data_global.csv\", index = False)\nfiltered_cc_msisaac_data_local.to_csv(\"~/Downloads/filtered_cc_msisaac_data_local.csv\", index = False)\n</code></pre> <pre><code>#code for the single variable models in the first objective\nimport argparse\nimport statsmodels.api as sm\nimport pandas as pd\nimport patsy\n\n\ndef run_linear_model(formula, data):\n    print(f\"Creating the model for formula: {formula}\")\n    y, X = patsy.dmatrices(formula, data=data)\n\n    print(\"Fitting the model...\")\n    model = sm.OLS(y, X)\n    fit_model = model.fit()\n\n    # Print the formula used\n    print(f\"Running model with formula: {formula}\")\n\n    # Return the summary\n    return fit_model.summary()\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=\"Run linear regression models from a CSV file.\"\n    )\n    parser.add_argument(\"--input\", help=\"Path to the input CSV file.\", required=True)\n\n    args = parser.parse_args()\n\n    # Load the data from the CSV file\n    data = pd.read_csv(args.input)\n    print(f\"Data loaded from {args.input}\")\n\n    # Define formulas for the different models\n    formulas = [\"LRR ~ gene_symbol\", \"LRR ~ TF_symbol\", \"LRR ~ LRB\"]\n\n    # Run and print summary for each model\n    for formula in formulas:\n        print(f\"Running model: {formula}\")\n        summary = run_linear_model(formula, data)\n        print(f\"Summary:\\n{summary}\\n\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code>#code for the joint model in the second objective\nimport argparse\nimport statsmodels.api as sm\nimport pandas as pd\nimport patsy\n\n\ndef run_joint_linear_model(formula, data):\n    print(f\"Creating the joint model for formula: {formula}\")\n    y, X = patsy.dmatrices(formula, data=data)\n\n    print(\"Fitting the joint model...\")\n    model = sm.OLS(y, X)\n    fit_model = model.fit()\n\n    # Print the formula used\n    print(f\"Running joint model with formula: {formula}\")\n\n    # Return the summary\n    return fit_model.summary()\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=\"Run a joint linear regression model from a CSV file.\"\n    )\n    parser.add_argument(\"--input\", help=\"Path to the input CSV file.\", required=True)\n\n    args = parser.parse_args()\n\n    # Load the data from the CSV file\n    data = pd.read_csv(args.input)\n    print(f\"Data loaded from {args.input}\")\n\n    # Define the formula for the joint model\n    joint_formula = \"LRR ~ gene_symbol + TF_symbol + LRB\"\n\n    # Run and print summary for the joint model\n    print(f\"Running joint model: {joint_formula}\")\n    summary = run_joint_linear_model(joint_formula, data)\n    print(f\"Summary of the joint model:\\n{summary}\\n\")\n\n\nif __name__ == \"__main__\":\n    main()\n\n</code></pre> <pre><code>#code for the linear models in the third objective\nimport argparse\nimport statsmodels.api as sm\nimport pandas as pd\nimport patsy\n\n\ndef calculate_residuals(formula, data):\n    print(f\"Fitting model to calculate residuals for formula: {formula}\")\n    y, X = patsy.dmatrices(formula, data=data)\n    model = sm.OLS(y, X).fit()\n    residuals = model.resid\n    return residuals\n\n\ndef run_residual_model(residuals, independent_var, data):\n    # Incorporating the residuals into the data used for regression\n    data[\"residuals\"] = residuals\n    formula = f\"residuals ~ {independent_var}\"\n    print(f\"Creating model for residuals with formula: {formula}\")\n    y, X = patsy.dmatrices(formula, data=data)\n    model = sm.OLS(y, X).fit()\n    return model.summary()\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=\"Run regression models on residuals from a CSV file.\"\n    )\n    parser.add_argument(\"--input\", help=\"Path to the input CSV file.\", required=True)\n\n    args = parser.parse_args()\n\n    # Load the data from the CSV file\n    data = pd.read_csv(args.input)\n    print(f\"Data loaded from {args.input}\")\n\n    # Calculate residuals for different combinations\n    residuals_full = calculate_residuals(\"LRR ~ TF_symbol + gene_symbol\", data)\n    residuals_gene = calculate_residuals(\"LRR ~ gene_symbol\", data)\n    residuals_tf = calculate_residuals(\"LRR ~ TF_symbol\", data)\n\n    # Model residuals against LRB\n    print(\"Modeling residuals from LRR ~ TF_symbol + gene_symbol\")\n    summary_full = run_residual_model(residuals_full, \"LRB\", data)\n    print(summary_full)\n\n    print(\"Modeling residuals from LRR ~ gene_symbol\")\n    summary_gene = run_residual_model(residuals_gene, \"LRB\", data)\n    print(summary_gene)\n\n    print(\"Modeling residuals from LRR ~ tf_symbol\")\n    summary_tf = run_residual_model(residuals_tf, \"LRB\", data)\n    print(summary_tf)\n\n\nif __name__ == \"__main__\":\n    main()\n\n</code></pre> <p>Here is an example of how to run the scripts after everything has been initialized into the cluster and ready to go.</p> <p>interactive</p> <p>eval $(spack load \u2013sh singularityce)</p> <p>sbatch \u2013mem=100G ../scripts/singularity_exec.sh statsmodel.sif \u201cpython3 single_variable_models.py \u2013input data/filtered_cc_mcisaac_data_global.csv\u201d - this has job id 17145975</p> <p>sbatch \u2013mem=100G ../scripts/singularity_exec.sh statsmodel.sif \u201cpython3 single_variable_models.py \u2013input data/filtered_cc_mcisaac_data_local.csv\u201d - this has job id 17145977</p> <p>sbatch \u2013mem=100G ../scripts/singularity_exec.sh statsmodel.sif \u201cpython3 joint_model.py \u2013input data/filtered_cc_mcisaac_data_global.csv\u201d - this has job id 17145979</p> <p>sbatch \u2013mem=100G ../scripts/singularity_exec.sh statsmodel.sif \u201cpython3 joint_model.py \u2013input data/filtered_cc_mcisaac_data_local.csv\u201d - this has job id 17145980</p> <p>sbatch \u2013mem=100G ../scripts/singularity_exec.sh statsmodel.sif \u201cpython3 residual_models.py \u2013input data/filtered_cc_mcisaac_data_global.csv\u201d - 17146057</p> <p>sbatch \u2013mem=100G ../scripts/singularity_exec.sh statsmodel.sif \u201cpython3 residual_models.py \u2013input data/filtered_cc_mcisaac_data_local.csv\u201d - 17146058</p> <p>sbatch \u2013mem=100G ../scripts/singularity_exec.sh statsmodel.sif \u201cpython3 single_variable_models.py \u2013input data/filtered_cc_msisaac_data_LRR_local_LRB_local.csv\u201d - 17151563</p> <p>sbatch \u2013mem=100G ../scripts/singularity_exec.sh statsmodel.sif \u201cpython3 joint_model.py \u2013input data/filtered_cc_msisaac_data_LRR_local_LRB_local.csv\u201d  - 17151564</p> <p>sbatch \u2013mem=100G ../scripts/singularity_exec.sh statsmodel.sif \u201cpython3 residual_models.py \u2013input data/filtered_cc_msisaac_data_LRR_local_LRB_local.csv\u201d  - 17151565</p> <p>sbatch \u2013mem=100G ../scripts/singularity_exec.sh statsmodel.sif \u201cpython3 single_variable_models.py \u2013input data/filtered_cc_msisaac_data_LRR_local_LRB_global.csv\u201d - 17151566</p> <p>sbatch \u2013mem=100G ../scripts/singularity_exec.sh statsmodel.sif \u201cpython3 joint_model.py \u2013input data/filtered_cc_msisaac_data_LRR_local_LRB_global.csv\u201d - 17151567</p> <p>sbatch \u2013mem=100G ../scripts/singularity_exec.sh statsmodel.sif \u201cpython3 residual_models.py \u2013input data/filtered_cc_msisaac_data_LRR_local_LRB_global.csv\u201d - 17151568</p> <p>LRR Global LRB Global sbatch \u2013mem=400G ../scripts/singularity_exec.sh statsmodel_plus.sif \u201cpython3 diagnostic_plots_new.py \u2013input data/filtered_cc_mcisaac_data_LRR_global_LRB_global.csv \u2013mode \u2018single\u2019\u201d - 17162198</p> <p>sbatch \u2013mem=400G ../scripts/singularity_exec.sh statsmodel_plus.sif \u201cpython3 diagnostic_plots_new.py \u2013input data/filtered_cc_mcisaac_data_LRR_global_LRB_global.csv \u2013mode \u2018joint\u2019\u201d - 17162199</p> <p>sbatch \u2013mem=400G ../scripts/singularity_exec.sh statsmodel_plus.sif \u201cpython3 diagnostic_plots_new.py \u2013input data/filtered_cc_mcisaac_data_LRR_global_LRB_global.csv \u2013mode \u2018residual\u2019\u201d - 17162200</p> <p>LRR Local LRB Global sbatch \u2013mem=400G ../scripts/singularity_exec.sh statsmodel_plus.sif \u201cpython3 diagnostic_plots_new.py \u2013input data/filtered_cc_msisaac_data_LRR_local_LRB_global.csv \u2013mode \u2018single\u2019\u201d - 17162204</p> <p>sbatch \u2013mem=400G ../scripts/singularity_exec.sh statsmodel_plus.sif \u201cpython3 diagnostic_plots_new.py \u2013input data/filtered_cc_msisaac_data_LRR_local_LRB_global.csv \u2013mode \u2018joint\u2019\u201d - 17162205</p> <p>sbatch \u2013mem=400G ../scripts/singularity_exec.sh statsmodel_plus.sif \u201cpython3 diagnostic_plots_new.py \u2013input data/filtered_cc_msisaac_data_LRR_local_LRB_global.csv \u2013mode \u2018residual\u2019\u201d - 17162206</p> <p>LRR Global LRB Local</p> <p>sbatch \u2013mem=400G ../scripts/singularity_exec.sh statsmodel_plus.sif \u201cpython3 diagnostic_plots_new.py \u2013input data/filtered_cc_mcisaac_data_LRR_global_LRB_local.csv \u2013mode \u2018single\u2019\u201d  - 17162207</p> <p>sbatch \u2013mem=400G ../scripts/singularity_exec.sh statsmodel_plus.sif \u201cpython3 diagnostic_plots_new.py \u2013input data/filtered_cc_mcisaac_data_LRR_global_LRB_local.csv \u2013mode \u2018joint\u2019\u201d - 17162208</p> <p>sbatch \u2013mem=400G ../scripts/singularity_exec.sh statsmodel_plus.sif \u201cpython3 diagnostic_plots_new.py \u2013input data/filtered_cc_mcisaac_data_LRR_global_LRB_local.csv \u2013mode \u2018residual\u2019\u201d - 17162209</p> <p>LRR Local LRB Local</p> <p>sbatch \u2013mem=400G ../scripts/singularity_exec.sh statsmodel_plus.sif \u201cpython3 diagnostic_plots_new.py \u2013input data/filtered_cc_msisaac_data_LRR_local_LRB_local.csv \u2013mode \u2018single\u2019\u201d - 17162215</p> <p>sbatch \u2013mem=400G ../scripts/singularity_exec.sh statsmodel_plus.sif \u201cpython3 diagnostic_plots_new.py \u2013input data/filtered_cc_msisaac_data_LRR_local_LRB_local.csv \u2013mode \u2018joint\u2019\u201d - 17162216</p> <p>sbatch \u2013mem=400G ../scripts/singularity_exec.sh statsmodel_plus.sif \u201cpython3 diagnostic_plots_new.py \u2013input data/filtered_cc_msisaac_data_LRR_local_LRB_local.csv \u2013mode \u2018residual\u2019\u201d - 17162217</p> <p>You should save the batch job number or reference the progress of your scripts using the following commands:</p> <p>check the status of how long your job has been running for: squeue -u $USER</p> <p>check the output of your job during/after it has finished running: cat slurm-{BATCH_JOB_NUMBER}.out</p> <p>LRR Global LRB Global</p> Model Formula R^2 LRR ~ gene_symbol 0.040 LRR ~ TF_symbol 0.100 LRR ~ LRB 0.002 LRR ~ gene_symbol + TF_symbol + LRB 0.142 resid(LRR~gene_symbol) ~ LRB 0.002 resid(LRR~TF_symbol) ~ LRB 0.001 resid(LRR~gene_symbol + TF_symbol) ~ LRB 0.001 <p>LRR Global LRB Local</p> Model Formula R^2 LRR ~ gene_symbol 0.040 LRR ~ TF_symbol 0.100 LRR ~ LRB 0.001 LRR ~ gene_symbol + TF_symbol + LRB 0.142 resid(LRR~gene_symbol) ~ LRB 0.001 resid(LRR~TF_symbol) ~ LRB 0.001 resid(LRR~gene_symbol + TF_symbol) ~ LRB 0.001 <p>LLR Local LRB Local</p> Model Formula R^2 LRR ~ gene_symbol 0.048 LRR ~ TF_symbol 0.010 LRR ~ LRB 0.001 LRR ~ gene_symbol + TF_symbol + LRB 0.059 resid(LRR~gene_symbol) ~ LRB 0.001 resid(LRR~TF_symbol) ~ LRB 0.001 resid(LRR~gene_symbol + TF_symbol) ~ LRB 0.001 <p>LRR Local LRB Global</p> Model Formula R^2 LRR ~ gene_symbol 0.048 LRR ~ TF_symbol 0.010 LRR ~ LRB 0.002 LRR ~ gene_symbol + TF_symbol + LRB 0.060 resid(LRR~gene_symbol) ~ LRB 0.001 resid(LRR~TF_symbol) ~ LRB 0.001 resid(LRR~gene_symbol + TF_symbol) ~ LRB 0.001 <p>When comparing both tables, there are only minor differences in the values for some of the linear models. This means that changing the way that the LRB is ranked (i.e. locally or globally) is ultimately rather insignificant because the LRB itself is a poor predictor of the LRR as evidenced by the low correlation coefficients in the single variable model. This also means that even after removing the effects of the other two variables, the LRB is still does not do well in accounting for the remaining explained variance based on the low correlations obtained in the last 3 rows of data. On the other hand, the TF_symbol seems to be the best predictor of the outcome as it acheives the highest correlation, which the gene_symbol has a correlation that is below half that of the TF_symbol. Interestingly, this may imply that the identify of the transcription factor itself may lend a better hand at explaining the magnitude of perturbation effects when interacting with a particular gene, as opposed to the LRB or the identity of the gene interacting with the TF.  </p> <pre><code>\n</code></pre>"},{"location":"tutorials/exploring_perturbation_response_relationship/#visualizing-relationship-between-binding-and-perturbation-response","title":"Visualizing Relationship Between Binding and Perturbation Response","text":"<p>This tutorial walks you through the methods and approaches taken to process the data and visualize trends within the data in regard to the binding and perturbation effects of TFs on genes in preparation for model training.</p>"},{"location":"tutorials/exploring_perturbation_response_relationship/#accessing-combined-tf-data","title":"Accessing combined TF data","text":""},{"location":"tutorials/exploring_perturbation_response_relationship/#code","title":"Code","text":"<p>Given the length of the methods below, it is optimal to hide the cells after running them to ensure readability of the notebook. </p>"},{"location":"tutorials/exploring_perturbation_response_relationship/#application","title":"Application","text":""},{"location":"tutorials/exploring_perturbation_response_relationship/#applying-transformations-to-the-data","title":"Applying Transformations to the Data","text":""},{"location":"tutorials/exploring_perturbation_response_relationship/#code_1","title":"Code","text":"<p>Thus, we take the negative log ranks of these two columns to determine if there is a relationship across all TFs. Taking the negative log of the rank means that the highest ranked data will be the least negative. Since we assign higher ranks to smaller poisson pvalues and larger perturbation effects, we would hope to see a positive linear trend when graphing this data as more significant binding interactions should ideally correlate with larger perturbations. The process_dataframe method below does this to enable more meaningful analysis.</p>"},{"location":"tutorials/exploring_perturbation_response_relationship/#visualizing-the-transformed-data-using-bins","title":"Visualizing the Transformed Data Using Bins","text":""},{"location":"tutorials/exploring_perturbation_response_relationship/#code_2","title":"Code","text":""},{"location":"tutorials/exploring_perturbation_response_relationship/#application_1","title":"Application","text":""},{"location":"tutorials/exploring_perturbation_response_relationship/#code_3","title":"Code","text":""},{"location":"tutorials/exploring_perturbation_response_relationship/#application_2","title":"Application","text":""},{"location":"tutorials/exploring_perturbation_response_relationship/#further-visualization-techniques-on-the-data","title":"Further Visualization Techniques on the Data","text":""},{"location":"tutorials/exploring_perturbation_response_relationship/#boxplots-of-adjacent-binned-mean-differences","title":"Boxplots of Adjacent Binned Mean Differences","text":""},{"location":"tutorials/exploring_perturbation_response_relationship/#code_4","title":"Code","text":""},{"location":"tutorials/exploring_perturbation_response_relationship/#application_3","title":"Application","text":""},{"location":"tutorials/exploring_perturbation_response_relationship/#boxplots-of-binned-mean-differences-between-the-first-and-last-bins","title":"Boxplots of Binned Mean Differences Between the First and Last Bins","text":""},{"location":"tutorials/exploring_perturbation_response_relationship/#code_5","title":"Code","text":""},{"location":"tutorials/exploring_perturbation_response_relationship/#application_4","title":"Application","text":""},{"location":"tutorials/exploring_perturbation_response_relationship/#boxplots-of-pearson-correlations-between-lrr-and-lrb","title":"Boxplots of Pearson Correlations between LRR and LRB","text":""},{"location":"tutorials/exploring_perturbation_response_relationship/#code_6","title":"Code","text":""},{"location":"tutorials/exploring_perturbation_response_relationship/#application_5","title":"Application","text":""},{"location":"tutorials/exploring_perturbation_response_relationship/#further-approaches-to-ranking-the-binding-and-perturbation-data","title":"Further Approaches to Ranking the Binding and Perturbation Data","text":""},{"location":"tutorials/exploring_perturbation_response_relationship/#which-approaches-of-ranking-the-perturbation-data-yields-the-best-trends","title":"Which Approaches of Ranking the Perturbation Data Yields the Best Trends?","text":""},{"location":"tutorials/exploring_perturbation_response_relationship/#1-using-average-ranking-for-multiple-timepoints-in-the-mcisaac-perturbation-data","title":"1) Using Average Ranking For Multiple Timepoints in the mcisaac Perturbation Data","text":""},{"location":"tutorials/exploring_perturbation_response_relationship/#code_7","title":"Code","text":""},{"location":"tutorials/exploring_perturbation_response_relationship/#application_6","title":"Application","text":""},{"location":"tutorials/exploring_perturbation_response_relationship/#2-using-average-ranking-between-mcisaac-and-kemmeren-perturbation-data","title":"2) Using Average Ranking Between mcisaac and kemmeren Perturbation Data","text":""},{"location":"tutorials/exploring_perturbation_response_relationship/#boxplot-comparisons-of-the-ranking-approaches","title":"Boxplot Comparisons of the Ranking Approaches","text":""},{"location":"tutorials/exploring_perturbation_response_relationship/#which-approaches-of-ranking-the-binding-data-yields-the-best-trends","title":"Which Approaches of Ranking the Binding Data Yields the Best Trends?","text":""},{"location":"tutorials/exploring_perturbation_response_relationship/#code_8","title":"Code","text":""},{"location":"tutorials/exploring_perturbation_response_relationship/#application_7","title":"Application","text":""},{"location":"tutorials/exploring_perturbation_response_relationship/#creating-linear-models-from-the-data","title":"Creating Linear Models from the Data","text":""},{"location":"tutorials/exploring_perturbation_response_relationship/#obtaining-data-for-the-linear-models","title":"Obtaining Data for the Linear Models","text":"<p>We will define a couple of modified functions that will process the data from the desired binding/perturbation source for our set of 78 TFs. </p>"},{"location":"tutorials/exploring_perturbation_response_relationship/#code_9","title":"Code","text":""},{"location":"tutorials/exploring_perturbation_response_relationship/#application_8","title":"Application","text":""},{"location":"tutorials/exploring_perturbation_response_relationship/#commands-and-scripts-for-running-the-linear-models-in-htcf-cluster","title":"Commands and Scripts for Running the Linear Models in HTCF Cluster","text":"<p>Given the size of the data, running these models within this notebook isn\u2019t optimal. We used the HTCF cluster to run the scripts below. Note that running the scripts using the cluster requires the installation of singularity; you can find more information on how to use and install singularity using spack on the cluster here: https://htcf.wustl.edu/docs/software/  Below are the files used to run the three models.</p>"},{"location":"tutorials/exploring_perturbation_response_relationship/#code-for-files","title":"Code for Files","text":""},{"location":"tutorials/exploring_perturbation_response_relationship/#application_9","title":"Application","text":""},{"location":"tutorials/exploring_perturbation_response_relationship/#table-of-results-for-the-linear-models","title":"Table of Results for the Linear Models","text":"<p>The two tables below outline the results obtained after successful execution of the scripts based on whether the binding data was ranked globally or locally. The first 3 rows in each table represent the single variable models described in the first objective. The next row represents the joint model described in the second objective, and the last 3 rows are the models described in the third objective.</p>"},{"location":"tutorials/generate_in_silico_data/","title":"Generate In-silico Data","text":"<pre><code>from yeastdnnexplorer.probability_models.relation_classes import And, Or\nfrom yeastdnnexplorer.probability_models.generate_data import (generate_gene_population, \n                                                               generate_binding_effects,\n                                                               generate_pvalues,\n                                                               generate_perturbation_effects,\n                                                               perturbation_effect_adjustment_function_with_tf_relationships,\n                                                               perturbation_effect_adjustment_function_with_tf_relationships_boolean_logic)\n\nimport torch\nimport matplotlib.pyplot as plt\n\ntorch.manual_seed(42)  # For CPU\ntorch.cuda.manual_seed_all(42)  # For all CUDA devices\n\n</code></pre> <pre>\n<code>Matplotlib is building the font cache; this may take a moment.\n</code>\n</pre> <pre><code>n_genes = 1000\nbound = [0.1, 0.15, 0.2, 0.25, 0.3]\nn_sample = [1, 1, 2, 2, 4]\n\n# this will be a list of length 10 with a GenePopulation object in each element\ngene_populations_list = []\nfor bound_proportion, n_draws in zip(bound, n_sample):\n    for _ in range(n_draws):\n        gene_populations_list.append(generate_gene_population(n_genes, bound_proportion))\n\n</code></pre> <pre><code># Generate binding data for each gene population\nbinding_effect_list = [generate_binding_effects(gene_population)\n                     for gene_population in gene_populations_list]\n\n\n# Calculate p-values for binding data\nbinding_pvalue_list = [generate_pvalues(binding_data) for binding_data in binding_effect_list]\n\nbinding_data_combined = [torch.stack((gene_population.labels, binding_effect, binding_pval), dim=1)\n                         for gene_population, binding_effect, binding_pval\n                         in zip (gene_populations_list, binding_effect_list, binding_pvalue_list)]\n\n# Stack along a new dimension (dim=1) to create a tensor of shape [num_genes, num_TFs, 3]\nbinding_data_tensor = torch.stack(binding_data_combined, dim=1)\n\n# Verify the shape\nprint(\"Shape of the binding data tensor:\", binding_data_tensor.shape)\n\n</code></pre> <pre>\n<code>Shape of the binding data tensor: torch.Size([1000, 10, 3])\n</code>\n</pre> <pre><code># See `generate_perturbation_effects()` in the help or the documentation for more details.\nperturbation_effects_list_no_mean_adjustment = [generate_perturbation_effects(binding_data_tensor[:, tf_index, :].unsqueeze(1), tf_index=0) \n                                                        for tf_index in range(sum(n_sample))]\nperturbation_pvalue_list_no_mean_adjustment = [generate_pvalues(perturbation_effects) for perturbation_effects in perturbation_effects_list_no_mean_adjustment]\n</code></pre> <pre><code># if you want to modify the default mean for bound genes, you can pass in the 'bound_mean' parameter\nperturbation_effects_list_normal_mean_adjustment = generate_perturbation_effects(\n    binding_data_tensor, \n    max_mean_adjustment=10.0\n)\n\n# since the p-value generation function operates on one column at a time, we must iterate over the columns of our perturb effects\n# list and generate p-values for each column\nperturbation_effects_list_normal_mean_adjustment_pvalues = torch.zeros_like(perturbation_effects_list_normal_mean_adjustment)\nfor col_idx in range(perturbation_effects_list_normal_mean_adjustment.shape[1]):\n    col = perturbation_effects_list_normal_mean_adjustment[:, col_idx]\n    col_pvals = generate_pvalues(col)\n    perturbation_effects_list_normal_mean_adjustment_pvalues[:, col_idx] = col_pvals\n</code></pre> <pre><code># define our dictionary of TF relationships\n# For each gene, if TF 0 is bound, then we only adjust its mean if TF 1 is also bound\n# similarly, if TF 7 is bound, we still only adjust its mean if TFs 1 and 4 are bound\ntf_relationships = {\n    0: [1],\n    1: [8],\n    2: [5, 6],\n    3: [4],\n    4: [5],\n    5: [9],\n    6: [4],\n    7: [1, 4],\n    8: [6],\n    9: [4],\n}\n\nperturbation_effects_list_dep_mean_adjustment = generate_perturbation_effects(\n    binding_data_tensor, \n    tf_relationships=tf_relationships,\n    adjustment_function=perturbation_effect_adjustment_function_with_tf_relationships,\n    max_mean_adjustment=10.0,\n)\nperturbation_effects_list_dep_mean_adjustment_pvalues = torch.zeros_like(perturbation_effects_list_dep_mean_adjustment)\nfor col_idx in range(perturbation_effects_list_dep_mean_adjustment.shape[1]):\n    col = perturbation_effects_list_dep_mean_adjustment[:, col_idx]\n    col_pvals = generate_pvalues(col)\n    perturbation_effects_list_dep_mean_adjustment_pvalues[:, col_idx] = col_pvals\n</code></pre> <pre><code># note that Or(1,1) is used to enforce a unary contraint\ntf_relationships_dict_boolean_logic = {\n    0: [And(3, 4, 8), Or(3, 7), Or(1, 1)],\n    1: [And(5, Or(7, 8))],\n    2: [],\n    3: [Or(7, 9), And(6, 7)],\n    4: [And(1, 2)],\n    5: [Or(0, 1, 2, 8, 9)],\n    6: [And(0, Or(1, 2))],\n    7: [Or(2, And(5, 6, 9))],\n    8: [],\n    9: [And(6, And(3, Or(0, 9)))],\n}\n\nperturbation_effects_list_boolean_logic = generate_perturbation_effects(\n    binding_data_tensor, \n    adjustment_function=perturbation_effect_adjustment_function_with_tf_relationships_boolean_logic,\n    tf_relationships=tf_relationships_dict_boolean_logic,\n    max_mean_adjustment=10.0,\n)\nperturbation_effects_list_boolean_logic_pvalues = torch.zeros_like(perturbation_effects_list_boolean_logic)\nfor col_idx in range(perturbation_effects_list_boolean_logic.shape[1]):\n    col = perturbation_effects_list_boolean_logic[:, col_idx]\n    col_pvals = generate_pvalues(col)\n    perturbation_effects_list_boolean_logic_pvalues[:, col_idx] = col_pvals\n</code></pre> <pre><code># Convert lists to tensors if they are not already\nperturbation_effects_tensor = torch.stack(perturbation_effects_list_no_mean_adjustment, dim=1)\nperturbation_pvalues_tensor = torch.stack(perturbation_pvalue_list_no_mean_adjustment, dim=1)\n\n# Ensure perturbation data is reshaped to match [n_genes, n_tfs]\n# This step might need adjustment based on the actual shapes of your tensors.\nperturbation_effects_tensor = perturbation_effects_tensor.unsqueeze(-1)  # Adds an extra dimension for concatenation\nperturbation_pvalues_tensor = perturbation_pvalues_tensor.unsqueeze(-1)  # Adds an extra dimension for concatenation\n\n# Concatenate along the last dimension to form a [n_genes, n_tfs, 5] tensor\nfinal_data_tensor = torch.cat((binding_data_tensor, perturbation_effects_tensor, perturbation_pvalues_tensor), dim=2)\n\n# Verify the shape\nprint(\"Shape of the final data tensor:\", final_data_tensor.shape)\n</code></pre> <pre>\n<code>Shape of the final data tensor: torch.Size([1000, 10, 5])\n</code>\n</pre> <p>As an aside, I choose to structure the data this way by looking at the result of strides, which describes how the data is stored in memory:</p> <pre><code>tensor_continuous = torch.empty(100, 1000, 3)\nstrides_continuous = tensor_continuous.stride()\nprint(strides_continuous)\n\n\ntensor_continuous = torch.empty(1000, 100, 3)\nstrides_continuous = tensor_continuous.stride()\nprint(strides_continuous)\n</code></pre> <pre>\n<code>(3000, 3, 1)\n(300, 3, 1)\n</code>\n</pre> <pre><code>tolerance = 1e-5\nare_equal = torch.isclose(\n    torch.sum(final_data_tensor[:, :, 0] == 1, axis=0),\n    torch.tensor([val * n_genes for val, count in zip(bound, n_sample) for _ in range(count)],\n                 dtype=torch.long),\n    atol=tolerance)\n\nprint(f\"bound/noise ratio is correct: {are_equal.all()}\")\n</code></pre> <pre>\n<code>bound/noise ratio is correct: True\n</code>\n</pre> <pre><code>labels = final_data_tensor[:, :, 0].flatten()\nunbound_binding = final_data_tensor[:, :, 1].flatten()[labels == 0]\nbound_binding = final_data_tensor[:, :, 1].flatten()[labels == 1]\n\nprint(f\"The unbound binding max is {unbound_binding.max()} and the min is {unbound_binding.min()}\")\nprint(f\"the unbound min is {unbound_binding.min()}\")\nprint(f\"the unbound mean is {unbound_binding.mean()} and the std is {unbound_binding.std()}\")\nprint(f\"The bound binding max is {bound_binding.max()} and the min is {bound_binding.min()}\")\nprint(f\"the bound min is {bound_binding.min()}\")\nprint(f\"the bound mean is {bound_binding.mean()} and the std is {bound_binding.std()}\")\n\n#this is the output before EJ change to adjustment mean\n# The unbound binding max is 13.157892227172852 and the min is 0.0\n# the unbound min is 0.0\n# the unbound mean is 0.3589712679386139 and the std is 1.1559306383132935\n# The bound binding max is 78.94734954833984 and the min is 0.1315789520740509\n# the bound min is 0.1315789520740509\n# the bound mean is 2.4840002059936523 and the std is 6.374814510345459\n</code></pre> <pre>\n<code>The unbound binding max is 13.157892227172852 and the min is 0.0\nthe unbound min is 0.0\nthe unbound mean is 0.3589712679386139 and the std is 1.1559306383132935\nThe bound binding max is 78.94734954833984 and the min is 0.1315789520740509\nthe bound min is 0.1315789520740509\nthe bound mean is 2.4840002059936523 and the std is 6.374814510345459\n</code>\n</pre> <pre><code>\n# Plotting\nplt.figure(figsize=(10, 6))\nplt.hist(unbound_binding, bins=30, alpha=0.5, label='Label 0', color='orange')\nplt.hist(bound_binding, bins=30, alpha=0.5, label='Label 1', color='blue')\nplt.xlim(0,5)\nplt.title('Histogram of Values in the 2nd Column')\nplt.xlabel('Values')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()\n</code></pre> <pre><code>unbound_perturbation = final_data_tensor[:, :, 3].flatten()[labels == 0]\nbound_perturbation = final_data_tensor[:, :, 3].flatten()[labels == 1]\n\nprint(f\"The unbound binding max is {unbound_perturbation.max()} and the min is {unbound_perturbation.min()}\")\nprint(f\"the unbound min is {unbound_perturbation.min()}\")\nprint(f\"the unbound mean is {unbound_perturbation.mean()} and the std is {unbound_perturbation.std()}\")\nprint(f\"The bound binding max is {bound_perturbation.max()} and the min is {bound_perturbation.min()}\")\nprint(f\"the bound min is {bound_perturbation.min()}\")\nprint(f\"the bound mean is {bound_perturbation.mean()} and the std is {bound_perturbation.std()}\")\n\n#pre change data\n# The unbound binding max is 3.423511505126953 and the min is -3.506139039993286\n# the unbound min is -3.506139039993286\n# the unbound mean is 0.010617653839290142 and the std is 0.988001823425293\n# The bound binding max is 6.107701301574707 and the min is -6.406703948974609\n# the bound min is -6.406703948974609\n# the bound mean is -0.011303802020847797 and the std is 3.136451482772827\n</code></pre> <pre>\n<code>The unbound binding max is 3.423511505126953 and the min is -3.506139039993286\nthe unbound min is -3.506139039993286\nthe unbound mean is 0.010617653839290142 and the std is 0.988001823425293\nThe bound binding max is 6.107701301574707 and the min is -6.406703948974609\nthe bound min is -6.406703948974609\nthe bound mean is -0.011303802020847797 and the std is 3.136451482772827\n</code>\n</pre> <pre><code># Plotting\nplt.figure(figsize=(10, 6))\nplt.hist(unbound_perturbation, bins=30, alpha=0.5, label='Label 0', color='orange')\nplt.hist(bound_perturbation, bins=30, alpha=0.5, label='Label 1', color='blue')\nplt.title('Histogram of Values in the 2nd Column')\nplt.xlabel('Values')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()\n</code></pre> <pre><code># plot the binding effects vs the perturbation effects\n# color the points by the label\n# make sure the labels are categorical\n# label 0 should be blue while label 1 should be orange\n\n# Plotting\nplt.figure(figsize=(10, 6))\nplt.scatter(final_data_tensor[:, :, 1].flatten(), final_data_tensor[:, :, 3].flatten().abs(), c=['orange' if x == 0 else 'blue' for x in labels])\nplt.title('Binding Effects vs Perturbation Effects')\nplt.xlabel('Binding Effects')\nplt.ylabel('Perturbation Effects')\nplt.xlim(0,100)\nplt.show()\n</code></pre> <pre><code># in this case, select the TF binding data that corresponds with the effect data\n# which we wish to produce. use the .unsqueeze(1) method to add the TF dimension\n# after selecting the TF\nperturbation_effects_tf_influenced = generate_perturbation_effects(\n    binding_data_tensor, \n    max_mean_adjustment=3.0, # try 0.1, 3.0, and 10.0\n    bound_mean=5.0, # try 3.0, 5.0, or 10.0\n    unbound_mean=0.0, # try adjusting this\n)\nperturbation_pvalue_tf_influenced = torch.zeros_like(perturbation_effects_tf_influenced)\nfor col_idx in range(perturbation_effects_tf_influenced.shape[1]):\n    col = perturbation_effects_tf_influenced[:, col_idx]\n    col_pvals = generate_pvalues(col)\n    perturbation_pvalue_tf_influenced[:, col_idx] = col_pvals\n\nperturbation_effects_tensor_tf_influened = perturbation_effects_tf_influenced.unsqueeze(-1)\nperturbation_pvalues_tensor_tf_influenced = perturbation_pvalue_tf_influenced.unsqueeze(-1)\n\nfinal_data_tensor_tf_influenced = torch.cat(\n    (binding_data_tensor,\n     perturbation_effects_tensor_tf_influened,\n     perturbation_pvalues_tensor_tf_influenced), \n    dim=2)\n\n# Verify the shape\nprint(\"Shape of the final data tensor:\", final_data_tensor.shape)\n</code></pre> <pre>\n<code>Shape of the final data tensor: torch.Size([1000, 10, 5])\n</code>\n</pre> <pre><code># Plotting. Note that the 'unbound' group effects are still range from 0 to 3\n\nplt.figure(figsize=(10, 6))\nplt.scatter(final_data_tensor_tf_influenced[:, :, 1].flatten(), final_data_tensor_tf_influenced[:, :, 3].flatten().abs(), c=['orange' if x == 0 else 'blue' for x in labels])\nplt.title('Binding Effects vs Perturbation Effects')\nplt.xlabel('Binding Effects')\nplt.ylabel('Perturbation Effects')\n\nlegend_labels = ['Bound', 'Unbound']\ncolors = ['blue', 'orange']\nlegend_handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10) for color in colors]\nplt.legend(legend_handles, legend_labels)\n\nplt.show()\n</code></pre> <pre>\n<code>&lt;Figure size 1000x600 with 1 Axes&gt;</code>\n</pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"tutorials/generate_in_silico_data/#generating-in-silico-data","title":"Generating in silico data","text":""},{"location":"tutorials/generate_in_silico_data/#step-1","title":"Step 1:","text":"<p>The first step is to generate a gene population, or set of gene populations. A gene population is simply a class that stores a 1D tensor called <code>labels</code>. <code>labels</code> is a boolean vector where 1 means the gene is part of the bound group (a gene which is both bound and responsive to the TF) while 0 means the gene is part of the background or unbound group. The length of <code>labels</code> is the number of genes in the population, and the index should be considered the unique gene identifier. In other words, the indicies should never change.</p>"},{"location":"tutorials/generate_in_silico_data/#step-2","title":"Step 2:","text":"<p>The second step is to generate binding data from the gene population(s).</p>"},{"location":"tutorials/generate_in_silico_data/#step-3-generate-perturbation-data","title":"Step 3: Generate perturbation data.","text":"<p>It is important to understand that there are four possible ways we provide for you to generate perturbation data. 1. No Mean Adjustment 2. Standard Mean Adjustment 3. Mean adjustment dependent on all TFs bound to gene in question 4. Mean adjustment dependent on binary relationships between bound and unbound TFs to gene in question.</p>"},{"location":"tutorials/generate_in_silico_data/#method-1-generating-perturbation-data-with-no-mean-adjustment","title":"Method 1: Generating perturbation data with no mean adjustment","text":"<p>If you don\u2019t pass in a value for <code>max_mean_adjustment</code> to <code>generate_perturbation_effects</code> it will default to zero, meaning the means of the perturbation effects will not be adjusted in any way and will all be equal to <code>bound_mean</code> (deault is 3.0) for bound TF-gene pairs and <code>unbound_mean</code> (default is 0.0) for unbound TF-gene pairs.</p>"},{"location":"tutorials/generate_in_silico_data/#method-2-generating-perturbation-data-with-a-simple-mean-adjustment","title":"Method 2: Generating perturbation data with a simple mean adjustment","text":"<p>If you do pass in a nonzero value for <code>max_mean_adjustment</code>, the means of bound gene-TF pairs will be adjusted by up to a maximum of <code>max_mean_adjustment</code>. Note that instead of passing in one column (corresponding to one TF) of the binding data tensor at a time, we instead pass in the entire binding data tensor at once. This syntactic difference is just a result of how our mean adjustment functions requires the entire matrix of all genes and TFs as opposed to being able to operate on one column at once. Using this data generation method, we adust the mean of any TF that is bound to a gene.</p>"},{"location":"tutorials/generate_in_silico_data/#method-3-generating-perturbation-data-with-a-mean-adjustment-dependent-on-which-tfs-are-bound-to-gene","title":"Method 3: Generating Perturbation Data with a mean adjustment dependent on which TFs are bound to gene","text":"<p>You are also able to specify a dictionary of TF relationships. Passing in this dictionary in combination with using our <code>perturbation_effect_adjustment_function_with_tf_relationships</code> mean adjustment function alows for you to only adjust the means of perturbation effects if the TF in the TF-gene pair in question is bound AND all other TFs associated with that TF are bound to the same gene. To associate a TF with another TF, put its index in the list of TFs corresponding to the other TF\u2019s index in the tf_relationships dictionary.</p>"},{"location":"tutorials/generate_in_silico_data/#method-4-generating-perturbation-data-with-a-mean-adjustment-dependent-on-boolean-relationships-between-tfs","title":"Method 4: Generating Perturbation Data with a mean adjustment dependent on boolean relationships between TFs","text":"<p>(see the documentation in <code>yeastdnnexplorer/probability_models/relation_classes.py</code> for more information on <code>And()</code> and <code>Or()</code>) </p> <p>This is a more advanced version of method 3 where instead of only specifying direct dependencies you can specify logical relations that must be satisfied for a gene-TF pair\u2019s perturbation effect value to be adjusted. For example, in the below example we only adjust the mean of TF 3 for each gene if TF 3 is bound and (7 || 9) &amp;&amp; (6 &amp;&amp; 7) are bound. </p>"},{"location":"tutorials/generate_in_silico_data/#step-4-assemble","title":"Step 4: Assemble","text":"<p>The final step is to assemble the data into a single tensor. Here is one way. The order of the matrix in the last dimension is:</p> <ol> <li>bound/unbound label</li> <li>binding effect</li> <li>binding pvalue</li> <li>perturbation effect</li> <li>perturbation pvalue</li> </ol> <p>For simplicity\u2019s sake, we will use the perturbation effect data we generated with no mean adjustment. However you can assemble the data using perturbation effect data generated from any of the 4 methods we covered above.</p>"},{"location":"tutorials/generate_in_silico_data/#sanity-checks","title":"Sanity checks","text":"<p>Ensure that the generated data matches expectations.</p>"},{"location":"tutorials/generate_in_silico_data/#the-boundunbound-ratios-should-match-exactly-the-initial-bound-ratio","title":"The bound/unbound ratios should match exactly the initial bound ratio","text":""},{"location":"tutorials/generate_in_silico_data/#binding-effect-distributions-should-match-expectations","title":"Binding effect distributions should match expectations","text":""},{"location":"tutorials/generate_in_silico_data/#perturbation-effect-distribtuions-should-match-expectations","title":"Perturbation effect distribtuions should match expectations","text":""},{"location":"tutorials/generate_in_silico_data/#the-binding-effects-should-be-positively-correlated-with-the-perturbaiton-effects","title":"The binding effects should be positively correlated with the perturbaiton effects","text":""},{"location":"tutorials/generate_in_silico_data/#re-generate-data-with-an-explicit-relationship-between-a-give-tfs-binding-and-perturbation-effects","title":"Re-generate data with an explicit relationship between a give TF\u2019s binding and perturbation effects","text":""},{"location":"tutorials/hyperparameter_sweep/","title":"Hyperparameter Sweep","text":"<p>This notebook introduces how to perform a hyperparameter sweep to find the best hyperparameters for our model using the Optuna library. Feel free to modify the objective function if you would like to test other hyperparameters or values.</p> <pre><code># imports \nimport argparse\nfrom argparse import Namespace\n\nfrom pytorch_lightning import Trainer, LightningModule, seed_everything\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\nfrom torchsummary import summary\n\nfrom yeastdnnexplorer.data_loaders.synthetic_data_loader import SyntheticDataLoader\nfrom yeastdnnexplorer.ml_models.simple_model import SimpleModel\nfrom yeastdnnexplorer.ml_models.customizable_model import CustomizableModel\n\nimport optuna\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# set random seed for reproducability\nseed_everything(42)\n</code></pre> <pre>\n<code>Seed set to 42\n</code>\n</pre> <pre>\n<code>42</code>\n</pre> <p>Here we define loggers and checkpoints for our model. Checkpoints tell pytorch when to save instances of the model (that can be loaded and inspected later) and loggers tell pytorch how to format the metrics that the model logs during its training. </p> <pre><code># Checkpoint to save the best version of model (during the entire training process) based on the metric passed into \"monitor\"\nbest_model_checkpoint = ModelCheckpoint(\n    monitor=\"val_mse\",  # You can modify this to save the best model based on any other metric that the model you're testing tracks and reports\n    mode=\"min\",\n    filename=\"best-model-{epoch:02d}-{val_loss:.2f}.ckpt\",\n    save_top_k=1,  # Can modify this to save the top k models\n)\n\n# Callback to save checkpoints every 2 epochs, regardless of performance\nperiodic_checkpoint = ModelCheckpoint(\n    filename=\"periodic-{epoch:02d}.ckpt\",\n    every_n_epochs=2,\n    save_top_k=-1,  # Setting -1 saves all checkpoints\n)\n\n# define loggers for the model\ntb_logger = TensorBoardLogger(\"logs/tensorboard_logs\")\ncsv_logger = CSVLogger(\"logs/csv_logs\")\n</code></pre> <p>Now we perform our hyperparameter sweep using the Optuna library. To do this, we need to define an objective function that returns a scalar value. This scalar value will be the value that our sweep is attempting to minimize. We train one instance of our model inside each call to the objective function (each model on each iteration will use a different selection of hyperparameters). In our objective function, we return the validation mse associated with the instance of the model. This is because we would like to find the combination of hyperparameters that leads to the lowest validation mse. We use validation mse instead of test mse since we do not want to risk fitting to the test data at all while tuning hyperparameters.</p> <p>If you\u2019d like to try different hyperparameters, you just need to modify the list of possible values corresponding to the hyperparameter in question.</p> <p>If you\u2019d like to run the hyperparamter sweep on real data instead of synthetic data, simply swap out the synthetic data loader for the real data loader.</p> <pre><code># on each call to the objective function, it will choose a hyperparameter value from each of the suggest_categorical arrays and pass them into the model\n    # this allows us to test many different hyperparameter configurations during our sweep\n\ndef objective(trial):\n    # model hyperparameters\n    lr = trial.suggest_categorical(\"lr\", [0.01])\n    hidden_layer_num = trial.suggest_categorical(\"hidden_layer_num\", [1, 2, 3, 5])\n    activation = trial.suggest_categorical(\n        \"activation\", [\"ReLU\", \"Sigmoid\", \"Tanh\", \"LeakyReLU\"]\n    )\n    optimizer = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\", \"RMSprop\"])\n    L2_regularization_term = trial.suggest_categorical(\n        \"L2_regularization_term\", [0.0, 0.1]\n    )\n    dropout_rate = trial.suggest_categorical(\n        \"dropout_rate\", [0.0, 0.5]\n    )\n\n    # data module hyperparameters\n    batch_size = trial.suggest_categorical(\"batch_size\", [32])\n\n    # training hyperparameters\n    max_epochs = trial.suggest_categorical(\n        \"max_epochs\", [1]\n    ) # default is 10\n\n    # defining what to pass in for the hidden layer sizes list based on the number of hidden layers\n    hidden_layer_sizes_configurations = {\n        1: [[64], [256]],\n        2: [[64, 32], [256, 64]],\n        3: [[256, 128, 32], [512, 256, 64]],\n        5: [[512, 256, 128, 64, 32]],\n    }\n    hidden_layer_sizes = trial.suggest_categorical(\n        f\"hidden_layer_sizes_{hidden_layer_num}_layers\",\n        hidden_layer_sizes_configurations[hidden_layer_num],\n    )\n\n    print(\"=\" * 70)\n    print(\"About to create model with the following hyperparameters:\")\n    print(f\"lr: {lr}\")\n    print(f\"hidden_layer_num: {hidden_layer_num}\")\n    print(f\"hidden_layer_sizes: {hidden_layer_sizes}\")\n    print(f\"activation: {activation}\")\n    print(f\"optimizer: {optimizer}\")\n    print(f\"L2_regularization_term: {L2_regularization_term}\")\n    print(f\"dropout_rate: {dropout_rate}\")\n    print(f\"batch_size: {batch_size}\")\n    print(f\"max_epochs: {max_epochs}\")\n    print(\"\")\n\n    # create data module\n    data_module = SyntheticDataLoader(\n        batch_size=batch_size,\n        num_genes=4000,\n        bound_mean=3.0,\n        bound=[0.5] * 10,\n        n_sample=[1, 2, 2, 4, 4],\n        val_size=0.1,\n        test_size=0.1,\n        random_state=42,\n        max_mean_adjustment=3.0,\n    )\n\n    num_tfs = sum(data_module.n_sample)  # sum of all n_sample is the number of TFs\n\n    # create model\n    model = CustomizableModel(\n        input_dim=num_tfs,\n        output_dim=num_tfs,\n        lr=lr,\n        hidden_layer_num=hidden_layer_num,\n        hidden_layer_sizes=hidden_layer_sizes,\n        activation=activation,\n        optimizer=optimizer,\n        L2_regularization_term=L2_regularization_term,\n        dropout_rate=dropout_rate,\n    )\n\n    # create trainer\n    trainer = Trainer(\n        max_epochs=max_epochs,\n        deterministic=True,\n        accelerator=\"cpu\",\n        # callbacks and loggers are commented out for now since running a large sweep would generate an unnecessarily huge amount of checkpoints and logs\n        # callbacks=[best_model_checkpoint, periodic_checkpoint],\n        # logger=[tb_logger, csv_logger],\n    )\n\n    # train model\n    trainer.fit(model, data_module)\n\n    # get best validation loss from the model\n    return trainer.callback_metrics[\"val_mse\"]\n</code></pre> <p>Now we define an optuna study, which represents our hyperparameter sweep. It will run the objective function n_trials times and choose the model that gave the best val_mse across all of those trials with different hyperparameters. Note that this will create a very large amount of output as it will show training stats for every model. This is why we print out the best params and loss in a separate cell.</p> <pre><code>STUDY_NAME = \"CustomizableModelHyperparameterSweep3\"\nNUM_TRIALS = 5 # you will need a lot more than 5 trials if you have many possible combinations of hyperparams\n\n# Perform hyperparameter optimization using Optuna\nstudy = optuna.create_study(\n    direction=\"minimize\", # we want to minimize the val_mse\n    study_name=STUDY_NAME,\n    # storage=\"sqlite:///db.sqlite3\", # you can save the study results in a database if you'd like, this is needed if you want to try and use the optuna dashboard library to dispaly results\n)\nstudy.optimize(objective, n_trials=NUM_TRIALS)\n\n# Get the best hyperparameters and their corresponding values\nbest_params = study.best_params\nbest_loss = study.best_value\n</code></pre> <pre>\n<code>[I 2024-05-29 13:18:03,548] A new study created in memory with name: CustomizableModelHyperparameterSweep3\n/Users/ericjia/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [64] which is of type list.\n  warnings.warn(message)\n/Users/ericjia/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [256] which is of type list.\n  warnings.warn(message)\n</code>\n</pre> <pre>\n<code>======================================================================\nAbout to create model with the following hyperparameters:\nlr: 0.01\nhidden_layer_num: 1\nhidden_layer_sizes: [256]\nactivation: Tanh\noptimizer: RMSprop\nL2_regularization_term: 0.1\ndropout_rate: 0.5\nbatch_size: 32\nmax_epochs: 1\n\n</code>\n</pre> <pre>\n<code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n/Users/ericjia/yeastdnnexplorer/yeastdnnexplorer/data_loaders/synthetic_data_loader.py:260: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_train, Y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(\n/Users/ericjia/yeastdnnexplorer/yeastdnnexplorer/data_loaders/synthetic_data_loader.py:263: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_val, Y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(\n/Users/ericjia/yeastdnnexplorer/yeastdnnexplorer/data_loaders/synthetic_data_loader.py:266: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_test, Y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(\n\n  | Name          | Type              | Params\n----------------------------------------------------\n0 | activation    | Tanh              | 0     \n1 | input_layer   | Linear            | 3.6 K \n2 | hidden_layers | ModuleList        | 0     \n3 | output_layer  | Linear            | 3.3 K \n4 | dropout       | Dropout           | 0     \n5 | mae           | MeanAbsoluteError | 0     \n6 | SMSE          | SMSE              | 0     \n----------------------------------------------------\n6.9 K     Trainable params\n0         Non-trainable params\n6.9 K     Total params\n0.028     Total estimated model params size (MB)\n</code>\n</pre> <pre>\n<code>Sanity Checking: |                                                                                            \u2026</code>\n</pre> <pre>\n<code>/Users/ericjia/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n</code>\n</pre> <pre>\n<code>Training: |                                                                                                   \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>`Trainer.fit` stopped: `max_epochs=1` reached.\n[I 2024-05-29 13:18:26,417] Trial 0 finished with value: 4.489274501800537 and parameters: {'lr': 0.01, 'hidden_layer_num': 1, 'activation': 'Tanh', 'optimizer': 'RMSprop', 'L2_regularization_term': 0.1, 'dropout_rate': 0.5, 'batch_size': 32, 'max_epochs': 1, 'hidden_layer_sizes_1_layers': [256]}. Best is trial 0 with value: 4.489274501800537.\n/Users/ericjia/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [64] which is of type list.\n  warnings.warn(message)\n/Users/ericjia/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [256] which is of type list.\n  warnings.warn(message)\nGPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n</code>\n</pre> <pre>\n<code>======================================================================\nAbout to create model with the following hyperparameters:\nlr: 0.01\nhidden_layer_num: 1\nhidden_layer_sizes: [256]\nactivation: LeakyReLU\noptimizer: SGD\nL2_regularization_term: 0.1\ndropout_rate: 0.5\nbatch_size: 32\nmax_epochs: 1\n\n</code>\n</pre> <pre>\n<code>\n  | Name          | Type              | Params\n----------------------------------------------------\n0 | activation    | LeakyReLU         | 0     \n1 | input_layer   | Linear            | 3.6 K \n2 | hidden_layers | ModuleList        | 0     \n3 | output_layer  | Linear            | 3.3 K \n4 | dropout       | Dropout           | 0     \n5 | mae           | MeanAbsoluteError | 0     \n6 | SMSE          | SMSE              | 0     \n----------------------------------------------------\n6.9 K     Trainable params\n0         Non-trainable params\n6.9 K     Total params\n0.028     Total estimated model params size (MB)\n</code>\n</pre> <pre>\n<code>Sanity Checking: |                                                                                            \u2026</code>\n</pre> <pre>\n<code>Training: |                                                                                                   \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>`Trainer.fit` stopped: `max_epochs=1` reached.\n[I 2024-05-29 13:18:45,320] Trial 1 finished with value: 6.033911228179932 and parameters: {'lr': 0.01, 'hidden_layer_num': 1, 'activation': 'LeakyReLU', 'optimizer': 'SGD', 'L2_regularization_term': 0.1, 'dropout_rate': 0.5, 'batch_size': 32, 'max_epochs': 1, 'hidden_layer_sizes_1_layers': [256]}. Best is trial 0 with value: 4.489274501800537.\n/Users/ericjia/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [64, 32] which is of type list.\n  warnings.warn(message)\n/Users/ericjia/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [256, 64] which is of type list.\n  warnings.warn(message)\nGPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n</code>\n</pre> <pre>\n<code>======================================================================\nAbout to create model with the following hyperparameters:\nlr: 0.01\nhidden_layer_num: 2\nhidden_layer_sizes: [256, 64]\nactivation: ReLU\noptimizer: SGD\nL2_regularization_term: 0.0\ndropout_rate: 0.5\nbatch_size: 32\nmax_epochs: 1\n\n</code>\n</pre> <pre>\n<code>\n  | Name          | Type              | Params\n----------------------------------------------------\n0 | activation    | ReLU              | 0     \n1 | input_layer   | Linear            | 3.6 K \n2 | hidden_layers | ModuleList        | 16.4 K\n3 | output_layer  | Linear            | 845   \n4 | dropout       | Dropout           | 0     \n5 | mae           | MeanAbsoluteError | 0     \n6 | SMSE          | SMSE              | 0     \n----------------------------------------------------\n20.9 K    Trainable params\n0         Non-trainable params\n20.9 K    Total params\n0.084     Total estimated model params size (MB)\n</code>\n</pre> <pre>\n<code>Sanity Checking: |                                                                                            \u2026</code>\n</pre> <pre>\n<code>Training: |                                                                                                   \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>`Trainer.fit` stopped: `max_epochs=1` reached.\n[I 2024-05-29 13:19:02,993] Trial 2 finished with value: 6.900921821594238 and parameters: {'lr': 0.01, 'hidden_layer_num': 2, 'activation': 'ReLU', 'optimizer': 'SGD', 'L2_regularization_term': 0.0, 'dropout_rate': 0.5, 'batch_size': 32, 'max_epochs': 1, 'hidden_layer_sizes_2_layers': [256, 64]}. Best is trial 0 with value: 4.489274501800537.\nGPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n</code>\n</pre> <pre>\n<code>======================================================================\nAbout to create model with the following hyperparameters:\nlr: 0.01\nhidden_layer_num: 2\nhidden_layer_sizes: [64, 32]\nactivation: Tanh\noptimizer: Adam\nL2_regularization_term: 0.1\ndropout_rate: 0.0\nbatch_size: 32\nmax_epochs: 1\n\n</code>\n</pre> <pre>\n<code>\n  | Name          | Type              | Params\n----------------------------------------------------\n0 | activation    | Tanh              | 0     \n1 | input_layer   | Linear            | 896   \n2 | hidden_layers | ModuleList        | 2.1 K \n3 | output_layer  | Linear            | 429   \n4 | dropout       | Dropout           | 0     \n5 | mae           | MeanAbsoluteError | 0     \n6 | SMSE          | SMSE              | 0     \n----------------------------------------------------\n3.4 K     Trainable params\n0         Non-trainable params\n3.4 K     Total params\n0.014     Total estimated model params size (MB)\n</code>\n</pre> <pre>\n<code>Sanity Checking: |                                                                                            \u2026</code>\n</pre> <pre>\n<code>Training: |                                                                                                   \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>`Trainer.fit` stopped: `max_epochs=1` reached.\n[I 2024-05-29 13:19:19,976] Trial 3 finished with value: 4.5260910987854 and parameters: {'lr': 0.01, 'hidden_layer_num': 2, 'activation': 'Tanh', 'optimizer': 'Adam', 'L2_regularization_term': 0.1, 'dropout_rate': 0.0, 'batch_size': 32, 'max_epochs': 1, 'hidden_layer_sizes_2_layers': [64, 32]}. Best is trial 0 with value: 4.489274501800537.\n/Users/ericjia/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [512, 256, 128, 64, 32] which is of type list.\n  warnings.warn(message)\nGPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n</code>\n</pre> <pre>\n<code>======================================================================\nAbout to create model with the following hyperparameters:\nlr: 0.01\nhidden_layer_num: 5\nhidden_layer_sizes: [512, 256, 128, 64, 32]\nactivation: Tanh\noptimizer: RMSprop\nL2_regularization_term: 0.1\ndropout_rate: 0.5\nbatch_size: 32\nmax_epochs: 1\n\n</code>\n</pre> <pre>\n<code>\n  | Name          | Type              | Params\n----------------------------------------------------\n0 | activation    | Tanh              | 0     \n1 | input_layer   | Linear            | 7.2 K \n2 | hidden_layers | ModuleList        | 174 K \n3 | output_layer  | Linear            | 429   \n4 | dropout       | Dropout           | 0     \n5 | mae           | MeanAbsoluteError | 0     \n6 | SMSE          | SMSE              | 0     \n----------------------------------------------------\n182 K     Trainable params\n0         Non-trainable params\n182 K     Total params\n0.729     Total estimated model params size (MB)\n</code>\n</pre> <pre>\n<code>Sanity Checking: |                                                                                            \u2026</code>\n</pre> <pre>\n<code>Training: |                                                                                                   \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>`Trainer.fit` stopped: `max_epochs=1` reached.\n[I 2024-05-29 13:19:37,861] Trial 4 finished with value: 4.612905502319336 and parameters: {'lr': 0.01, 'hidden_layer_num': 5, 'activation': 'Tanh', 'optimizer': 'RMSprop', 'L2_regularization_term': 0.1, 'dropout_rate': 0.5, 'batch_size': 32, 'max_epochs': 1, 'hidden_layer_sizes_5_layers': [512, 256, 128, 64, 32]}. Best is trial 0 with value: 4.489274501800537.\n</code>\n</pre> <p>Print out the best hyperparameters and the val_mse assocaited with the model with the best hyperparameters.</p> <pre><code>print(\"RESULTS\" + (\"=\" * 70))\nprint(f\"Best hyperparameters: {best_params}\")\nprint(f\"Best loss: {best_loss}\")\n</code></pre> <pre>\n<code>RESULTS======================================================================\nBest hyperparameters: {'lr': 0.01, 'hidden_layer_num': 1, 'activation': 'Tanh', 'optimizer': 'RMSprop', 'L2_regularization_term': 0.1, 'dropout_rate': 0.5, 'batch_size': 32, 'max_epochs': 1, 'hidden_layer_sizes_1_layers': [256]}\nBest loss: 4.489274501800537\n</code>\n</pre> <p>And that\u2019s it! Now you could take what you found to be the best hyperparameters and train a model with them for many more epochs. The Optuna Documentation will be a helpful resource if you\u2019d like to add more to this notebook or the hyperparam sweep functions</p> <pre><code>\n</code></pre>"},{"location":"tutorials/lightning_crash_course/","title":"Lightning Crash Course","text":"<pre><code># imports\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\n\nfrom yeastdnnexplorer.data_loaders.synthetic_data_loader import SyntheticDataLoader\nfrom yeastdnnexplorer.data_loaders.real_data_loader import RealDataLoader\nfrom yeastdnnexplorer.ml_models.simple_model import SimpleModel\nfrom yeastdnnexplorer.ml_models.customizable_model import CustomizableModel\n</code></pre> <p>In Pytorch Lightning, the data is kept completely separate from the models. This allows for you to easy train a model using different datasets or train different models on the same dataset. <code>DataModules</code> encapsulate all the logic of loading in a specific dataset and splitting into training, testing, and validation sets. In this project, we have two data loaders defined: <code>SyntheticDataLoader</code> for the in silico data (which takes in many parameters that allow you to specify how the data is generated) and <code>RealDataLoader</code> which contains all of the logic for loading in the real experiment data and putting it into a form that the models expect.</p> <p>Once you decide what model you want to train and what dataModule you want to use, you can bundle these with a <code>Trainer</code> object to train the model on the dataset.</p> <p>If you\u2019d like to learn more about the models and dataModules we\u2019ve defined, there is extensive documentation in each of the files that explains each method\u2019s purpose.</p> <pre><code># define an instance of our simple linear baseline model\nmodel = SimpleModel(\n    input_dim=10,\n    output_dim=10,\n    lr=1e-2,\n)\n\n# define an instance of the synthetic data loader\n# see the constructor for the full list of params and their explanations\ndata_module = SyntheticDataLoader(\n    batch_size=32,\n    num_genes=3000,\n    bound=[0.5] * 5,\n    n_sample=[1, 1, 2, 2, 4],\n    val_size=0.1,\n    test_size=0.1,\n    bound_mean=3.0,\n)\n\n# define a trainer instance\ntrainer = Trainer(\n    max_epochs=10,\n    deterministic=True,\n    accelerator=\"cpu\", # change to \"gpu\" if you have access to one\n)\n\n# train the model\ntrainer.fit(model, data_module)\n\n# test the model (recall that data_module specifies the train / test split, we don't need to do it explicitly here)\ntest_results = trainer.test(model, data_module)\nprint(test_results)\n\n</code></pre> <pre>\n<code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\nMissing logger folder: /Users/ericjia/yeastdnnexplorer/docs/tutorials/lightning_logs\n/Users/ericjia/yeastdnnexplorer/yeastdnnexplorer/data_loaders/synthetic_data_loader.py:260: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_train, Y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(\n/Users/ericjia/yeastdnnexplorer/yeastdnnexplorer/data_loaders/synthetic_data_loader.py:263: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_val, Y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(\n/Users/ericjia/yeastdnnexplorer/yeastdnnexplorer/data_loaders/synthetic_data_loader.py:266: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_test, Y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(\n\n  | Name    | Type              | Params\n----------------------------------------------\n0 | mae     | MeanAbsoluteError | 0     \n1 | SMSE    | SMSE              | 0     \n2 | linear1 | Linear            | 110   \n----------------------------------------------\n110       Trainable params\n0         Non-trainable params\n110       Total params\n0.000     Total estimated model params size (MB)\n</code>\n</pre> <pre>\n<code>Sanity Checking: |                                                                                            \u2026</code>\n</pre> <pre>\n<code>/Users/ericjia/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n</code>\n</pre> <pre>\n<code>Training: |                                                                                                   \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>`Trainer.fit` stopped: `max_epochs=10` reached.\n</code>\n</pre> <pre>\n<code>Testing: |                                                                                                    \u2026</code>\n</pre> <pre>\n<code>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        test_mae            1.1637259721755981\n        test_mse            1.8661913871765137\n        test_smse           10.101052284240723\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n[{'test_mse': 1.8661913871765137, 'test_mae': 1.1637259721755981, 'test_smse': 10.101052284240723}]\n</code>\n</pre> <p>It\u2019s very easy to train the same model on a different dataset, for example if we want to use real world data we can just swap to the data module that we\u2019ve defined for the real world data.</p> <pre><code># we need to redefine a new instance with the same params unless we want it to pick up where it left off\nnew_model = SimpleModel(\n    input_dim=30,  # note that the input and output dims are equal to the num TFs in the dataset\n    output_dim=30,\n    lr=1e-2,\n)\n\nreal_data_module = RealDataLoader(\n    batch_size=32,\n    val_size=0.1,\n    test_size=0.1,\n    data_dir_path=\"../../data/init_analysis_data_20240409/\", # note that this is relative to where real_data_loader.py is\n    perturbation_dataset_title=\"hu_reimann_tfko\",\n)\n\n# we also have to define a new trainer instance, not really sure why but it seems to be necessary\ntrainer = Trainer(\n    max_epochs=10,\n    deterministic=True,\n    accelerator=\"cpu\", # change to \"gpu\" if you have access to one\n)\n\ntrainer.fit(new_model, real_data_module)\ntest_results = trainer.test(new_model, real_data_module)\nprint(test_results)\n</code></pre> <pre>\n<code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n</code>\n</pre> <pre>\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In[3], line 23\n     16 # we also have to define a new trainer instance, not really sure why but it seems to be necessary\n     17 trainer = Trainer(\n     18     max_epochs=10,\n     19     deterministic=True,\n     20     accelerator=\"cpu\", # change to \"gpu\" if you have access to one\n     21 )\n---&gt; 23 trainer.fit(new_model, real_data_module)\n     24 test_results = trainer.test(new_model, real_data_module)\n     25 print(test_results)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:544, in Trainer.fit(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\n    542 self.state.status = TrainerStatus.RUNNING\n    543 self.training = True\n--&gt; 544 call._call_and_handle_interrupt(\n    545     self, self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n    546 )\n\nFile ~/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:44, in _call_and_handle_interrupt(trainer, trainer_fn, *args, **kwargs)\n     42     if trainer.strategy.launcher is not None:\n     43         return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n---&gt; 44     return trainer_fn(*args, **kwargs)\n     46 except _TunerExitException:\n     47     _call_teardown_hook(trainer)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:580, in Trainer._fit_impl(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\n    573 assert self.state.fn is not None\n    574 ckpt_path = self._checkpoint_connector._select_ckpt_path(\n    575     self.state.fn,\n    576     ckpt_path,\n    577     model_provided=True,\n    578     model_connected=self.lightning_module is not None,\n    579 )\n--&gt; 580 self._run(model, ckpt_path=ckpt_path)\n    582 assert self.state.stopped\n    583 self.training = False\n\nFile ~/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:947, in Trainer._run(self, model, ckpt_path)\n    944 self.__setup_profiler()\n    946 log.debug(f\"{self.__class__.__name__}: preparing data\")\n--&gt; 947 self._data_connector.prepare_data()\n    949 call._call_setup_hook(self)  # allow user to set up LightningModule in accelerator environment\n    950 log.debug(f\"{self.__class__.__name__}: configuring model\")\n\nFile ~/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:94, in _DataConnector.prepare_data(self)\n     92     dm_prepare_data_per_node = datamodule.prepare_data_per_node\n     93     if (dm_prepare_data_per_node and local_rank_zero) or (not dm_prepare_data_per_node and global_rank_zero):\n---&gt; 94         call._call_lightning_datamodule_hook(trainer, \"prepare_data\")\n     95 # handle lightning module prepare data:\n     96 # check for prepare_data_per_node before calling lightning_module.prepare_data\n     97 if lightning_module is not None:\n\nFile ~/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:179, in _call_lightning_datamodule_hook(trainer, hook_name, *args, **kwargs)\n    177 if callable(fn):\n    178     with trainer.profiler.profile(f\"[LightningDataModule]{trainer.datamodule.__class__.__name__}.{hook_name}\"):\n--&gt; 179         return fn(*args, **kwargs)\n    180 return None\n\nFile ~/yeastdnnexplorer/yeastdnnexplorer/data_loaders/real_data_loader.py:118, in RealDataLoader.prepare_data(self)\n    106 \"\"\"\n    107 This function reads in the binding data and perturbation data from the CSV files\n    108 that we have for these datasets.\n   (...)\n    113 \n    114 \"\"\"\n    116 brent_cc_path = os.path.join(self.data_dir_path, \"binding/brent_nf_cc\")\n    117 brent_nf_csv_files = [\n--&gt; 118     f for f in os.listdir(brent_cc_path) if f.endswith(\".csv\")\n    119 ]\n    120 perturb_dataset_path = os.path.join(\n    121     self.data_dir_path, f\"perturbation/{self.perturbation_dataset_title}\"\n    122 )\n    123 perturb_dataset_csv_files = [\n    124     f for f in os.listdir(perturb_dataset_path) if f.endswith(\".csv\")\n    125 ]\n\nFileNotFoundError: [Errno 2] No such file or directory: '../../data/init_analysis_data_20240409/binding/brent_nf_cc'</pre> <p>If we wanted to do the same thing with our more complex and customizable <code>CustomizableModel</code> (which allows you to pass in many params like the number of hidden layers, dropout rate, choice of optimizer, etc) the code would look identical to above except that we would be initializing a <code>CustomizableModel</code> instead of a <code>SimpleModel</code>. See the documentation in <code>customizable_model.py</code> for more</p> <pre><code># this will be used to save the model checkpoint that performs the best on the validation set\nbest_model_checkpoint = ModelCheckpoint(\n    monitor=\"val_mse\", # we can depend on any metric we want\n    mode=\"min\",\n    filename=\"best-model-{epoch:02d}-{val_loss:.2f}\",\n    save_top_k=1, # we can save more than just the top model if we want\n)\n\n# Callback to save checkpoints every 2 epochs, regardless of model performance\nperiodic_checkpoint = ModelCheckpoint(\n    filename=\"periodic-{epoch:02d}\",\n    every_n_epochs=2,\n    save_top_k=-1,  # Setting -1 saves all checkpoints  \n)\n\n# csv logger is a very basic logger that will create a csv file with our metrics as we train\ncsv_logger = CSVLogger(\"logs/csv_logs\")  # we define the directory we want the logs to be saved in\n\n# tensorboard logger is a more advanced logger that will create a directory with a bunch of files that can be visualized with tensorboard\n# tensorboard is a library that can be ran via the command line, and will create a local server that can be accessed via a web browser\n# that displays the training metrics in a more interactive way (on a dashboard)\n# you can run tensorboard by running the command `tensorboard --logdir=path/to/log/dir` in the terminal\ntb_logger = TensorBoardLogger(\"logs/tensorboard_logs\", name=\"test-run-2\")\n\n# If we wanted to use these checkpoints and loggers, we would pass them to the trainer like so:\ntrainer_with_checkpoints_and_loggers = Trainer(\n    max_epochs=10,\n    deterministic=True,\n    accelerator=\"cpu\",\n    callbacks=[best_model_checkpoint, periodic_checkpoint],\n    logger=[csv_logger, tb_logger],\n)\n</code></pre> <pre>\n<code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n</code>\n</pre> <pre><code># Load a model from a checkpoint\n# We can load a model from a checkpoint like so:\npath_to_checkpoint = \"example/path/not/real.ckpt\"\n\n# note that we need to use the same model class that was used to save the checkpoint\nmodel = SimpleModel.load_from_checkpoint(path_to_checkpoint)\n\n# we can load the model and continue training from where it left off\ntrainer.fit(model, data_module)\n\n# we could also load the model and test it\ntest_results = trainer.test(model, data_module)\n\n# we could also load the model and make predictions\npredictions = model(data_module.test_dataloader())\n</code></pre> <pre>\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In[5], line 6\n      3 path_to_checkpoint = \"example/path/not/real.ckpt\"\n      5 # note that we need to use the same model class that was used to save the checkpoint\n----&gt; 6 model = SimpleModel.load_from_checkpoint(path_to_checkpoint)\n      8 # we can load the model and continue training from where it left off\n      9 trainer.fit(model, data_module)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/pytorch_lightning/utilities/model_helpers.py:125, in _restricted_classmethod_impl.__get__.&lt;locals&gt;.wrapper(*args, **kwargs)\n    120 if instance is not None and not is_scripting:\n    121     raise TypeError(\n    122         f\"The classmethod `{cls.__name__}.{self.method.__name__}` cannot be called on an instance.\"\n    123         \" Please call it on the class type and make sure the return value is used.\"\n    124     )\n--&gt; 125 return self.method(cls, *args, **kwargs)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/pytorch_lightning/core/module.py:1581, in LightningModule.load_from_checkpoint(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\n   1492 @_restricted_classmethod\n   1493 def load_from_checkpoint(\n   1494     cls,\n   (...)\n   1499     **kwargs: Any,\n   1500 ) -&gt; Self:\n   1501     r\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\n   1502     passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\n   1503 \n   (...)\n   1579 \n   1580     \"\"\"\n-&gt; 1581     loaded = _load_from_checkpoint(\n   1582         cls,  # type: ignore[arg-type]\n   1583         checkpoint_path,\n   1584         map_location,\n   1585         hparams_file,\n   1586         strict,\n   1587         **kwargs,\n   1588     )\n   1589     return cast(Self, loaded)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/pytorch_lightning/core/saving.py:63, in _load_from_checkpoint(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\n     61 map_location = map_location or _default_map_location\n     62 with pl_legacy_patch():\n---&gt; 63     checkpoint = pl_load(checkpoint_path, map_location=map_location)\n     65 # convert legacy checkpoints to the new format\n     66 checkpoint = _pl_migrate_checkpoint(\n     67     checkpoint, checkpoint_path=(checkpoint_path if isinstance(checkpoint_path, (str, Path)) else None)\n     68 )\n\nFile ~/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/lightning_fabric/utilities/cloud_io.py:56, in _load(path_or_url, map_location)\n     51     return torch.hub.load_state_dict_from_url(\n     52         str(path_or_url),\n     53         map_location=map_location,  # type: ignore[arg-type]\n     54     )\n     55 fs = get_filesystem(path_or_url)\n---&gt; 56 with fs.open(path_or_url, \"rb\") as f:\n     57     return torch.load(f, map_location=map_location)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/fsspec/spec.py:1298, in AbstractFileSystem.open(self, path, mode, block_size, cache_options, compression, **kwargs)\n   1296 else:\n   1297     ac = kwargs.pop(\"autocommit\", not self._intrans)\n-&gt; 1298     f = self._open(\n   1299         path,\n   1300         mode=mode,\n   1301         block_size=block_size,\n   1302         autocommit=ac,\n   1303         cache_options=cache_options,\n   1304         **kwargs,\n   1305     )\n   1306     if compression is not None:\n   1307         from fsspec.compression import compr\n\nFile ~/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/fsspec/implementations/local.py:191, in LocalFileSystem._open(self, path, mode, block_size, **kwargs)\n    189 if self.auto_mkdir and \"w\" in mode:\n    190     self.makedirs(self._parent(path), exist_ok=True)\n--&gt; 191 return LocalFileOpener(path, mode, fs=self, **kwargs)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/fsspec/implementations/local.py:355, in LocalFileOpener.__init__(self, path, mode, autocommit, fs, compression, **kwargs)\n    353 self.compression = get_compression(path, compression)\n    354 self.blocksize = io.DEFAULT_BUFFER_SIZE\n--&gt; 355 self._open()\n\nFile ~/Library/Caches/pypoetry/virtualenvs/yeastdnnexplorer-iu4_cpc2-py3.11/lib/python3.11/site-packages/fsspec/implementations/local.py:360, in LocalFileOpener._open(self)\n    358 if self.f is None or self.f.closed:\n    359     if self.autocommit or \"w\" not in self.mode:\n--&gt; 360         self.f = open(self.path, mode=self.mode)\n    361         if self.compression:\n    362             compress = compr[self.compression]\n\nFileNotFoundError: [Errno 2] No such file or directory: '/Users/ericjia/yeastdnnexplorer/docs/tutorials/example/path/not/real.ckpt'</pre> <pre><code>\n</code></pre>"},{"location":"tutorials/lightning_crash_course/#lightning-crash-course","title":"Lightning Crash Course","text":"<p>This project uses the PyTorch Lightning Library to define and train the machine learning models. PyTorch Lightning is built on top of pytorch, and it abstracts away some of the setup and biolerplate for models (such as writing out training loops). In this notebook, we provide a brief introduction to how to use the models and dataModules we\u2019ve defined to train models.</p>"},{"location":"tutorials/lightning_crash_course/#checkpointing-logging","title":"Checkpointing &amp; Logging","text":"<p>PyTorch lightning gives us the power to define checkpoints and loggers that will be used during training. Checkpoints will save checkpoints of your model during training. In the following code, we define a checkpoint that saves the model\u2019s state when it produced the lowest validation mean squared error on the validation set during training. We also define another checkpoint to periodically save a checkpoint of the model after every 2 training epochs. These checkpoints are powerful because they can be reloaded later. You can continue training a model after loading its checkpoint or you can test the model checkpoint on new data.</p> <p>Loggers are responsible for saving metrics about the model as it is training for us to look at later. We define several loggers to track this data. See the comments above the Tensorboard logger to see how to use Tensorboard to visualize the metrics as the model trains</p> <p>To use checkpoints and loggers, we have to pass them into the Trainer object that we use to train the model with a dataModule. </p> <p>There are many more types of checkpoints and loggers you can create and use, PyTorch Lightning\u2019s documentation is very helpful here</p>"},{"location":"tutorials/lightning_crash_course/#loading-in-and-using-a-checkpoint","title":"Loading in and using a Checkpoint","text":""},{"location":"tutorials/testing_model_metrics/","title":"Testing Model Metrics","text":"<p>In this notebook, we run several simple experiments to gain a deeper understanding of the metrics that we use to evaluate our models and how they respond to changes in the parameters we use for generating our in silico data.</p> <pre><code># imports\nimport torch\n\nfrom pytorch_lightning import Trainer, LightningModule\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\n\nfrom yeastdnnexplorer.data_loaders.synthetic_data_loader import SyntheticDataLoader\nfrom yeastdnnexplorer.ml_models.simple_model import SimpleModel\n\nimport matplotlib.pyplot as plt\n\ntorch.manual_seed(42)  # For CPU\ntorch.cuda.manual_seed_all(42)  # For all CUDA devices\n</code></pre> <p>Define checkpoints and loggers for the models</p> <pre><code># define checkpoints for the model\n# tells it when to save snapshots of the model during training\n# Callback to save the best model based on validation loss\nbest_model_checkpoint = ModelCheckpoint(\n    monitor=\"val_mse\",\n    mode=\"min\",\n    filename=\"best-model-{epoch:02d}-{val_loss:.2f}\",\n    save_top_k=1,\n)\n\n# Callback to save checkpoints every 5 epochs, regardless of performance\nperiodic_checkpoint = ModelCheckpoint(\n    filename=\"periodic-{epoch:02d}\",\n    every_n_epochs=2,\n    save_top_k=-1,  # Setting -1 saves all checkpoints\n)\n\n# configure loggers\ntb_logger = TensorBoardLogger(\"logs/tensorboard_logs\")\ncsv_logger = CSVLogger(\"logs/csv_logs\")\n</code></pre> <p>Here we define a helper function that will generate data and train a simple linear model with all of the given parameters, print the test results of the model, and return the trained model and its test results. This will allow us to easily run experiments where we compare model performance while tweaking data or model parameters.</p> <pre><code>def train_simple_model_with_params(\n    batch_size: int,\n    lr: float,\n    max_epochs: int,\n    using_random_seed: bool,\n    accelerator: str,\n    num_genes: int,\n    bound_mean: float,\n    val_size: float,\n    test_size: float,\n    bound: list[float],\n    n_sample: list[int],\n    max_mean_adjustment: float,\n) -&amp;gt; LightningModule:\n    data_module = SyntheticDataLoader(\n        batch_size=batch_size,\n        num_genes=num_genes,\n        bound_mean=bound_mean,\n        bound=bound,  # old: [0.1, 0.15, 0.2, 0.25, 0.3],\n        n_sample=n_sample,  # sum of this is num of tfs\n        val_size=val_size,\n        test_size=test_size,\n        random_state=42,\n        max_mean_adjustment=max_mean_adjustment,\n    )\n\n    num_tfs = sum(data_module.n_sample)  # sum of all n_sample is the number of TFs\n\n    model = SimpleModel(input_dim=num_tfs, output_dim=num_tfs, lr=lr)\n    trainer = Trainer(\n        max_epochs=max_epochs,\n        deterministic=using_random_seed,\n        accelerator=accelerator,\n        # callbacks=[best_model_checkpoint, periodic_checkpoint],\n        # logger=[tb_logger, csv_logger],\n    )\n    trainer.fit(model, data_module)\n    test_results = trainer.test(model, datamodule=data_module)\n    print(\"Printing test results...\")\n    print(\n        test_results\n    )  # this prints all metrics that were logged during the test phase\n\n    return model, test_results\n</code></pre> <pre><code>bound_means = [0.5, 1.0, 2.0, 3.0, 5.0]\ntest_mses = []\nfor bound_mean in bound_means:\n    model, test_results = train_simple_model_with_params(\n        batch_size=32,\n        lr=0.01,\n        max_epochs=10,\n        using_random_seed=True,\n        accelerator=\"cpu\",\n        num_genes=1000,\n        val_size=0.1,\n        test_size=0.1,\n        bound=[0.5] * 5,\n        n_sample=[1, 1, 2, 2, 4],  # sum of this is num of tfs\n        bound_mean=bound_mean,\n        max_mean_adjustment=0.0\n    )\n    test_mses.append(test_results[0][\"test_mse\"])\n</code></pre> <pre>\n<code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n  | Name    | Type              | Params\n----------------------------------------------\n0 | mae     | MeanAbsoluteError | 0     \n1 | SMSE    | SMSE              | 0     \n2 | linear1 | Linear            | 110   \n----------------------------------------------\n110       Trainable params\n0         Non-trainable params\n110       Total params\n0.000     Total estimated model params size (MB)\n</code>\n</pre> <pre>\n<code>Sanity Checking: |                                                                                            \u2026</code>\n</pre> <pre>\n<code>Training: |                                                                                                   \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>`Trainer.fit` stopped: `max_epochs=10` reached.\n</code>\n</pre> <pre>\n<code>Testing: |                                                                                                    \u2026</code>\n</pre> <pre>\n<code>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        test_mae            0.5135628581047058\n        test_mse             0.416797935962677\n        test_smse           10.241324424743652\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</code>\n</pre> <pre>\n<code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n  | Name    | Type              | Params\n----------------------------------------------\n0 | mae     | MeanAbsoluteError | 0     \n1 | SMSE    | SMSE              | 0     \n2 | linear1 | Linear            | 110   \n----------------------------------------------\n110       Trainable params\n0         Non-trainable params\n110       Total params\n0.000     Total estimated model params size (MB)\n</code>\n</pre> <pre>\n<code>Printing test results...\n[{'test_mse': 0.416797935962677, 'test_mae': 0.5135628581047058, 'test_smse': 10.241324424743652}]\n</code>\n</pre> <pre>\n<code>Sanity Checking: |                                                                                            \u2026</code>\n</pre> <pre>\n<code>Training: |                                                                                                   \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>`Trainer.fit` stopped: `max_epochs=10` reached.\n</code>\n</pre> <pre>\n<code>Testing: |                                                                                                    \u2026</code>\n</pre> <pre>\n<code>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        test_mae            0.5821905136108398\n        test_mse            0.5283595323562622\n        test_smse           10.348736763000488\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</code>\n</pre> <pre>\n<code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n  | Name    | Type              | Params\n----------------------------------------------\n0 | mae     | MeanAbsoluteError | 0     \n1 | SMSE    | SMSE              | 0     \n2 | linear1 | Linear            | 110   \n----------------------------------------------\n110       Trainable params\n0         Non-trainable params\n110       Total params\n0.000     Total estimated model params size (MB)\n</code>\n</pre> <pre>\n<code>Printing test results...\n[{'test_mse': 0.5283595323562622, 'test_mae': 0.5821905136108398, 'test_smse': 10.348736763000488}]\n</code>\n</pre> <pre>\n<code>Sanity Checking: |                                                                                            \u2026</code>\n</pre> <pre>\n<code>Training: |                                                                                                   \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>`Trainer.fit` stopped: `max_epochs=10` reached.\n</code>\n</pre> <pre>\n<code>Testing: |                                                                                                    \u2026</code>\n</pre> <pre>\n<code>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        test_mae            0.8307084441184998\n        test_mse             1.050934910774231\n        test_smse           10.213595390319824\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</code>\n</pre> <pre>\n<code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n</code>\n</pre> <pre>\n<code>Printing test results...\n[{'test_mse': 1.050934910774231, 'test_mae': 0.8307084441184998, 'test_smse': 10.213595390319824}]\n</code>\n</pre> <pre>\n<code>\n  | Name    | Type              | Params\n----------------------------------------------\n0 | mae     | MeanAbsoluteError | 0     \n1 | SMSE    | SMSE              | 0     \n2 | linear1 | Linear            | 110   \n----------------------------------------------\n110       Trainable params\n0         Non-trainable params\n110       Total params\n0.000     Total estimated model params size (MB)\n</code>\n</pre> <pre>\n<code>Sanity Checking: |                                                                                            \u2026</code>\n</pre> <pre>\n<code>Training: |                                                                                                   \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>`Trainer.fit` stopped: `max_epochs=10` reached.\n</code>\n</pre> <pre>\n<code>Testing: |                                                                                                    \u2026</code>\n</pre> <pre>\n<code>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        test_mae            1.1858488321304321\n        test_mse             2.014770984649658\n        test_smse           10.195466995239258\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</code>\n</pre> <pre>\n<code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n  | Name    | Type              | Params\n----------------------------------------------\n0 | mae     | MeanAbsoluteError | 0     \n1 | SMSE    | SMSE              | 0     \n2 | linear1 | Linear            | 110   \n----------------------------------------------\n110       Trainable params\n0         Non-trainable params\n110       Total params\n0.000     Total estimated model params size (MB)\n</code>\n</pre> <pre>\n<code>Printing test results...\n[{'test_mse': 2.014770984649658, 'test_mae': 1.1858488321304321, 'test_smse': 10.195466995239258}]\n</code>\n</pre> <pre>\n<code>Sanity Checking: |                                                                                            \u2026</code>\n</pre> <pre>\n<code>Training: |                                                                                                   \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u2026</code>\n</pre> <pre>\n<code>`Trainer.fit` stopped: `max_epochs=10` reached.\n</code>\n</pre> <pre>\n<code>Testing: |                                                                                                    \u2026</code>\n</pre> <pre>\n<code>\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        test_mae             2.091959238052368\n        test_mse              6.157958984375\n        test_smse           11.987293243408203\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nPrinting test results...\n[{'test_mse': 6.157958984375, 'test_mae': 2.091959238052368, 'test_smse': 11.987293243408203}]\n</code>\n</pre> <p>Plot Results</p> <pre><code>plt.plot(bound_means, test_mses, marker=\"o\")\nplt.xlabel(\"bound Mean\")\nplt.xticks(bound_means, rotation=45)\nplt.yticks(test_mses)\nplt.ylabel(\"Test MSE\")\nplt.title(\"Test MSE as a function of bound Mean\")\nplt.show()\n</code></pre> <pre><code>bound_unbound_ratios = [0.05, 0.1, 0.25, 0.5, 0.75, 0.9]\ntest_mses = []\n\nfor bound_unbound_ratio in bound_unbound_ratios:\n    model, test_results = train_simple_model_with_params(\n        batch_size=32,\n        lr=0.01,\n        max_epochs=10,\n        using_random_seed=True,\n        accelerator=\"cpu\",\n        num_genes=1000,\n        val_size=0.1,\n        test_size=0.1,\n        bound=[bound_unbound_ratio] * 5,\n        n_sample=[1, 1, 2, 2, 4],\n        bound_mean=3.0,\n        max_mean_adjustment=0.0\n    )\n    print(test_results)\n    test_mses.append(test_results[0][\"test_mse\"])\n</code></pre> <pre><code>plt.plot(bound_unbound_ratios, test_mses, marker=\"o\")\nplt.xlabel(\"Percentage of Data in bound Group\")\nplt.ylabel(\"Test MSE\")\nplt.xticks(bound_unbound_ratios, rotation=45)\nplt.yticks(test_mses)\nplt.title(\"Test MSE as a function of bound/unbound ratio (bound mean = 3.0)\")\nplt.show()\n</code></pre> <pre><code># these params will be consistent for both datasets\nnum_genes = 3000\nval_size = 0.1\ntest_size = 0.1\nbound = [0.5] * 5\nn_sample = [1, 1, 2, 2, 4]\nrandom_state = 42\n\n# the first data loader will load a dataset with a small scale and a small bound mean\nsmall_scale_and_mean_dataloader = SyntheticDataLoader(\n    num_genes=num_genes,\n    bound=bound, \n    n_sample=n_sample,\n    val_size=val_size,\n    test_size=test_size,\n    random_state=random_state,\n    bound_mean=1.0,\n    max_mean_adjustment=1.0\n)\n\n# the second data loader will generate a dataset with a large scale and a large bound mean\nlarge_scale_and_mean_dataloader = SyntheticDataLoader(\n    num_genes=num_genes,\n    bound=bound, \n    n_sample=n_sample,\n    val_size=val_size,\n    test_size=test_size,\n    random_state=random_state,\n    bound_mean=10.0,\n    max_mean_adjustment=10.0\n)\n\nnum_tfs = sum(n_sample)  # sum of all n_sample is the number of TFs\n\nmodel = SimpleModel(input_dim=num_tfs, output_dim=num_tfs, lr=0.01)\ntrainer = Trainer(\n    max_epochs=10,\n    deterministic=True,\n    accelerator='cpu',\n    # callbacks=[best_model_checkpoint, periodic_checkpoint],\n    # logger=[tb_logger, csv_logger],\n)\n\ntrainer.fit(model, small_scale_and_mean_dataloader)\nsmall_test_results = trainer.test(model, datamodule=small_scale_and_mean_dataloader)\nprint(\"Printing small test results...\")\nprint(small_test_results)\n\n\ntrainer.fit(model, large_scale_and_mean_dataloader)\nlarge_test_results = trainer.test(model, datamodule=large_scale_and_mean_dataloader)\nprint(\"Printing large test results...\")\nprint(large_test_results)\n</code></pre>"},{"location":"tutorials/testing_model_metrics/#experiment-1","title":"Experiment 1","text":"<p>Now we can use this function to run simple experiments, like testing how the model\u2019s test mse changes when we tweak the mean of the bound genes while holding all other parameters the same. For simplicity, we will not be performing any mean adjustments while generating the data, but we could modify this in the future by incresing the max_mean_adjustment (to use a normal mean adjustment) or adding onto our experiment function to take in our special mean adjustment functions (to use either of the special dependent mean adjustment logic that we\u2019ve defined, see <code>generate_in_silico_data.ipynb</code> for more about this).</p> <p>Note that this will create a lot of output since we are training several models, so we create the plot in a separate cell.</p>"},{"location":"tutorials/testing_model_metrics/#experiment-2","title":"Experiment 2","text":"<p>We can run a similar experiment where we test the effect of the bound / unbound ratio (aka bound / unbound ratio) on the model\u2019s MSE</p>"},{"location":"tutorials/testing_model_metrics/#experiment-3","title":"Experiment 3","text":"<p>Here we run a little experiment to verify that our smse (standardized mean squared error) metric is actually scale and mean invariant (ie doesn\u2019t depend on the scale or mean of the data so long as the variance is roughly the same). Note that this isn\u2019t a perfect experiment, as increasing the max mean adjustment (and therefore the scale) will increase the variance by a factor as a result of how our in silico data generation functions work, so there will definitely be a little difference in smse values, but the difference in mse and mae should be a much larger percentage.</p> <p>We will train and test two models that are exactly the same except that one is trained on a dataset with a small bound mean and mean adjustment and one is trained on a dataset with a large bound mean adn mean adjustment. This will give the two datasets drastically different scales and means. Unfortunately, it will also give them slightly different variances which should cause a slight difference in smse. But again it should be a much smaller percentage difference than the difference between mses and maes</p>"},{"location":"tutorials/visualizing_and_testing_data_generation_methods/","title":"Visualizing and Testing Data Generation Methods","text":"<pre><code># imports\nfrom yeastdnnexplorer.probability_models.generate_data import (\n    generate_gene_population, \n    generate_binding_effects, \n    generate_pvalues, \n    generate_perturbation_effects\n)\n\nimport torch\nimport matplotlib.pyplot as plt\n\nfrom yeastdnnexplorer.probability_models.relation_classes import Relation, And, Or\nfrom yeastdnnexplorer.probability_models.generate_data import (\n    default_perturbation_effect_adjustment_function,\n    perturbation_effect_adjustment_function_with_tf_relationships,\n    perturbation_effect_adjustment_function_with_tf_relationships_boolean_logic\n)\n\nfrom pytorch_lightning import Trainer, seed_everything\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.metrics import explained_variance_score\n\nfrom yeastdnnexplorer.data_loaders.synthetic_data_loader import SyntheticDataLoader\nfrom yeastdnnexplorer.ml_models.simple_model import SimpleModel\nfrom yeastdnnexplorer.ml_models.customizable_model import CustomizableModel\nfrom typing import Tuple, List, Dict, Union\n\nseed_everything(42)\n</code></pre> <pre>\n<code>Seed set to 42\n</code>\n</pre> <pre>\n<code>42</code>\n</pre> <p>Generating the binding data will be the same as always, see <code>generate_in_silico_data.ipynb</code></p> <pre><code>n_genes = 3000\nbound = [0.5, 0.5, 0.5, 0.5, 0.5]\nn_sample = [1, 1, 2, 2, 4]\n\n# Generate gene populations\ngene_populations_list = []\nfor bound_proportion, n_draws in zip(bound, n_sample):\n    for _ in range(n_draws):\n        gene_populations_list.append(generate_gene_population(n_genes, bound_proportion))\n\n# Generate binding data for each gene population\nbinding_effect_list = [generate_binding_effects(gene_population) for gene_population in gene_populations_list]\n\n# Calculate p-values for binding data\nbinding_pvalue_list = [generate_pvalues(binding_data) for binding_data in binding_effect_list]\n\n# Combine binding data into a tensor\nbinding_data_combined = [torch.stack((gene_population.labels, binding_effect, binding_pval), dim=1)\n                         for gene_population, binding_effect, binding_pval in zip(gene_populations_list, binding_effect_list, binding_pvalue_list)]\nbinding_data_tensor = torch.stack(binding_data_combined, dim=1)\n</code></pre> <p>Now we define our experiment, this function will return the average perturbation effects (across n_iterations iterations) for each TF for a specific gene for each of the 4 data generation method we have at our disposal. Due to the randomness in the generated data, we need to find the averages over a number of iterations to get the true common values.</p> <p>We also need to define dictionaries of TF relationships for our third and fourth methods of generating perturbation data, see generate_in_silico_data.ipynb for an explanation of what these represent and how they are used / structured. The documentation in generate_data.py may be helpful as well.</p> <pre><code># TF relationships\ntf_relationships_dict = {\n    0: [1],\n    1: [8],\n    2: [5, 6],\n    3: [4],\n    4: [5],\n    5: [9],\n    6: [4],\n    7: [1, 4],\n    8: [6],\n    9: [4],\n}\n\n# TF relationships that incorporate boolean logic; this is more complex than\n# the simple relationships above  as it implements \"and\" and \"or\" operations\ntf_relationships_dict_boolean_logic = {\n    0: [And(3, 4, 8), Or(3, 7), Or(1, 1)],\n    1: [And(5, Or(7, 8))],\n    2: [],\n    3: [Or(7, 9), And(6, 7)],\n    4: [And(1, 2)],\n    5: [Or(0, 1, 2, 8, 9)],\n    6: [And(0, Or(1, 2))],\n    7: [Or(2, And(5, 6, 9))],\n    8: [],\n    9: [And(6, And(3, Or(0, 9)))],\n}\n\ndef experiment(n_iterations: int = 10, GENE_IDX: int = 0) -&amp;gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Conducts an experiment by generating perturbation effects for a specific gene over multiple iterations\n    using different methods and averaging the results.\n\n    :param n_iterations: Number of iterations to perform.\n    :type n_iterations: int\n    :param GENE_IDX: Index of the gene to analyze.\n    :type GENE_IDX: int\n\n    :returns: A tuple containing averaged perturbation effects scores for each method.\n    :rtype: Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]\n    \"\"\"\n    print(\"Bound (1) and Unbound (0) Labels for gene \" + str(GENE_IDX) + \":\")\n    print(binding_data_tensor[GENE_IDX, :, 0])\n\n    num_tfs = sum(n_sample)\n\n    no_mean_adjustment_scores = torch.zeros(num_tfs)\n    normal_mean_adjustment_scores = torch.zeros(num_tfs)\n    dep_mean_adjustment_scores = torch.zeros(num_tfs)\n    boolean_logic_scores = torch.zeros(num_tfs)\n\n    for i in range(n_iterations):\n        # Method 1: Generate perturbation effects without mean adjustment\n        perturbation_effects_list_no_mean_adjustment = [generate_perturbation_effects(binding_data_tensor[:, tf_index, :].unsqueeze(1), tf_index=0) \n                                                        for tf_index in range(num_tfs)]\n        perturbation_effects_list_no_mean_adjustment = torch.stack(perturbation_effects_list_no_mean_adjustment, dim=1)\n\n        # Method 2: Generate perturbation effects with normal mean adjustment\n        perturbation_effects_list_normal_mean_adjustment = generate_perturbation_effects(\n            binding_data_tensor, \n            max_mean_adjustment=10.0\n        )\n\n        # Method 3: Generate perturbation effects with dependent mean adjustment\n        perturbation_effects_list_dep_mean_adjustment = generate_perturbation_effects(\n            binding_data_tensor, \n            tf_relationships=tf_relationships,\n            adjustment_function=perturbation_effect_adjustment_function_with_tf_relationships,\n            max_mean_adjustment=10.0,\n        )\n\n        # Method 4: Generate perturbation effects with binary relations between the TFs\n        perturbation_effects_list_boolean_logic = generate_perturbation_effects(\n            binding_data_tensor, \n            adjustment_function=perturbation_effect_adjustment_function_with_tf_relationships_boolean_logic,\n            tf_relationships=tf_relationships_dict_boolean_logic,\n            max_mean_adjustment=10.0,\n        )\n\n        no_mean_adjustment_scores += abs(perturbation_effects_list_no_mean_adjustment[GENE_IDX, :])\n        normal_mean_adjustment_scores += abs(perturbation_effects_list_normal_mean_adjustment[GENE_IDX, :])\n        dep_mean_adjustment_scores += abs(perturbation_effects_list_dep_mean_adjustment[GENE_IDX, :])\n        boolean_logic_scores += abs(perturbation_effects_list_boolean_logic[GENE_IDX, :])\n\n        if (i + 1) % 5 == 0:\n            print(f\"iteration {i+1} completed\")\n\n    no_mean_adjustment_scores /= n_iterations\n    normal_mean_adjustment_scores /= n_iterations\n    dep_mean_adjustment_scores /= n_iterations\n    boolean_logic_scores /= n_iterations\n\n    return no_mean_adjustment_scores, normal_mean_adjustment_scores, dep_mean_adjustment_scores, boolean_logic_scores\n</code></pre> <pre><code>GENE_IDX = 0\nexperiment_results = experiment(n_iterations=50, GENE_IDX=GENE_IDX)\n</code></pre> <pre>\n<code>Bound (1) and Unbound (0) Labels for gene 0:\ntensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 1.])\niteration 5 completed\niteration 10 completed\niteration 15 completed\niteration 20 completed\niteration 25 completed\niteration 30 completed\niteration 35 completed\niteration 40 completed\niteration 45 completed\niteration 50 completed\n</code>\n</pre> <p>Now we plot our results.</p> <pre><code>x_vals = list(range(sum(n_sample)))\nprint(\"Bound (bound) TFs for gene \" + str(GENE_IDX) + \" are: \" + str(binding_data_tensor[GENE_IDX, :, 0].nonzero().flatten().tolist()))\nprint(\"Unbound (unbound) TFs for gene \" + str(GENE_IDX) + \" are: \" + str((1 - binding_data_tensor[GENE_IDX, :, 0]).nonzero().flatten().tolist()))\nprint(binding_data_tensor[GENE_IDX, :, 0])\nplt.figure(figsize=(10, 6))\n\n# Plot each set of experiment results with a different color\ncolors = ['red', 'green', 'blue', 'orange']\nfor index, results in enumerate(experiment_results):\n    plt.scatter(x_vals, results, color=colors[index])\n\nplt.title('Pertubation Effects for Gene ' + str(GENE_IDX) + ' with Different Adjustment Functions (averaged across 100 trials)')\nplt.xlabel('TF Index')\nplt.ylabel('Perturbation Effect Val')\n\n#added to compare this to previous graph, REMOVE LATER\nplt.ylim(0,9)\n\n\nplt.xticks(x_vals)\nplt.grid(True)\nplt.legend(['No Mean Adjustment', 'Normal (non-dependent) Mean Adjust', 'Dependent Mean Adjustment', 'Boolean Logic Adjustment'])\nplt.show()\n</code></pre> <pre>\n<code>Bound (bound) TFs for gene 0 are: [3, 4, 5, 6, 7, 9]\nUnbound (unbound) TFs for gene 0 are: [0, 1, 2, 8]\ntensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 1.])\n</code>\n</pre> <p>The x-axis labels represent the corresponding TFs whose perturbation effect values are being plotted on the y-axis. The color of each plotted point indicates which of the four data generation methods it was derived from. For example, based on the legend included in the graph, a red point was generated using no mean adjustment. This graph allows us to visualize the perturbation effects for the same TF under a variety of conditions.</p> <p>Recall that for the dependent mean adjustment, the TF in question must be bound and all of the TFs in its dependency array (in the tf_relationships dictionary) must be bound as well. This is why we do not adjust the mean for TF 7 despite it being bound, it depends on TF 1 and TF 4 both being bound, and TF1 is not bound.</p> <p>Similarly, for the boolean logic adjustment, we do not adjust the mean for 6 despite it being bound because it depends on (TF0 &amp;&amp; (TF1 || TF2)) being bound, and none of those 3 TFs are bound to the gene we are studying.</p> <p>Note that if you change GENE_IDX, the random seed, or any of the relationship dictionaris that this explanation will no longer apply to the data you are seeing in the plot.</p> <pre><code># define checkpoints and loggers\nbest_model_checkpoint = ModelCheckpoint(\n    monitor=\"val_explained_variance\",\n    mode=\"min\",\n    filename=\"best-model-{epoch:02d}-{val_loss:.2f}\",\n    save_top_k=1,\n)\n\n    :param max_mean_adjustment: Maximum mean adjustment value.\n    :type max_mean_adjustment: float\n    :param adjustment_function: Function to adjust perturbation effects.\n    :type adjustment_function: callable\n    :param tf_relationships_dict: Dictionary of transcription factor relationships.\n    :type tf_relationships_dict: Dict[str, Union[List[int], float]]\n\n    :returns: Configured data loader for synthetic data.\n    :rtype: SyntheticDataLoader\n    \"\"\"\n    return SyntheticDataLoader(\n        batch_size=32,\n        num_genes=4000,\n        bound_mean=3.0,\n        bound=[0.5] * 5,\n        n_sample=[1, 1, 2, 2, 4],\n        val_size=0.1,\n        test_size=0.1,\n        random_state=42,\n        max_mean_adjustment=max_mean_adjustment,\n        adjustment_function=adjustment_function,\n        tf_relationships=tf_relationships_dict,\n    )\n\ndef get_model(num_tfs: int) -&amp;gt; CustomizableModel:\n    \"\"\"\n    Creates a customizable model.\n\n    :param num_tfs: Number of transcription factors.\n    :type num_tfs: int\n\n    :returns: Configured model.\n    :rtype: CustomizableModel\n    \"\"\"\n    return CustomizableModel(\n        input_dim=num_tfs,\n        output_dim=num_tfs,\n        lr=0.01,\n        hidden_layer_num=2,\n        hidden_layer_sizes=[64, 32],\n        activation=\"LeakyReLU\",\n        optimizer=\"RMSprop\",\n        L2_regularization_term=0.0,\n        dropout_rate=0.0,\n    )\n\ndef get_linear_model(num_tfs: int) -&amp;gt; SimpleModel:\n    \"\"\"\n    Creates a simple linear model.\n\n    :param num_tfs: Number of transcription factors.\n    :type num_tfs: int\n\n    :returns: Configured linear model.\n    :rtype: SimpleModel\n    \"\"\"\n    return SimpleModel(\n        input_dim=num_tfs,\n        output_dim=num_tfs,\n        lr=0.01\n    )\n\ndef get_trainer() -&amp;gt; Trainer:\n    \"\"\"\n    Creates a trainer for model training.\n\n    :returns: Configured trainer.\n    :rtype: Trainer\n    \"\"\"\n    return Trainer(\n        max_epochs=10,\n        deterministic=True,\n        accelerator=\"cpu\",\n        # The following are turned false to reduce the output in the training cells below. You can toggle them to true to see\n        # a model summary and training progress if desired \n        logger=False, \n        enable_progress_bar=False,  \n        enable_model_summary=False,  \n        enable_checkpointing=False  \n    )\n\ndef calculate_explained_variance( \n    model: torch.nn.Module, data_module: DataLoader\n) -&amp;gt; float:\n    \"\"\"\n    Calculates the explained variance of a model's predictions on a test dataset.\n\n    :param test_results: List of test results containing the expected outcomes.\n    :type test_results: List[Union[float, int]]\n    :param data_module: Data loader for the test dataset.\n    :type data_module: DataLoader\n    :param model: The model to evaluate.\n    :type model: torch.nn.Module\n\n    :returns: The explained variance of the model's predictions.\n    :rtype: float\n    \"\"\"\n    predictions = []\n    targets = []\n\n    model.eval()  # Set the model to evaluation mode\n\n    with torch.no_grad():  # Disable gradient calculation\n        for batch in data_module.test_dataloader():\n            x, y = batch\n            outputs = model(x).cpu().numpy()\n            predictions.extend(outputs)\n            targets.extend(y.cpu().numpy())\n\n    # Use scikit-learn to calculate explained variance\n    if len(targets) &amp;gt; 0:\n        explained_variance = explained_variance_score(targets, predictions)\n        return explained_variance\n    else:\n        return None\n\n# These lists will store the test results for different models and data generation methods\nmodel_ves = []\nlinear_model_test_ves = []\n</code></pre> <pre><code>import torch\nfrom sklearn.metrics import explained_variance_score\n\ndata_module = get_data_module(0.0)\nnum_tfs = sum(data_module.n_sample)\nmodel_ves = []  # List to store explained variance for the non-linear model\nlinear_model_test_ves = [] # List to store explained variance for the linear model\n\ndef calculate_explained_variance(test_results, data_module, model):\n    predictions = []\n    targets = []\n\n    model.eval()  # Set the model to evaluation mode\n\n    with torch.no_grad():  # Disable gradient calculation\n        for batch in data_module.test_dataloader():\n             # Assuming your data is in the format (x, y)\n            x, y = batch\n            outputs = model(x)\n            predictions.append(outputs)\n            targets.append(y)\n    mse = torch.nn.functional.mse_loss(torch.tensor(predictions), torch.tensor(targets)).item()\n    var_y = torch.var(torch.tensor(targets)).item() \n    explained_variance = 1 - (mse / var_y)\n    return explained_variance \n\n# # Function to calculate explained variance from test results\n# def calculate_explained_variance(test_results, data_module, model):\n#     \"\"\"\n#     Calculates the explained variance score using PyTorch and scikit-learn.\n\n#     Args:\n#         test_results: The results dictionary from the trainer.test() function.\n#         data_module: The data module containing the test dataloader.\n#         model: The trained PyTorch model.\n\n#     Returns:\n#         float: The explained variance score.\n#     \"\"\"\n#     predictions = []\n#     targets = []\n\n#     model.eval()  # Set the model to evaluation mode\n\n#     with torch.no_grad():  # Disable gradient calculation\n#         for batch in data_module.test_dataloader():\n#             # Assuming your data is in the format (x, y)\n#             x, y = batch\n#             outputs = model(x)\n#             predictions.append(outputs)\n#             targets.append(y)\n\n#     predictions = torch.cat(predictions, dim=0).numpy()  # Concatenate predictions\n#     targets = torch.cat(targets, dim=0).numpy()  # Concatenate targets\n\n#     return explained_variance_score(targets, predictions)\n</code></pre> <pre><code>import warnings\nimport logging\n\n# Suppress specific warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*torch.tensor.*\")\nwarnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*DataLoader.*\")\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n</code></pre> <pre>\n<code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n/Users/ericjia/yeastdnnexplorer/yeastdnnexplorer/data_loaders/synthetic_data_loader.py:260: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_train, Y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(\n/Users/ericjia/yeastdnnexplorer/yeastdnnexplorer/data_loaders/synthetic_data_loader.py:263: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_val, Y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(\n/Users/ericjia/yeastdnnexplorer/yeastdnnexplorer/data_loaders/synthetic_data_loader.py:266: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_test, Y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(\n\n  | Name          | Type              | Params\n----------------------------------------------------\n0 | activation    | LeakyReLU         | 0     \n1 | input_layer   | Linear            | 704   \n2 | hidden_layers | ModuleList        | 2.1 K \n3 | output_layer  | Linear            | 330   \n4 | dropout       | Dropout           | 0     \n5 | mae           | MeanAbsoluteError | 0     \n6 | SMSE          | SMSE              | 0     \n----------------------------------------------------\n3.1 K     Trainable params\n0         Non-trainable params\n3.1 K     Total params\n0.012     Total estimated model params size (MB)\n</code>\n</pre> <pre>\n<code>Sanity Checking: |                                                                   | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>/Users/ericjia/yeastdnnexplorer/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n</code>\n</pre> <pre>\n<code>Training: |                                                                          | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>`Trainer.fit` stopped: `max_epochs=10` reached.\nGPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n/Users/ericjia/yeastdnnexplorer/yeastdnnexplorer/data_loaders/synthetic_data_loader.py:260: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_train, Y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(\n/Users/ericjia/yeastdnnexplorer/yeastdnnexplorer/data_loaders/synthetic_data_loader.py:263: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_val, Y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(\n/Users/ericjia/yeastdnnexplorer/yeastdnnexplorer/data_loaders/synthetic_data_loader.py:266: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_test, Y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(\n\n  | Name    | Type              | Params\n----------------------------------------------\n0 | mae     | MeanAbsoluteError | 0     \n1 | SMSE    | SMSE              | 0     \n2 | linear1 | Linear            | 110   \n----------------------------------------------\n110       Trainable params\n0         Non-trainable params\n110       Total params\n0.000     Total estimated model params size (MB)\n</code>\n</pre> <pre>\n<code>Nonlinear Model Explained Variance: 0.22300392389297485\n</code>\n</pre> <pre>\n<code>Sanity Checking: |                                                                   | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>/Users/ericjia/yeastdnnexplorer/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n</code>\n</pre> <pre>\n<code>Training: |                                                                          | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>`Trainer.fit` stopped: `max_epochs=10` reached.\n</code>\n</pre> <pre>\n<code>Linear Model Explained Variance: 0.005077654123306274\n</code>\n</pre> <pre><code># Initialize data module\ndata_module = get_data_module(0.0)\nnum_tfs = sum(data_module.n_sample)\n\n# --- Nonlinear Model ---\nmodel = get_model(num_tfs)\ntrainer = get_trainer()\ntrainer.fit(model, data_module)\nexplained_variance = calculate_explained_variance(model, data_module)\nmodel_ves.append(explained_variance)\nprint(\"Nonlinear Model Explained Variance:\", explained_variance)\n\n# --- Linear Model ---\nlinear_model = get_linear_model(num_tfs)\ntrainer = get_trainer()\ntrainer.fit(linear_model, data_module)\nexplained_variance_linear = calculate_explained_variance(linear_model, data_module)\nlinear_model_test_ves.append(explained_variance_linear)\n\nprint(\"Linear Model Explained Variance:\", explained_variance_linear)\n</code></pre> <pre>\n<code>Nonlinear Model Explained Variance: 0.24550879001617432\nLinear Model Explained Variance: -0.00506981611251831\n</code>\n</pre> <p>The explained variance for the linear model is surprisingly sightly negative in contrast to the nonlinear, customizable model which yielded a significantly larger positive explained variance. This suggests that the customizable model is able to better account for the distribution of the generated data with no mean adjustments, yielding a significantly higher explained variance. It is interesting to consider whether the same relationship will be observed in the next few conditions as the data generation methods becoome increasingly more complex. old: The explained variance for the linear model is quite small compared to the complex, customizable model which yielded a significantly larger positive explained variance. This suggests that the customizable model is able to better fit to the generated data in this condition. </p> <pre><code>data_module = get_data_module(3.0)\nnum_tfs = sum(data_module.n_sample)\n\n# Nonlinear model\nmodel = get_model(num_tfs)\ntrainer = get_trainer()\ntrainer.fit(model, data_module)\nexplained_variance = calculate_explained_variance(model, data_module)\nmodel_ves.append(explained_variance)\nprint(\"Nonlinear Model Explained Variance (Method 2):\", explained_variance)\n\n# Linear model\nlinear_model = get_linear_model(num_tfs)\ntrainer = get_trainer()\ntrainer.fit(linear_model, data_module)\nexplained_variance_linear = calculate_explained_variance(linear_model, data_module)\nlinear_model_test_ves.append(explained_variance_linear)\nprint(\"Linear Model Explained Variance (Method 2):\", explained_variance_linear)\n\n</code></pre> <pre>\n<code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n/Users/ericjia/yeastdnnexplorer/yeastdnnexplorer/data_loaders/synthetic_data_loader.py:260: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_train, Y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(\n/Users/ericjia/yeastdnnexplorer/yeastdnnexplorer/data_loaders/synthetic_data_loader.py:263: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_val, Y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(\n/Users/ericjia/yeastdnnexplorer/yeastdnnexplorer/data_loaders/synthetic_data_loader.py:266: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_test, Y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(\n\n  | Name          | Type              | Params\n----------------------------------------------------\n0 | activation    | LeakyReLU         | 0     \n1 | input_layer   | Linear            | 704   \n2 | hidden_layers | ModuleList        | 2.1 K \n3 | output_layer  | Linear            | 330   \n4 | dropout       | Dropout           | 0     \n5 | mae           | MeanAbsoluteError | 0     \n6 | SMSE          | SMSE              | 0     \n----------------------------------------------------\n3.1 K     Trainable params\n0         Non-trainable params\n3.1 K     Total params\n0.012     Total estimated model params size (MB)\n</code>\n</pre> <pre>\n<code>Sanity Checking: |                                                                   | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>/Users/ericjia/yeastdnnexplorer/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n</code>\n</pre> <pre>\n<code>Training: |                                                                          | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>`Trainer.fit` stopped: `max_epochs=10` reached.\nGPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n</code>\n</pre> <pre>\n<code>Nonlinear Model Explained Variance (Method 2): 0.2848140120506287\n</code>\n</pre> <pre>\n<code>/Users/ericjia/yeastdnnexplorer/yeastdnnexplorer/data_loaders/synthetic_data_loader.py:260: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_train, Y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(\n/Users/ericjia/yeastdnnexplorer/yeastdnnexplorer/data_loaders/synthetic_data_loader.py:263: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_val, Y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(\n/Users/ericjia/yeastdnnexplorer/yeastdnnexplorer/data_loaders/synthetic_data_loader.py:266: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_test, Y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(\n\n  | Name    | Type              | Params\n----------------------------------------------\n0 | mae     | MeanAbsoluteError | 0     \n1 | SMSE    | SMSE              | 0     \n2 | linear1 | Linear            | 110   \n----------------------------------------------\n110       Trainable params\n0         Non-trainable params\n110       Total params\n0.000     Total estimated model params size (MB)\n</code>\n</pre> <pre>\n<code>Sanity Checking: |                                                                   | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>/Users/ericjia/yeastdnnexplorer/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n</code>\n</pre> <pre>\n<code>Training: |                                                                          | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Validation: |                                                                        | 0/? [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>`Trainer.fit` stopped: `max_epochs=10` reached.\n</code>\n</pre> <pre>\n<code>Linear Model Explained Variance (Method 2): 0.022074705362319945\n</code>\n</pre> <pre><code>data_module = get_data_module(3.0)\nnum_tfs = sum(data_module.n_sample)\n\n# Nonlinear model\nmodel = get_model(num_tfs)\ntrainer = get_trainer()\ntrainer.fit(model, data_module)\nexplained_variance = calculate_explained_variance(model, data_module)\nmodel_ves.append(explained_variance)\nprint(\"Nonlinear Model Explained Variance (Method 2):\", explained_variance)\n\n# Linear model\nlinear_model = get_linear_model(num_tfs)\ntrainer = get_trainer()\ntrainer.fit(linear_model, data_module)\nexplained_variance_linear = calculate_explained_variance(linear_model, data_module)\nlinear_model_test_ves.append(explained_variance_linear)\nprint(\"Linear Model Explained Variance (Method 2):\", explained_variance_linear)\n\n</code></pre> <pre>\n<code>Nonlinear Model Explained Variance (Method 2): 0.2549255728721619\nLinear Model Explained Variance (Method 2): 0.07210595607757568\n</code>\n</pre> <p>Once again, a similar explained variance metric was obtained using both models. However, this time, the simple linear model achieved a positive value, meaning that it was able to somewhat account for the distribution of the data. However, given the difference in explained variance scores, this once again suggests that the nonlinear, customizable model performs substantially better than the simple linear model based on the generated data with a mean adjustment of 3. It seems that the additional parameters in the nonlinear neural network can better accomodate the complexity of the data relatively better than the simple linear model.</p> <pre><code>data_module = get_data_module(3.0, perturbation_effect_adjustment_function_with_tf_relationships, tf_relationships_dict)\nnum_tfs = sum(data_module.n_sample)\n\n# Nonlinear model\nmodel = get_model(num_tfs)\ntrainer = get_trainer()\ntrainer.fit(model, data_module)\nexplained_variance = calculate_explained_variance(model, data_module)\nmodel_ves.append(explained_variance)\nprint(\"Nonlinear Model Explained Variance (Method 3):\", explained_variance)\n\n# Linear model\nlinear_model = get_linear_model(num_tfs)\ntrainer = get_trainer()\ntrainer.fit(linear_model, data_module)\nexplained_variance_linear = calculate_explained_variance(linear_model, data_module)\nlinear_model_test_ves.append(explained_variance_linear)\nprint(\"Linear Model Explained Variance (Method 3):\", explained_variance_linear)\n</code></pre> <pre>\n<code>Nonlinear Model Explained Variance (Method 3): 0.18576881289482117\nLinear Model Explained Variance (Method 3): 0.00479055643081665\n</code>\n</pre> <p>It appears once again that the customizable model obtains a more positive and larger explained variance compared to the simple linear model when implementing dependencies among TFs. It is possible that the added layer of complexity makes it more difficult for the simple linear model to make an accurate prediction. Lastly, it would be interesting to consider how the models will perform on data including more complex dependencies that involve binary relations.</p> <pre><code>data_module = get_data_module(\n    3.0, \n    adjustment_function=perturbation_effect_adjustment_function_with_tf_relationships_boolean_logic, \n    tf_relationships_dict=tf_relationships_dict_boolean_logic)\nnum_tfs = sum(data_module.n_sample)\n\n# Nonlinear model\nmodel = get_model(num_tfs)\ntrainer = get_trainer()\ntrainer.fit(model, data_module)\nexplained_variance = calculate_explained_variance(model, data_module)\nmodel_ves.append(explained_variance)\nprint(\"Nonlinear Model Explained Variance (Method 4):\", explained_variance)\n\n# Linear model\nlinear_model = get_linear_model(num_tfs)\ntrainer = get_trainer()\ntrainer.fit(linear_model, data_module)\nexplained_variance_linear = calculate_explained_variance(linear_model, data_module)\nlinear_model_test_ves.append(explained_variance_linear)\nprint(\"Linear Model Explained Variance (Method 4):\", explained_variance_linear)\n</code></pre> <pre>\n<code>Nonlinear Model Explained Variance (Method 4): 0.1844494581222534\nLinear Model Explained Variance (Method 4): 0.012156563997268676\n</code>\n</pre> <p>Once again, our customizable model outperforms the simple linear model in terms of obtaining a higher explained variance. Surprisingly, both models acheive explained variance scores that are somewhat similar to their scores previous when implementing dependencies among TFs. This may be of further interest and could use more research to better determine exactly why this is occurring based on the generated data.</p> <p>Now we can plot the results across each of the 4 conditions tested above to visualize how the simple linear model and the nonlinear, customizable model perform compared to one another with regard to their explained variance scores.</p> <pre><code>data_module = get_data_module(3.0, perturbation_effect_adjustment_function_with_tf_relationships, tf_relationships_dict)\nnum_tfs = sum(data_module.n_sample)\n\n# Nonlinear model\nmodel = get_model(num_tfs)\ntrainer = get_trainer()\ntrainer.fit(model, data_module)\nexplained_variance = calculate_explained_variance(model, data_module)\nmodel_ves.append(explained_variance)\nprint(\"Nonlinear Model Explained Variance (Method 3):\", explained_variance)\n\n# Linear model\nlinear_model = get_linear_model(num_tfs)\ntrainer = get_trainer()\ntrainer.fit(linear_model, data_module)\nexplained_variance_linear = calculate_explained_variance(linear_model, data_module)\nlinear_model_test_ves.append(explained_variance_linear)\nprint(\"Linear Model Explained Variance (Method 3):\", explained_variance_linear)\n</code></pre> <pre>\n<code>Nonlinear Model Explained Variance (Method 3): 0.18576881289482117\nLinear Model Explained Variance (Method 3): 0.00479055643081665\n</code>\n</pre> <p>It appears once again that the customizable model obtains a more positive and larger explained variance compared to the simple linear model when implementing dependencies among TFs. It is possible that the added layer of complexity makes it more difficult for the simple linear model to make an accurate prediction. Lastly, it would be interesting to consider how the models will perform on data including more complex dependencies that involve binary relations.</p> <pre><code>data_module = get_data_module(\n    3.0, \n    adjustment_function=perturbation_effect_adjustment_function_with_tf_relationships_boolean_logic, \n    tf_relationships_dict=tf_relationships_dict_boolean_logic)\nnum_tfs = sum(data_module.n_sample)\n\n# Nonlinear model\nmodel = get_model(num_tfs)\ntrainer = get_trainer()\ntrainer.fit(model, data_module)\nexplained_variance = calculate_explained_variance(model, data_module)\nmodel_ves.append(explained_variance)\nprint(\"Nonlinear Model Explained Variance (Method 4):\", explained_variance)\n\n# Linear model\nlinear_model = get_linear_model(num_tfs)\ntrainer = get_trainer()\ntrainer.fit(linear_model, data_module)\nexplained_variance_linear = calculate_explained_variance(linear_model, data_module)\nlinear_model_test_ves.append(explained_variance_linear)\nprint(\"Linear Model Explained Variance (Method 4):\", explained_variance_linear)\n</code></pre> <pre>\n<code>Nonlinear Model Explained Variance (Method 4): 0.1844494581222534\nLinear Model Explained Variance (Method 4): 0.012156563997268676\n</code>\n</pre> <p>Once again, our customizable model outperforms the simple linear model in terms of obtaining a higher explained variance. Surprisingly, both models acheive explained variance scores that are somewhat similar to their scores previous when implementing dependencies among TFs. This may be of further interest and could use more research to better determine exactly why this is occurring based on the generated data.</p> <p>Now we can plot the results across each of the 4 conditions tested above to visualize how the simple linear model and the nonlinear, customizable model perform compared to one another with regard to their explained variance scores.</p> <pre><code>data_gen_methods = [\"No Mean Adjustment\", \"Dependent Mean Adjustment\", \"TF Dependent Mean Adjustment\", \"TF Dependent Mean Adjust with Boolean Logic\"]\nplt.figure(figsize=(10, 6))\nplt.scatter(data_gen_methods, model_ves, color='blue')\nplt.scatter(data_gen_methods, linear_model_test_ves, color='orange')\nplt.title('Model VE Comparison (bound mean = 3.0)')\nplt.xlabel('Model')\nplt.ylabel('Variance Explained')\nplt.grid(True)\nplt.xticks(rotation=45, ha=\"right\")\nplt.legend(['Complex (Customizable) Model', 'Linear Model'])\nplt.tight_layout()\nplt.show()\n</code></pre> <pre>\n<code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n/Users/ericjia/yeastdnnexplorer/yeastdnnexplorer/data_loaders/synthetic_data_loader.py:260: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_train, Y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(\n/Users/ericjia/yeastdnnexplorer/yeastdnnexplorer/data_loaders/synthetic_data_loader.py:263: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_val, Y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(\n/Users/ericjia/yeastdnnexplorer/yeastdnnexplorer/data_loaders/synthetic_data_loader.py:266: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_test, Y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(\n\n  | Name          | Type              | Params\n----------------------------------------------------\n0 | activation    | LeakyReLU         | 0     \n1 | input_layer   | Linear            | 704   \n2 | hidden_layers | ModuleList        | 2.1 K \n3 | output_layer  | Linear            | 330   \n4 | dropout       | Dropout           | 0     \n5 | mae           | MeanAbsoluteError | 0     \n6 | SMSE          | SMSE              | 0     \n----------------------------------------------------\n3.1 K     Trainable params\n0         Non-trainable params\n3.1 K     Total params\n0.012     Total estimated model params size (MB)\n</code>\n</pre> <p>The x-axis labels the method in which the data was generated according to the 4 options above. The y-axis represents the corresponding variance explained attained by these models. Each point represents the variance explained achieved after generating the data based on the x-axis, and the color of the point represents which model architecture was trained on the data resulting in the specificed explained variance. Now, we can clearly see that across the 4 conditions, the nonlinear, customizable model acheives a significantly higher positive explained variance compared to the simple linear model, which is good because it helps to confirm that the nonlinear model we are using is able to train on the data and better account for the distribution of the data, resulting in a higher explained variance compared to the simple linear model. </p>"},{"location":"tutorials/visualizing_and_testing_data_generation_methods/#visualizing-and-testing-data-generation-methods","title":"Visualizing and Testing Data Generation Methods","text":"<p>In this notebook, we will run an experiment to display the average perturbation effect values that we generate with the 4 different methods we have for perturbation effect generation (other than the method for generating the perturbation effect values, we will be holding everything else the same). </p> <p>Recall that we have 4 methods for generating perturbation effect data (see <code>generate_in_silico_data.ipynb</code> for more information on these): 1. No Mean Adjustment 2. Standard Mean Adjustment 3. Mean adjustment dependent on all TFs bound to gene in question 4. Mean adjustment dependent on binary relationships between bound and unbound TFs to gene in question.</p> <p>After understanding what the generated data looks like for each of these methods, we will perform another experiment where we train the same model on data generated with each of these methods and compare the model\u2019s performance to a simple linear model.</p>"},{"location":"tutorials/visualizing_and_testing_data_generation_methods/#generating-the-data","title":"Generating the Data","text":""},{"location":"tutorials/visualizing_and_testing_data_generation_methods/#generating-the-data_1","title":"Generating the Data","text":""},{"location":"tutorials/visualizing_and_testing_data_generation_methods/#visualizing-the-results","title":"Visualizing the Results","text":""},{"location":"tutorials/visualizing_and_testing_data_generation_methods/#training-models-on-data-generated-from-the-4-different-methods","title":"Training models on data generated from the 4 different methods","text":"<p>In the next experiment, we will be training the exact same model on data generated from each of these 4 methods. We will also train a simple linear model on all four methods to use as a baseline to compare to. Other than the method used to generate the data, everything else will be held the same. We define a few helper functions to run our experiment. We make helper functions for things that will mostly be the same across each training loop so that we don\u2019t have to keep redefining them.</p>"},{"location":"tutorials/visualizing_and_testing_data_generation_methods/#1-train-models-on-data-generated-with-no-mean-adjustment","title":"1) Train models on data generated with no mean adjustment","text":"<p>We will first compare the models performances on data generated without any mean adjustments. This is the most simple dataset we will generate, and serves as a good starting point for the models.</p>"},{"location":"tutorials/visualizing_and_testing_data_generation_methods/#train-models-on-data-generated-with-normal-mean-adjustments","title":"Train models on data generated with normal mean adjustments","text":"<p>Now, let us perform the same comparison but using this condition, with a normal mean adjustment of 3.</p>"},{"location":"tutorials/visualizing_and_testing_data_generation_methods/#2-train-models-on-data-generated-with-normal-mean-adjustments","title":"2) Train models on data generated with normal mean adjustments","text":"<p>Now, let us perform the same comparison but using this condition, with a normal mean adjustment of 3.</p>"},{"location":"tutorials/visualizing_and_testing_data_generation_methods/#3-train-models-on-data-generated-with-dependent-mean-adjustments","title":"3) Train models on data generated with dependent mean adjustments","text":"<p>Now we are implementing a dataset that contains dependent mean adjustments as shown below, with a mean adjustment of 3 if the TF meets the criteria defined by the dictionary.</p>"},{"location":"tutorials/visualizing_and_testing_data_generation_methods/#4-train-models-on-data-generated-using-the-binary-relations-between-tfs","title":"4) Train models on data generated using the binary relations between TFs","text":"<p>Similar to the previous condition, we are implementing dependencies between TFs. However, the following dictionary contains simple logic that makes these dependencies far more complex. For example, in order for transcription factor 4 to be perturbed based on the dictionary below, both TFs 1 and 2 need to be considered perturbed in order for this TF to be perturbed as well. Adding this additional layer of complexity will be an interesting challenge: let us see how the two models perform here.</p>"},{"location":"tutorials/visualizing_and_testing_data_generation_methods/#visualizing-the-explained-variance","title":"Visualizing the Explained Variance","text":""},{"location":"tutorials/visualizing_and_testing_data_generation_methods/#4-train-models-on-data-generated-using-the-binary-relations-between-tfs_1","title":"4) Train models on data generated using the binary relations between TFs","text":"<p>Similar to the previous condition, we are implementing dependencies between TFs. However, the following dictionary contains simple logic that makes these dependencies far more complex. For example, in order for transcription factor 4 to be perturbed based on the dictionary below, both TFs 1 and 2 need to be considered perturbed in order for this TF to be perturbed as well. Adding this additional layer of complexity will be an interesting challenge: let us see how the two models perform here.</p>"},{"location":"tutorials/visualizing_and_testing_data_generation_methods/#visualizing-the-explained-variance_1","title":"Visualizing the Explained Variance","text":""}]}